{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import shap\n",
    "import tensorboard\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report\n",
    "from functools import partial\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP\n",
    "from codes.models.metrics import calculate_roc_auc\n",
    "\n",
    "import featurewiz as gwiz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from codes.models.Decision_tree.utils import get_indice, get_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### framework constants:\n",
    "model_type = 'decision_tree'\n",
    "model_version = 'gradient_boosting'\n",
    "test_name = '1_test_train_transfo_V1'\n",
    "tryout = True # True if we are ding a tryout, False otherwise \n",
    "### data constants:\n",
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "pheno_method = 'Paul' # Paul, Abby\n",
    "ld = 'no'\n",
    "rollup_depth = 4\n",
    "binary_classes = True #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = False\n",
    "save_data = True\n",
    "remove_none = True\n",
    "decorelate = False\n",
    "equalize_label = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 50\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "compute_features = True\n",
    "padding = False\n",
    "list_env_features = ['age', 'sex']\n",
    "list_pheno_ids = None #list(np.load(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/phewas/list_associations_snps/{SNP}_paul.npy'))\n",
    "\n",
    "### data format\n",
    "\n",
    "batch_size = 20\n",
    "data_share = 1\n",
    "\n",
    "##### model constants\n",
    "\n",
    "\n",
    "##### training constants\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=remove_none,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share,\n",
    "                         list_pheno_ids=list_pheno_ids,\n",
    "                         binary_classes=binary_classes, \n",
    "                         ld = ld)\n",
    "#patient_list = dataT.get_patientlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels_patients, indices_env, name_envs, eids = dataT.get_tree_data(with_env=False, load_possible=True, only_relevant=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([data, labels_patients.reshape( len(labels_patients),1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_ini = np.sum(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized = False\n",
    "interest = False\n",
    "keep = False\n",
    "scaled = False\n",
    "remove = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interest:\n",
    "    data_use, labels_use = data[:nb_patients_interest, :-1], labels_patients[:nb_patients_interest]\n",
    "else:\n",
    "    data_use, labels_use = data_complete, labels_patients\n",
    "if remove:\n",
    "    eids_remove = np.load('/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/UKBB/eids_remove_1.npy')\n",
    "    indices_eids = (1-np.isin(eids, eids_remove)).astype(bool)\n",
    "    eids_use = eids[indices_eids]\n",
    "    data_use = data_use[indices_eids]\n",
    "    labels_use = labels_use[indices_eids]\n",
    "    \n",
    "if equalized:\n",
    "    pheno, labels = DataTransfo_1SNP.equalize_label(data=data_use, labels = labels_use)\n",
    "else:\n",
    "    pheno, labels = data_use, labels_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_patients_train, diseases_patients_test, label_patients_train, label_patients_test = train_test_split(data, labels, test_size = 1-prop_train_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_keep = (frequencies_ini > 0) & (frequencies_ini > 100)\n",
    "#indices_keep = shaps!=0\n",
    "diseases_patients_train_keep = diseases_patients_train[:,indices_keep]\n",
    "diseases_patients_test_keep = diseases_patients_test[:, indices_keep]\n",
    "if keep:\n",
    "    diseases_patients_train_model = diseases_patients_train_keep\n",
    "    diseases_patients_test_model = diseases_patients_test_keep\n",
    "else:\n",
    "    diseases_patients_train_model = diseases_patients_train\n",
    "    diseases_patients_test_model = diseases_patients_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### maskage:\n",
    "nb_features = diseases_patients_train_model.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, feature_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, feature_dim),\n",
    "            nn.Sigmoid()  # Assuming input data range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, data_random, data_truth):\n",
    "        data_gen = self.model(data_random)\n",
    "        loss_pheno = torch.norm(data_gen[:, :-1], data_truth[:, :-1] ) / np.sqrt(data_gen.numel())\n",
    "        loss_labels = torch.norm(data_gen[:, -1], data_truth[:, -1] ) / np.sqrt(data_gen.numel())\n",
    "\n",
    "\n",
    "        return data_gen, loss_pheno + loss_labels\n",
    "\n",
    "    def eval(self, data_mask, data_truth):\n",
    "        data_gen = self.model(data_mask)\n",
    "        indices_mask = np.where(data)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        validity = self.model(features)\n",
    "        return validity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = nb_features\n",
    "feature_dim = nb_features # Number of features in your input data\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim, feature_dim)\n",
    "discriminator = Discriminator(feature_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "adversarial_loss = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Convert your data to PyTorch TensorDataset\n",
    "data_tensor_train_random = torch.tensor(diseases_patients_train_model, dtype=torch.float32)\n",
    "data_tensor_train_truth = torch.tensor(diseases_patients_train_model, dtype=torch.float32)\n",
    "data_tensor_train_random[:, -1] = torch.rand(len(data_tensor_train_random[:, -1]))\n",
    "\n",
    "data_tensor_test_random = torch.tensor(diseases_patients_test_model, dtype=torch.float32)\n",
    "data_tensor_test_truth = torch.tensor(diseases_patients_test_model, dtype=torch.float32)\n",
    "data_tensor_test_random[:, -1] = torch.rand(len(data_tensor_test_random[:, -1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(list(zip(data_tensor_train, labels_tensor_train)), batch_size=20)\n",
    "dataloader_test = DataLoader(list(zip(data_tensor_test, labels_tensor_test)), batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch_train.shape, labels_batch_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (data_batch_train, labels_batch_train) in enumerate(dataloader_train):\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        d_gen, loss = generator(data_batch_train, labels_batch_train)\n",
    "        loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \"\"\"\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        d_real_loss = adversarial_loss(discriminator(features), valid)\n",
    "        d_fake_loss = adversarial_loss(discriminator(gen_features.detach()), fake)\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \"\"\"\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [G loss: %f]\"\n",
    "            % (epoch, epochs, i, len(dataloader_train), loss.item())\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
