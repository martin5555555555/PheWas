{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR, LinearLR, SequentialLR\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "from codes.models.utils import clear_last_line, print_file\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.models.Generative.Embeddings import EmbeddingPheno, EmbeddingSNPS\n",
    "from codes.models.Generative.GenerativeModel import GenerativeModelPheWasV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "class EmbeddingPheno(nn.Module):\n",
    "    def __init__(self, method=None, vocab_size=None, Embedding_size=None, rollup_depth=4, freeze_embed=False, dicts=None):\n",
    "        super(EmbeddingPheno, self).__init__()\n",
    "\n",
    "        self.dicts = dicts\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.nb_distinct_diseases_patient = vocab_size\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.metadata = None\n",
    "\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        \n",
    "        if method == None:\n",
    "            self.distinct_diseases_embeddings = nn.Embedding(vocab_size, Embedding_size)\n",
    "            #self.counts_embeddings = nn.Embedding(max_count_same_disease, Embedding_size)\n",
    "            torch.nn.init.normal_(self.distinct_diseases_embeddings.weight, mean=0.0, std=0.02)\n",
    "           # torch.nn.init.normal_(self.counts_embeddings.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        elif method == 'Abby':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            #self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "\n",
    "\n",
    "\n",
    "        elif method=='Paul':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            #self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "\n",
    "        embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "        pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "        pretrained_weights_diseases = pretrained_weights_diseases[1:]\n",
    "        nb_phenos = pretrained_weights_diseases.shape[0]\n",
    "        self.similarities_tab = torch.tensor(np.array([F.cosine_similarity(pretrained_weights_diseases, pretrained_weights_diseases[i], dim=-1).view(nb_phenos) for i in range(nb_phenos)]))\n",
    "        \n",
    "\n",
    "    def write_embedding(self, writer):\n",
    "            embedding_tensor = self.distinct_diseases_embeddings.weight.data.detach().cpu().numpy()\n",
    "            writer.add_embedding(embedding_tensor, metadata=self.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "class EmbeddingSNPS(nn.Module):\n",
    "    def __init__(self, method=None, nb_SNPS=1, Embedding_size=10, freeze_embed=False):\n",
    "        super(EmbeddingSNPS, self).__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.nb_SNPS = nb_SNPS\n",
    "\n",
    "        if method == None:\n",
    "            self.SNPS_embeddings = nn.Embedding(self.nb_SNPS*2, Embedding_size)\n",
    "            #self.counts_embeddings = nn.Embedding(max_count_same_disease, Embedding_size)\n",
    "            torch.nn.init.normal_(self.SNPS_embeddings.weight, mean=0.0, std=0.02)\n",
    "           # torch.nn.init.normal_(self.counts_embeddings.weight, mean=0.0, std=0.02)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "\n",
    "\n",
    "class PhenotypeEncoding(nn.Module):\n",
    "    def __init__(self, Embedding, Head_size, n_head, n_layer, mask_padding=False, padding_token=None, p_dropout=0, device='cpu', instance_size=None, proj_embed=True):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.Embedding_size = Embedding.Embedding_size\n",
    "        self.instance_size=instance_size\n",
    "        self.proj_embed = proj_embed\n",
    "        if not self.proj_embed:\n",
    "            self.instance_size = self.Embedding_size\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size)\n",
    "        self.blocks =PadMaskSequential(*[BlockPheno(self.instance_size, n_head=n_head, Head_size=Head_size, p_dropout=p_dropout) for _ in range(n_layer)]) #Block(self.instance_size, n_head=n_head, Head_size=Head_size) \n",
    "        self.ln_f = nn.LayerNorm(self.instance_size) # final layer norm\n",
    "        self.Embedding = Embedding\n",
    "        self.mask_padding = mask_padding\n",
    "        self.padding_token = padding_token\n",
    "        self.padding_mask = None\n",
    "        self.device = device\n",
    "       \n",
    "        self.diseases_embedding_table = Embedding.distinct_diseases_embeddings\n",
    "        #if self.pheno_method == 'Paul':\n",
    "        # self.counts_embedding_table = Embedding.counts_embeddings\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, diseases_sentence):\n",
    "        B, S = np.shape(diseases_sentence)[0], np.shape(diseases_sentence)[1]\n",
    "        mask = torch.where(diseases_sentence==self.padding_token)\n",
    "        padding_mask_mat = torch.ones((B, S, S), dtype=torch.bool)\n",
    "        padding_mask_mat[mask] = 0\n",
    "        padding_mask_mat.transpose(-2,-1)[mask] = 1\n",
    "\n",
    "        padding_mask_probas = torch.zeros((B, S)).to(bool)\n",
    "        padding_mask_probas[mask] = True\n",
    "        padding_mask_probas = padding_mask_probas.view(B, S)\n",
    "        return padding_mask_mat.to(self.device), padding_mask_probas # 1 if masked, 0 else\n",
    "\n",
    "    def set_padding_mask_transformer(self, padding_mask, padding_mask_probas):\n",
    "        self.padding_mask = padding_mask\n",
    "        self.padding_mask_probas = padding_mask_probas\n",
    "    \n",
    "    def forward(self, diseases_sentence):\n",
    "        Batch_len, Sentence_len = diseases_sentence.shape\n",
    "\n",
    "        diseases_sentence = diseases_sentence.to(self.device)\n",
    "        #counts_diseases = counts_diseases.to(self.device)\n",
    "        \n",
    "        if self.mask_padding:\n",
    "            padding_mask, padding_mask_probas = self.create_padding_mask(diseases_sentence)\n",
    "            self.set_padding_mask_transformer(padding_mask, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask)\n",
    "\n",
    "        diseases_sentences_embedded = self.diseases_embedding_table(diseases_sentence) # shape B, S, E\n",
    "\n",
    "        x = diseases_sentences_embedded \n",
    "    \n",
    "        #if self.pheno_method == 'Paul':\n",
    "        #    counts_diseases_embedded = self.counts_embedding_table(counts_diseases) # shape B, S, E\n",
    "        #    #x = x + counts_diseases_embedded # shape B, S, E \n",
    "        \n",
    "        if self.proj_embed:\n",
    "            x = self.projection_embed(x)\n",
    "        x = self.blocks(x) # shape B, S, E\n",
    "        \n",
    "        return x\n",
    "   \n",
    "\n",
    "class PadMaskSequential(nn.Sequential):\n",
    "    def __init__(self, *args):\n",
    "        super(PadMaskSequential, self).__init__(*args)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sequential(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module(x)\n",
    "        return x\n",
    "   \n",
    "class BlockPheno(nn.Module):\n",
    "    def __init__(self, instance_size, n_head, Head_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadSelfAttention(n_head, Head_size, instance_size, p_dropout)\n",
    "        self.ffwd = FeedForward(instance_size, p_dropout)\n",
    "        self.ln1 = nn.LayerNorm(instance_size)\n",
    "        self.ln2 = nn.LayerNorm(instance_size)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_block(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, Head_size, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.qkv_network = nn.Linear(instance_size, Head_size * 3, bias = False) #group the computing of the nn.Linear for q, k and v, shape \n",
    "        self.proj = nn.Linear(Head_size, instance_size)\n",
    "        self.attention_dropout = nn.Dropout(p_dropout)\n",
    "        self.resid_dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "        self.multi_head_size = Head_size // n_head\n",
    "        self.flash = False\n",
    "        self.n_head = n_head\n",
    "        self.Head_size = Head_size\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sa(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "        q, k, v  = self.qkv_network(x).split(self.Head_size, dim=2) #q, k, v of shape (B, S, H)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas = self.attention_dropout(attention_probas)\n",
    "\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out        \n",
    "    \n",
    "  \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear( instance_size, 4 * instance_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * instance_size, instance_size),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    " \n",
    "\n",
    "class SNPEncoding(nn.Module):\n",
    "    def __init__(self, Embedding, Head_size, n_head, n_layer, p_dropout=0, device='cpu'):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.Embedding_size = Embedding.Embedding_size\n",
    "        self.instance_size = self.Embedding_size\n",
    "        self.blocks = nn.Sequential(*[BlockSNP(self.instance_size, n_head=n_head, Head_size=Head_size, p_dropout=p_dropout) for _ in range(n_layer)]) #Block(self.instance_size, n_head=n_head, Head_size=Head_size) \n",
    "        self.ln_f = nn.LayerNorm(self.instance_size) # final layer norm\n",
    "        self.Embedding = Embedding\n",
    "        self.device = device\n",
    "       \n",
    "        self.SNPS_embedding_table = Embedding.SNPS_embeddings\n",
    "        #if self.pheno_method == 'Paul':\n",
    "        # self.counts_embedding_table = Embedding.counts_embeddings\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, diseases_sentence):\n",
    "        B, S = np.shape(diseases_sentence)[0], np.shape(diseases_sentence)[1]\n",
    "        mask = torch.where(diseases_sentence==self.padding_token)\n",
    "        padding_mask_mat = torch.zeros((B, S, S), dtype=torch.bool)\n",
    "        padding_mask_mat[mask] = 1\n",
    "        padding_mask_mat.transpose(-2,-1)[mask] = 1\n",
    "\n",
    "        padding_mask_probas = torch.zeros((B, S)).to(bool)\n",
    "        padding_mask_probas[mask] = True\n",
    "        padding_mask_probas = padding_mask_probas.view(B, S)\n",
    "        return padding_mask_mat, padding_mask_probas # 1 if masked, 0 else\n",
    "\n",
    "    def set_padding_mask_transformer(self, padding_mask, padding_mask_probas):\n",
    "        self.padding_mask = padding_mask\n",
    "        self.padding_mask_probas = padding_mask_probas\n",
    "    \n",
    "    def forward(self, SNPS_sentence):\n",
    "        Batch_len, Nb_SNP = SNPS_sentence.shape\n",
    "        pos_SNPS = torch.arange(Nb_SNP)*2\n",
    "        SNPS_sentence = SNPS_sentence + pos_SNPS # Shape B, nb_SNPS*2\n",
    "        SNP_sentences_embedded = self.SNPS_embedding_table(SNPS_sentence) # shape B, Nb_SNP, E\n",
    "\n",
    "        #if self.pheno_method == 'Paul':\n",
    "        #    counts_diseases_embedded = self.counts_embedding_table(counts_diseases) # shape B, S, E\n",
    "        #    #x = x + counts_diseases_embedded # shape B, S, E \n",
    "        x = self.blocks(SNP_sentences_embedded) # shape B, S, E\n",
    "        \n",
    "        return x\n",
    "   \n",
    "\n",
    "class BlockSNP(nn.Module):\n",
    "    def __init__(self, instance_size, n_head, Head_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadSelfAttention(n_head, Head_size, instance_size, p_dropout)\n",
    "        self.ffwd = FeedForward(instance_size, p_dropout)\n",
    "        self.ln1 = nn.LayerNorm(instance_size)\n",
    "        self.ln2 = nn.LayerNorm(instance_size)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossMultiAttentionSNPPheno(nn.Module):\n",
    "        # Key are the phenos, Queries are the SNPS\n",
    "    def __init__(self, n_head, Head_size, instance_size_pheno, instance_size_SNPS, p_dropout):\n",
    "        super().__init__()\n",
    "        self.queries_network = nn.Linear(instance_size_pheno, Head_size, bias=False)\n",
    "        self.keys_network = nn.Linear(instance_size_pheno, Head_size, bias=False)\n",
    "        self.values_network_SNP = nn.Linear(instance_size_SNPS, Head_size, bias=False)\n",
    "        self.values_network_pheno = nn.Linear(instance_size_pheno, Head_size, bias=False)\n",
    "\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(p_dropout)\n",
    "        self.resid_dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "        self.multi_head_size = Head_size // n_head\n",
    "        self.n_head = n_head\n",
    "        self.Head_size = Head_size\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sa(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, pheno_encoded, SNPS_encoded):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len_pheno, _ = pheno_encoded.shape\n",
    "        Batch_len, Sentence_len_SNPS, _ = SNPS_encoded.shape\n",
    "        keys = self.keys_network(pheno_encoded)\n",
    "        queries = self.queries_network(SNPS_encoded) \n",
    "             \n",
    "        values_pheno = self.values_network_pheno(pheno_encoded)\n",
    "        values_SNPS = self.values_network_SNP(SNPS_encoded)\n",
    "       \n",
    "    \n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = queries.view(Batch_len, Sentence_len_SNPS, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S_SNPS, MH\n",
    "        k_multi_head = keys.view(Batch_len, Sentence_len_pheno, self.n_head, self.multi_head_size).transpose(1, 2)#shape B, HN, S_PHENO, MH\n",
    "        values_pheno_multihead = values_pheno.view(Batch_len, Sentence_len_pheno, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        values_SNPS_multihead = values_SNPS.view(Batch_len, Sentence_len_SNPS, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        attention_weights = (k_multi_head @ q_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, HN, S_PHENO, S_SNPS\n",
    "        ### padding mask #####\n",
    "        if self.padding_mask != None:\n",
    "            attention_weights[self.padding_mask] = 1**-10     #float('-inf')\n",
    "        #print(f'wei0={attention_weights}')\n",
    "        \n",
    "\n",
    "        #print(f'wei1={attention_probas}')\n",
    "        #attention_probas = self.dropout(attention_probas)\n",
    "        ## weighted aggregation of the values\n",
    "       \n",
    "        attention_probas_phenos = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "        attention_probas_SNPS = F.softmax(attention_weights.transpose(-1, -2), dim=-1) # shape B, S, S\n",
    "\n",
    "\n",
    "        attention_probas_phenos = self.attention_dropout(attention_probas_phenos)\n",
    "        attention_probas_SNPS = self.attention_dropout(attention_probas_SNPS)\n",
    "\n",
    "        out_pheno = attention_probas_phenos @ values_SNPS_multihead  # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out_SNPS = attention_probas_SNPS @ values_pheno_multihead\n",
    "        \n",
    "        out_pheno = out_pheno.transpose(1, 2).contiguous().view(Batch_len, Sentence_len_pheno, self.Head_size) + values_pheno\n",
    "        out_SNPS = out_SNPS.transpose(1, 2).contiguous().view(Batch_len, Sentence_len_SNPS, self.Head_size) + values_SNPS\n",
    "\n",
    "        out_pheno = self.resid_dropout(out_pheno)\n",
    "        out_SNPS = self.resid_dropout(out_SNPS)\n",
    "        return out_pheno, out_SNPS       \n",
    "    \n",
    "class BlockCrossSNPPHENO(nn.Module):\n",
    "    def __init__(self, instance_size_SNPS, instance_size_pheno, n_head, Head_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.ca = CrossMultiAttentionSNPPheno(n_head=n_head, Head_size=Head_size, instance_size_SNPS=instance_size_SNPS, \n",
    "                                             instance_size_pheno=instance_size_pheno,\n",
    "                                              p_dropout=p_dropout)       \n",
    "        self.ffwd_pheno = FeedForward(instance_size_pheno, p_dropout)\n",
    "        self.ffwd_SNPS = FeedForward(instance_size_SNPS, p_dropout)\n",
    "\n",
    "        self.ln1_pheno = nn.LayerNorm(instance_size_pheno)\n",
    "        self.ln1_SNPS = nn.LayerNorm(instance_size_SNPS)\n",
    "\n",
    "        self.proj_pheno = nn.Linear(Head_size, instance_size_pheno)\n",
    "        self.proj_SNPS = nn.Linear(Head_size, instance_size_SNPS)\n",
    "\n",
    "\n",
    "\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def forward(self, encoded_phenos, encoded_SNPS):\n",
    "        encoded_phenos = self.ln1_pheno(encoded_phenos)\n",
    "        encoded_SNPS = self.ln1_SNPS(encoded_SNPS)\n",
    "\n",
    "        out_pheno, out_SNPS = self.ca(encoded_phenos, encoded_SNPS)\n",
    "\n",
    "        \n",
    "        out_pheno = self.proj_pheno(out_pheno)\n",
    "        out_pheno = self.ln1_pheno(out_pheno)\n",
    "        out_pheno = out_pheno + self.ffwd_pheno(out_pheno)\n",
    "\n",
    "        out_SNPS = self.proj_SNPS(out_SNPS)\n",
    "        out_SNPS = self.ln1_SNPS(out_SNPS) \n",
    "        out_SNPS = out_SNPS + self.ffwd_SNPS(out_SNPS)  \n",
    "            \n",
    "        return out_pheno, out_SNPS\n",
    "    \n",
    "class CrossPadMaskSequential(nn.Sequential):\n",
    "    def __init__(self, *args):\n",
    "        super(CrossPadMaskSequential, self).__init__(*args)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sequential(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, encoded_phenos, encoded_SNPS):\n",
    "        for module in self:\n",
    "            encoded_phenos, encoded_SNPS = module(encoded_phenos, encoded_SNPS)\n",
    "        return encoded_phenos, encoded_SNPS\n",
    "   \n",
    "class PredictLogit(nn.Module):\n",
    "    def __init__(self, instance_size_SNPS, instance_size_pheno, nb_phenos_possible):\n",
    "        super().__init__()\n",
    "        self.ln2_phenos = nn.LayerNorm(instance_size_pheno)\n",
    "        self.ln2_SNPS = nn.LayerNorm(instance_size_SNPS)\n",
    "\n",
    "        \n",
    "        self.get_logits_phenos = nn.Linear(instance_size_pheno, nb_phenos_possible)\n",
    "        self.get_logits_SNPS = nn.Linear(instance_size_SNPS, 2)\n",
    "    \n",
    "    def forward(self, pheno_sentence, SNPS_sentence, value):\n",
    "        if value == 'pheno':\n",
    "            pheno_sentence = self.ln2_phenos(pheno_sentence)\n",
    "            logits = self.get_logits_phenos(pheno_sentence)\n",
    "        else:\n",
    "            SNPS_sentence = self.ln2_SNPS(SNPS_sentence)\n",
    "            logits = self.get_logits_SNPS(SNPS_sentence)\n",
    "        logits_mean = logits.mean(axis=1)\n",
    "        return logits_mean\n",
    "\n",
    "class GenerativeModelPheWasV1(nn.Module):\n",
    "    def __init__(self, n_head_pheno, Head_size_pheno, Embedding_pheno, Embedding_SNPS, instance_size_pheno,\n",
    "                n_layer_pheno,  nb_SNPS, n_layer_SNPS, n_head_SNPS, Head_size_SNPS, instance_size_SNPS, nb_phenos_possible,\n",
    "                n_head_cross, Head_size_cross, n_layer_cross, p_dropout, device='cpu', mask_padding=True,\n",
    "                loss_version_pheno='cross_entropy', loss_version_SNPS='cross_entropy', gamma=2, alpha=1, padding_token=0):\n",
    "        super().__init__()\n",
    "        print(device, flush=True)\n",
    "        self.Embedding_pheno = Embedding_pheno\n",
    "        self.Embedding_SNPS = Embedding_SNPS\n",
    "        self.Embedding_size_pheno = Embedding_pheno.Embedding_size\n",
    "        self.Embedding_size_SNP = Embedding_SNPS.Embedding_size\n",
    "        \n",
    "        self.instance_size_pheno = instance_size_pheno\n",
    "        self.n_head_pheno = n_head_pheno\n",
    "        self.Head_size_pheno = Head_size_pheno\n",
    "        self.n_layer_pheno = n_layer_pheno\n",
    "        self.loss_version_pheno = loss_version_pheno\n",
    "\n",
    "        self.Head_size_SNPS = Head_size_SNPS\n",
    "        self.n_layer_SNPS = n_layer_SNPS\n",
    "        self.nb_SNPS = nb_SNPS\n",
    "        self.n_head_SNPS = n_head_SNPS\n",
    "        self.instance_size_SNPS = instance_size_SNPS\n",
    "        self.loss_version_SNPS = loss_version_SNPS\n",
    "\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.n_layer_cross = n_layer_cross\n",
    "        self.Head_size_cross = Head_size_cross\n",
    "        self.n_head_cross = n_head_cross\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        self.nb_phenos_possible = nb_phenos_possible\n",
    "        self.device = device\n",
    "        self.padding_token = padding_token\n",
    "\n",
    "      \n",
    "\n",
    "        self.encoding_phenos = PhenotypeEncoding(Embedding=Embedding_pheno, Head_size=Head_size_pheno, \n",
    "            n_head=n_head_pheno, n_layer=n_layer_pheno, instance_size=instance_size_pheno, device=device, mask_padding=mask_padding,\n",
    "            p_dropout=p_dropout, padding_token=self.padding_token)\n",
    "        self.encoding_SNPS = SNPEncoding(Embedding=Embedding_SNPS, Head_size=Head_size_SNPS, n_head=n_head_SNPS,\n",
    "                    device=device, n_layer=n_layer_pheno, p_dropout=p_dropout)\n",
    "        self.blocks = CrossPadMaskSequential(*[ BlockCrossSNPPHENO(n_head=n_head_cross, Head_size=Head_size_cross, \n",
    "                                             instance_size_SNPS=instance_size_SNPS, \n",
    "                                             instance_size_pheno=instance_size_pheno,\n",
    "                                             p_dropout=p_dropout) for _ in range(n_layer_cross)]) #Block(self.instance_size, n_head=n_head, Head_size=Head_size) \n",
    "\n",
    "        self.predict_logit = PredictLogit(instance_size_pheno=instance_size_pheno, instance_size_SNPS=instance_size_SNPS, nb_phenos_possible=nb_phenos_possible-1) #-1 for padding\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, diseases_sentence, SNPS_sentence, value, targets=None):\n",
    "        diseases_sentence = diseases_sentence.to(self.device)\n",
    "        SNPS_sentence = SNPS_sentence.to(self.device)\n",
    "        \n",
    "        print('diseases_sentence_device'+str(diseases_sentence.device), flush=True)\n",
    "        phenotype_encoded = self.encoding_phenos(diseases_sentence)\n",
    "        SNPS_encoded = self.encoding_SNPS(SNPS_sentence)\n",
    "\n",
    "        out_pheno, out_SNPS = self.blocks(phenotype_encoded, SNPS_encoded)\n",
    "        logits = self.predict_logit(out_pheno, out_SNPS, value)\n",
    "\n",
    "        loss = None\n",
    "    \n",
    "        if targets != None:\n",
    "            targets = targets.to(self.device)\n",
    "            if value == 'pheno':\n",
    "                loss = self.calcul_loss_pheno(logits, targets, loss_version=self.loss_version_pheno)\n",
    "            elif value == 'SNP':\n",
    "                loss  = self.calcul_loss_SNPS(logits, targets, loss_version=self.loss_version_SNPS, gamma=self.gamma, alpha=self.alpha)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def calcul_loss_SNPS(self, logits, targets=None, loss_version='cross_entropy', gamma=None, alpha=None):\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #target : shape B,\n",
    "            \n",
    "            if loss_version == 'cross_entropy':\n",
    "                cross_entropy = F.cross_entropy(logits, targets)\n",
    "                return cross_entropy\n",
    "            elif loss_version == 'focal_loss':\n",
    "                alphas = ((1 - targets) * (alpha-1)).to(torch.float) + 1\n",
    "                probas = F.softmax(logits)\n",
    "                certidude_factor =  (1-probas[[range(probas.shape[0]), targets]])**gamma * alphas\n",
    "                cross_entropy = F.cross_entropy(logits, targets, reduce=False)\n",
    "                loss = torch.dot(cross_entropy, certidude_factor)\n",
    "                return loss\n",
    "            elif loss_version == 'predictions':\n",
    "                probas = F.softmax(logits)\n",
    "                predictions = (probas[:,1] > 0.5).to(int)\n",
    "                return torch.sum((predictions-targets)**2)/len(predictions)\n",
    "        \n",
    "    def predict_pheno(self, pheno_sentence, SNPS_sentences):\n",
    "        self.eval()\n",
    "        logits, loss = self.forward(pheno_sentence, SNPS_sentences, value='pheno')\n",
    "        return torch.argmax(logits, axis=1)\n",
    "        self.train()\n",
    "\n",
    "    def calcul_loss_pheno(self, logits, targets=None, loss_version='cross_entropy'):\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits_similarities_embed = self.Embedding_pheno.similarities_tab[targets-1] #-1 to get at the level of without padding\n",
    "            loss = F.cross_entropy(logits, logits_similarities_embed )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, dataloader_test):\n",
    "        print('beginning inference evaluation')\n",
    "        start_time_inference = time.time()\n",
    "        predicted_labels_list = []\n",
    "        predicted_probas_list = []\n",
    "        true_labels_list = []\n",
    "\n",
    "        total_loss = 0.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_sentences_pheno, batch_labels_pheno, batch_sentences_SNPS in dataloader_test:\n",
    "\n",
    "\n",
    "                logits, loss = self(batch_sentences_pheno, batch_sentences_SNPS,  value = 'pheno', targets=batch_labels_pheno)\n",
    "                total_loss += loss.item()\n",
    "                predicted_labels = self.predict_pheno(batch_sentences_pheno, batch_sentences_SNPS)\n",
    "                predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "                predicted_probas = F.softmax(logits, dim=1)\n",
    "                predicted_probas_list.extend(predicted_probas.cpu().numpy())\n",
    "                true_labels_list.extend(batch_labels_pheno.cpu().numpy())\n",
    "        f1 = f1_score(true_labels_list, predicted_labels_list, average='macro')\n",
    "        accuracy = accuracy_score(true_labels_list, predicted_labels_list)\n",
    "        auc_score = 0#calculate_roc_auc(true_labels_list, np.array(predicted_probas_list)[:, 1], return_nan=True)\n",
    "        proba_avg_zero, proba_avg_one = get_proba(true_labels_list, predicted_probas_list)\n",
    "    \n",
    "        self.train()\n",
    "        print(f'end inference evaluation in {time.time() - start_time_inference}s')\n",
    "        return f1, accuracy, auc_score, total_loss/len(dataloader_test), proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "rollup_depth = 4\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "decorelate = False\n",
    "equalize_label = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 50\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "compute_features = True\n",
    "padding = False\n",
    "list_env_features = ['age', 'sex']\n",
    "### data format\n",
    "batch_size = 20\n",
    "data_share = 1/1000\n",
    "\n",
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=True,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share)\n",
    "#patient_list = dataT.get_patientlist()\n",
    "patient_list = dataT.get_patientlist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_list.unpad_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_phenos = patient_list.get_nb_distinct_diseases_tot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pheno_truth = []\n",
    "list_labels = []\n",
    "list_diseases_sentence_masked = []\n",
    "for patient in patient_list[:5]:\n",
    "    diseases_sentence = torch.tensor(patient.diseases_sentence)\n",
    "    nb_diseases = len(diseases_sentence)\n",
    "    masks = np.zeros((nb_diseases, nb_diseases)).astype(bool)\n",
    "    np.fill_diagonal(masks,True)\n",
    "    diseases_sentence_masked = np.tile(diseases_sentence, nb_diseases).reshape(nb_diseases, nb_diseases)\n",
    "    pheno_Truth = diseases_sentence_masked[masks]\n",
    "    labels = [np.array([patient.SNP_label])]*nb_diseases\n",
    "    diseases_sentence_masked[masks] = nb_phenos \n",
    "\n",
    "    list_pheno_truth.extend(pheno_Truth)\n",
    "    list_labels.extend(labels)\n",
    "    list_diseases_sentence_masked.extend(diseases_sentence_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## padding the data\n",
    "list_diseases_new = []\n",
    "nb_max_distinct_diseases_patient= patient_list.get_nb_max_distinct_diseases_patient() \n",
    "for list_diseases in list_diseases_sentence_masked:\n",
    "    padd = np.zeros(nb_max_distinct_diseases_patient- len(list_diseases), dtype=int)\n",
    "    list_diseases_new.append(np.concatenate([list_diseases, padd]).astype(int))\n",
    "list_diseases_sentence_masked = list_diseases_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_gen = list(zip(list_diseases_sentence_masked, list_pheno_truth, list_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices= np.arange(len(list_data_gen))\n",
    "np.random.shuffle(indices)\n",
    "indices_train= indices[:int(prop_train_test * len(list_data_gen))]\n",
    "indices_test = indices[int(prop_train_test * len(list_data_gen)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = [list_data_gen[i] for i in indices_train]\n",
    "data_test = [list_data_gen[i] for i in indices_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_training = DataLoader(data_training, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollup_depth = 4\n",
    "Head_size_pheno = 4\n",
    "n_head_pheno = 2\n",
    "n_layer_pheno = 2\n",
    "instance_size_pheno = 10\n",
    "Embedding_size_pheno = 10\n",
    "embedding_method_pheno = None\n",
    "proj_embed_pheno = False\n",
    "freeze_embed_pheno = False\n",
    "loss_version_pheno = 'cross_entropy'\n",
    "p_dropout = 0.1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pheno_method = 'Abby'\n",
    "embedding_method_pheno = None\n",
    "embedding_method_SNPS = None\n",
    "freeze_embed_SNPS = False\n",
    "nb_phenos = patient_list.get_nb_distinct_diseases_tot()\n",
    "nb_SNPS = 2\n",
    "Embedding_size_SNPS = 10\n",
    "n_head_SNPS = 2\n",
    "Head_size_SNPS = 4\n",
    "loss_version_SNPS = 'cross_entropy'\n",
    "n_layer_SNPS = 2\n",
    "instance_size_SNPS = 10\n",
    "mask_padding = True\n",
    "#multi\n",
    "n_head_cross = 2\n",
    "Head_size_cross = 4\n",
    "n_layer_cross = 2\n",
    "instance_size_cross = 10\n",
    "\n",
    "nb_phenos_possible = patient_list.get_nb_distinct_diseases_tot()\n",
    "vocab_size = nb_phenos_possible + 1 # masking\n",
    "##### training constants\n",
    "total_epochs = 100# number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max\n",
    "warm_up_size = int(warm_up_frac*total_epochs)\n",
    "padding_masking = True\n",
    "\n",
    "eval_batch_interval = 40\n",
    "eval_epochs_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_pheno = EmbeddingPheno(method=embedding_method_pheno, vocab_size=vocab_size, Embedding_size=Embedding_size_pheno,\n",
    "     rollup_depth=rollup_depth, freeze_embed=freeze_embed_pheno, dicts=dataT.dicts)\n",
    "\n",
    "Embedding_SNPS = EmbeddingSNPS(method=embedding_method_SNPS, nb_SNPS=nb_SNPS, Embedding_size=Embedding_size_SNPS, freeze_embed=freeze_embed_SNPS)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GenerativeModelPheWasV1(n_head_pheno=n_head_pheno, Head_size_pheno=Head_size_pheno, Embedding_pheno=Embedding_pheno, Embedding_SNPS=Embedding_SNPS,\n",
    "    instance_size_pheno=instance_size_pheno, n_layer_pheno=n_layer_pheno,  nb_SNPS=nb_SNPS, n_layer_SNPS=n_layer_SNPS, n_head_SNPS=n_head_SNPS, mask_padding=mask_padding,\n",
    "    Head_size_SNPS=Head_size_SNPS, instance_size_SNPS=instance_size_SNPS, nb_phenos_possible=nb_phenos_possible,\n",
    "    n_head_cross=n_head_cross, Head_size_cross=Head_size_cross, n_layer_cross=n_layer_cross, p_dropout=p_dropout, device=device,\n",
    "    loss_version_pheno=loss_version_pheno, loss_version_SNPS=loss_version_SNPS, gamma=2, alpha=1, padding_token=padding_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate_max)\n",
    "lr_scheduler_warm_up = LinearLR(optimizer, start_factor=start_factor_lr , end_factor=1, total_iters=warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=total_epochs-warm_up_size, end_factor=end_factor_lr)\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[warm_up_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "start_time_training = time.time()\n",
    "print(f'Beginning of the program for {total_epochs} epochs')\n",
    "# Training Loop\n",
    "for epoch in range(1, total_epochs+1):\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    total_loss = 0.0  \n",
    "    \n",
    "    #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "    for k, (batch_sentences_pheno, batch_labels_pheno, batch_sentences_SNPS) in enumerate(dataloader_training):\n",
    "        start_time_batch = time.time()\n",
    "        \n",
    "        batch_sentences_pheno = batch_sentences_pheno.to(device)\n",
    "        batch_labels_pheno = batch_labels_pheno.to(device)\n",
    "        batch_sentences_SNPS = batch_sentences_SNPS.to(device)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(batch_sentences_pheno, batch_sentences_SNPS,value='pheno', targets= batch_labels_pheno)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "    \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if k % eval_batch_interval == 0:\n",
    "            print( f'Progress in epoch {epoch}  = {round(k / len(dataloader_training)*100, 2)} %, time batch : {time.time() - start_time_batch}')\n",
    "\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        f1_val, accuracy_val, auc_score_val, loss_val, proba_avg_zero_val, proba_avg_one_val, predicted_probas_list_val, true_labels_list_val = model.evaluate(dataloader_test)\n",
    "        print( f'epoch {epoch}, time epoch : {time.time() - start_time_epoch}')\n",
    "\n",
    "    \n",
    "    \n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_decomposed(batch_sentences_pheno,\n",
    "        batch_labels_pheno,\n",
    "        batch_sentences_SNPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_file = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/TestGene_output.txt'\n",
    "error_file  = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/TestGene_error.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makefile(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "start_time_training = time.time()\n",
    "print_file(output_file, f'Beginning of the program for {total_epochs} epochs')\n",
    "# Training Loop\n",
    "for epoch in range(1, total_epochs+1):\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    total_loss = 0.0  \n",
    "    \n",
    "    #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "    for k, (batch_sentences_pheno, batch_labels_pheno, batch_sentences_SNPS) in enumerate(dataloader_training):\n",
    "        start_time_batch = time.time()\n",
    "        \n",
    "        batch_sentences_pheno = batch_sentences_pheno.to(device)\n",
    "        batch_labels_pheno = batch_labels_pheno.to(device)\n",
    "        batch_sentences_SNPS = batch_sentences_SNPS.to(device)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(batch_sentences_pheno, batch_sentences_SNPS,value='pheno', targets= batch_labels_pheno)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "    \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if k % eval_batch_interval == 0:\n",
    "            clear_last_line(output_file)\n",
    "            print_file(output_file, f'Progress in epoch {epoch}  = {round(k / len(dataloader_training)*100, 2)} %, time batch : {time.time() - start_time_batch}')\n",
    "\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        f1_val, accuracy_val, auc_score_val, loss_val, proba_avg_zero_val, proba_avg_one_val, predicted_probas_list_val, true_labels_list_val = model.evaluate(dataloader_test)\n",
    "        print_file(output_file,  f'epoch {epoch}, time epoch : {time.time() - start_time_epoch}')\n",
    "\n",
    "    \n",
    "    \n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences_pheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(diseases_sentence, SNPS_sentence, 'f',targets=torch.tensor([0,1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_encoded = phenotype_encoding.forward(diseases_sentence)\n",
    "SNPS_encoded = SNP_encoding.forward(SNP_sentence)\n",
    "pheno_encoded.shape, SNP_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = crossattention.forward(pheno_encoded=pheno_encoded, SNPS_encoded=SNPS_encoded, value='phenos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_network = nn.Linear(instance_size_pheno, Head_size_cross, bias = False)\n",
    "values_network_SNPS = nn.Linear(instance_size_SNPS, Head_size_cross, bias = False)\n",
    "values_network_pheno= nn.Linear(instance_size_pheno, Head_size_cross, bias = False)\n",
    "\n",
    "queries_network = nn.Linear(instance_size_SNPS, Head_size_cross, bias = False)\n",
    "multi_head_size = Head_size_cross // n_head_cross\n",
    "\n",
    "Batch_len, Sentence_len_pheno, _ = pheno_encoded.shape\n",
    "Batch_len, Sentence_len_SNPS, _ = SNPS_encoded.shape\n",
    "Sentence_len_out = Sentence_len_pheno \n",
    "keys = keys_network(pheno_encoded)\n",
    "queries = queries_network(SNPS_encoded) \n",
    "values_pheno = values_network_pheno(pheno_encoded)\n",
    "values_SNPS = values_network_SNPS(SNPS_encoded)\n",
    "\n",
    "# add a dimension to compute the different attention heads separately\n",
    "q_multi_head = queries.view(Batch_len, Sentence_len_SNPS, n_head_cross, multi_head_size).transpose(1, 2) #shape B, HN, S_SNPS, MH\n",
    "k_multi_head = keys.view(Batch_len, Sentence_len_pheno, n_head_cross, multi_head_size).transpose(1, 2)#shape B, HN, S_PHENO, MH\n",
    "values_SNPS_multihead = values_SNPS.view(Batch_len, Sentence_len_SNPS, n_head_cross, multi_head_size).transpose(1, 2)\n",
    "values_pheno_multihead = values_pheno.view(Batch_len, Sentence_len_pheno, n_head_cross, multi_head_size).transpose(1, 2)\n",
    "\n",
    "\n",
    "attention_weights = (k_multi_head @ q_multi_head.transpose(-2, -1))/np.sqrt(multi_head_size) # shape B, HN, S_PHENO, S_SNPS\n",
    "attention_probas = F.softmax(attention_weights, dim=-1)\n",
    "out = attention_probas @ values_SNPS_multihead # shape B, S, S @ B, S, MH = B, S, MH\n",
    "out = out.transpose(2, 1).contiguous().view(Batch_len, Sentence_len_pheno, Head_size_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_pheno_multihead.shape, attention_probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = q_multi_head.transpose(-2, -1)\n",
    "u = k_multi_head @a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape, k_multi_head.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights.transpose(1, 2)..shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contiguous().view(Batch_len, Sentence_len_out, Head_size_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## padding mask #####\n",
    "if padding_mask != None:\n",
    "    attention_weights[padding_mask] = 1**-10     #float('-inf')\n",
    "#print(f'wei0={attention_weights}')\n",
    "attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "attention_probas = attention_dropout(attention_probas)\n",
    "\n",
    "#print(f'wei1={attention_probas}')\n",
    "#attention_probas = dropout(attention_probas)\n",
    "## weighted aggregation of the values\n",
    "out = attention_probas @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len_out, Head_size)\n",
    "out = proj(out)\n",
    "out = resid_dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def predict(self, diseases_sentence, diseases_count):\n",
    "        logits, _ = self(diseases_sentence, diseases_count) # shape B, Classes_nb\n",
    "        return torch.argmax(logits, dim=1)  # (B,)\n",
    "        \n",
    "    def predict_proba(self, diseases_sentence, diseases_count):\n",
    "        logits, _ = self(diseases_sentence, diseases_count)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        weights_logits[self.padding_mask_probas] = -torch.inf\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.Classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, targets, self.loss_version, self.gamma, self.alpha)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "x = torch.load(embedding_file_diseases)\n",
    "target = torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_embedding_loss(x, x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = F.cosine_similarity(x, x[1], dim=-1).view(1, 1718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Embedding_pheno.distinct_diseases_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences_pheno[batch_sentences_pheno==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x(batch_sentences_pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_tab = torch.tensor(np.array([F.cosine_similarity(x, x[i], dim=-1).view(1, 1718) for i in range(x.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = l[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.rand((2, 1718))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR, LinearLR, SequentialLR\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "from codes.models.Generative.Embeddings import EmbeddingPheno, EmbeddingSNPS\n",
    "from codes.models.Generative.GenerativeModel import GenerativeModelPheWasV1\n",
    "from codes.models.utils import print_file, plot_infos, plot_ini_infos, clear_last_line\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### data constants:\n",
    "model_type = 'Generative_Transformer'\n",
    "model_version = 'V1'\n",
    "test_name = 'tests_generative_1'\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "rollup_depth = 4\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "decorelate = False\n",
    "equalize_label = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 50\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "compute_features = True\n",
    "padding = False\n",
    "list_env_features = ['age', 'sex']\n",
    "### data format\n",
    "batch_size = 20\n",
    "data_share = 1/10000\n",
    "\n",
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=True,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share)\n",
    "#patient_list = dataT.get_patientlist()\n",
    "patient_list = dataT.get_patientlist()\n",
    "patient_list.unpad_data()\n",
    "\n",
    "\n",
    "rollup_depth = 4\n",
    "Head_size_pheno = 4\n",
    "n_head_pheno = 2\n",
    "n_layer_pheno = 2\n",
    "instance_size_pheno = 10\n",
    "Embedding_size_pheno = 10\n",
    "embedding_method_pheno = None\n",
    "proj_embed_pheno = False\n",
    "freeze_embed_pheno = False\n",
    "loss_version_pheno = 'cross_entropy'\n",
    "p_dropout = 0.1\n",
    "device = 'cpu'\n",
    "pheno_method = 'Abby'\n",
    "embedding_method_pheno = None\n",
    "embedding_method_SNPS = None\n",
    "freeze_embed_SNPS = False\n",
    "nb_phenos = patient_list.get_nb_distinct_diseases_tot()\n",
    "nb_SNPS = 2\n",
    "Embedding_size_SNPS = 10\n",
    "n_head_SNPS = 2\n",
    "Head_size_SNPS = 4\n",
    "loss_version_SNPS = 'cross_entropy'\n",
    "n_layer_SNPS = 2\n",
    "instance_size_SNPS = 10\n",
    "mask_padding = True\n",
    "#multi\n",
    "n_head_cross = 2\n",
    "Head_size_cross = 4\n",
    "n_layer_cross = 2\n",
    "instance_size_cross = 10\n",
    "\n",
    "nb_phenos_possible = patient_list.get_nb_distinct_diseases_tot()\n",
    "vocab_size = nb_phenos_possible + 1 # masking\n",
    "##### training constants\n",
    "total_epochs = 10# number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max\n",
    "warm_up_size = int(warm_up_frac*total_epochs)\n",
    "padding_masking = True\n",
    "\n",
    "eval_batch_interval = 40\n",
    "eval_epochs_interval = 1\n",
    "\n",
    "#################### generate the ouptut files and dirs ############################################\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "#check test name\n",
    "model_dir = path + f'logs/runs/SNPS/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}'\n",
    "model_plot_dir = path + f'logs/plots/tests/SNP/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}/'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(model_plot_dir, exist_ok=True)\n",
    "#check number tests\n",
    "test_dir = f'{model_dir}/{test_name}/'\n",
    "print(test_dir)\n",
    "log_data_dir = f'{test_dir}/data/'\n",
    "log_tensorboard_dir = f'{test_dir}/tensorboard/'\n",
    "log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "os.makedirs(log_data_dir, exist_ok=True)\n",
    "os.makedirs(log_tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(log_slurm_outputs_dir, exist_ok=True)\n",
    "os.makedirs(log_slurm_errors_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "log_data_path_pickle = f'{test_dir}/data/{test_name}.pkl'\n",
    "log_tensorboard_path = f'{test_dir}/tensorboard/{test_name}'\n",
    "log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{test_name}.txt'\n",
    "log_slurm_error_path = f'{test_dir}/Slurm/Errors/{test_name}.txt'\n",
    "model_plot_path = path + f'logs/plots/tests/SNP/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}/{test_name}.png'\n",
    "\n",
    "############ generate the masked list of diseases #############################################\n",
    "start_time = time.time()\n",
    "print('generating the data files')\n",
    "list_pheno_truth = []\n",
    "list_labels = []\n",
    "list_diseases_sentence_masked = []\n",
    "for patient in patient_list:\n",
    "    diseases_sentence = torch.tensor(patient.diseases_sentence)\n",
    "    nb_diseases = len(diseases_sentence)\n",
    "    masks = np.zeros((nb_diseases, nb_diseases)).astype(bool)\n",
    "    np.fill_diagonal(masks,True)\n",
    "    diseases_sentence_masked = np.tile(diseases_sentence, nb_diseases).reshape(nb_diseases, nb_diseases)\n",
    "    pheno_Truth = diseases_sentence_masked[masks]\n",
    "    labels = [np.array([patient.SNP_label])]*nb_diseases\n",
    "    diseases_sentence_masked[masks] = nb_phenos \n",
    "\n",
    "    list_pheno_truth.extend(pheno_Truth)\n",
    "    list_labels.extend(labels)\n",
    "    list_diseases_sentence_masked.extend(diseases_sentence_masked)\n",
    "print(f'generated files in {time.time() - start_time} seconds')\n",
    "\n",
    "################################### padding the data ###################################################\n",
    "list_diseases_new = []\n",
    "nb_max_distinct_diseases_patient= patient_list.get_nb_max_distinct_diseases_patient() \n",
    "for list_diseases in list_diseases_sentence_masked:\n",
    "    padd = np.zeros(nb_max_distinct_diseases_patient- len(list_diseases), dtype=int)\n",
    "    list_diseases_new.append(np.concatenate([list_diseases, padd]).astype(int))\n",
    "list_diseases_sentence_masked = list_diseases_new\n",
    "\n",
    "\n",
    "list_data_gen = list(zip(list_diseases_sentence_masked, list_pheno_truth, list_labels))\n",
    "indices= np.arange(len(list_data_gen))\n",
    "np.random.shuffle(indices)\n",
    "indices_train= indices[:int(prop_train_test * len(list_data_gen))]\n",
    "indices_test = indices[int(prop_train_test * len(list_data_gen)):]\n",
    "\n",
    "\n",
    "data_training = [list_data_gen[i] for i in indices_train]\n",
    "data_test = [list_data_gen[i] for i in indices_test]\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(data_training, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "Embedding_pheno = EmbeddingPheno(method=embedding_method_pheno, vocab_size=vocab_size, Embedding_size=Embedding_size_pheno,\n",
    "     rollup_depth=rollup_depth, freeze_embed=freeze_embed_pheno, dicts=dataT.dicts)\n",
    "\n",
    "Embedding_SNPS = EmbeddingSNPS(method=embedding_method_SNPS, nb_SNPS=nb_SNPS, Embedding_size=Embedding_size_SNPS, freeze_embed=freeze_embed_SNPS)\n",
    "    \n",
    "\n",
    "model = GenerativeModelPheWasV1(n_head_pheno=n_head_pheno, Head_size_pheno=Head_size_pheno, Embedding_pheno=Embedding_pheno, Embedding_SNPS=Embedding_SNPS,\n",
    "    instance_size_pheno=instance_size_pheno, n_layer_pheno=n_layer_pheno,  nb_SNPS=nb_SNPS, n_layer_SNPS=n_layer_SNPS, n_head_SNPS=n_head_SNPS, mask_padding=mask_padding,\n",
    "    Head_size_SNPS=Head_size_SNPS, instance_size_SNPS=instance_size_SNPS, nb_phenos_possible=nb_phenos_possible,\n",
    "    n_head_cross=n_head_cross, Head_size_cross=Head_size_cross, n_layer_cross=n_layer_cross, p_dropout=p_dropout, device=device,\n",
    "    loss_version_pheno=loss_version_pheno, loss_version_SNPS=loss_version_SNPS, gamma=2, alpha=1, padding_token=padding_token)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate_max)\n",
    "lr_scheduler_warm_up = LinearLR(optimizer, start_factor=start_factor_lr , end_factor=1, total_iters=warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=total_epochs-warm_up_size, end_factor=end_factor_lr)\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[warm_up_size])\n",
    "\n",
    "\n",
    "######################################################## Training Loop ###################################################\n",
    "output_file = log_slurm_outputs_path\n",
    "writer = SummaryWriter(log_tensorboard_path)\n",
    "\n",
    "## Open tensor board writer\n",
    "dic_features_list = {\n",
    "'list_training_loss' : [],\n",
    "'list_validation_loss' : [],\n",
    "'list_proba_avg_zero' : [],\n",
    "'list_proba_avg_one' : [],\n",
    "'list_auc_validation' : [],\n",
    "'list_accuracy_validation' : [],\n",
    "'list_f1_validation' : [],\n",
    "'epochs' : [] }\n",
    "\n",
    "# Training Loop\n",
    "start_time_training = time.time()\n",
    "print_file(output_file, f'Beginning of the program for {total_epochs} epochs', new_line=True)\n",
    "# Training Loop\n",
    "plot_ini_infos(model, output_file, dataloader_test, dataloader_train, writer, dic_features_list)\n",
    "for epoch in range(1, total_epochs+1):\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    total_loss = 0.0  \n",
    "    \n",
    "    #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "    for k, (batch_sentences_pheno, batch_labels_pheno, batch_sentences_SNPS) in enumerate(dataloader_train):\n",
    "        start_time_batch = time.time()\n",
    "        \n",
    "        batch_sentences_pheno = batch_sentences_pheno.to(device)\n",
    "        batch_labels_pheno = batch_labels_pheno.to(device)\n",
    "        batch_sentences_SNPS = batch_sentences_SNPS.to(device)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(batch_sentences_pheno, batch_sentences_SNPS,value='pheno', targets= batch_labels_pheno)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "    \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if k % eval_batch_interval == 0:\n",
    "            clear_last_line(output_file)\n",
    "            print_file(output_file, f'Progress in epoch {epoch}  = {round(k / len(dataloader_train)*100, 2)} %, time batch : {time.time() - start_time_epoch}', new_line=False)\n",
    "\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        dic_features = plot_infos(model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, model_plot_path)\n",
    "\n",
    "    \n",
    "    \n",
    "    lr_scheduler.step()\n",
    "\n",
    "dic_features = dic_features\n",
    "model.to('cpu')\n",
    "#model.write_embedding(writer)\n",
    "# Print time\n",
    "print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Add hyper parameters to tensorboard\n",
    "hyperparams = {\"CHR\" : CHR, \"SNP\" : SNP, \"ROLLUP LEVEL\" : rollup_depth,\n",
    "            'PHENO_METHOD': pheno_method, 'EMBEDDING_METHOD': embedding_method_pheno,\n",
    "            'EMBEDDING SIZE' : Embedding_size_pheno, 'ATTENTION HEADS' : n_head_pheno, 'BLOCKS' : n_layer_pheno,\n",
    "            'LR':1 , 'DROPOUT' : p_dropout, 'NUM_EPOCHS' : total_epochs, \n",
    "            'BATCH_SIZE' : batch_size, \n",
    "            'PADDING_MASKING': padding_masking,\n",
    "            'VERSION' : model_version,\n",
    "            'NB_Patients'  : len(patient_list),\n",
    "            'LOSS_VERSION'  : loss_version_pheno,\n",
    "            }\n",
    "\n",
    "writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_sentence == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_diseases in list_diseases_sentence_masked:\n",
    "    padd = np.zeros(nb_max_distinct_diseases_patient- len(list_diseases), dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_max_distinct_diseases_patient- len(list_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# test.py file\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR, LinearLR, SequentialLR\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "from codes.models.Generative.Embeddings import EmbeddingPheno, EmbeddingSNPS\n",
    "from codes.models.Generative.GenerativeModel import GenerativeModelPheWasV1\n",
    "from codes.models.utils import print_file, plot_infos, plot_ini_infos, clear_last_line\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### data constants:\n",
    "model_type = 'Generative_Transformer'\n",
    "model_version = 'V1'\n",
    "test_name = 'tests_generative_1'\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "rollup_depth = 4\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "decorelate = False\n",
    "equalize_label = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 50\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "compute_features = True\n",
    "padding = False\n",
    "list_env_features = ['age', 'sex']\n",
    "### data format\n",
    "batch_size = 200\n",
    "data_share = 1\n",
    "\n",
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=True,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share)\n",
    "#patient_list = dataT.get_patientlist()\n",
    "patient_list = dataT.get_patientlist()\n",
    "patient_list.unpad_data()\n",
    "\n",
    "\n",
    "rollup_depth = 4\n",
    "Head_size_pheno = 4\n",
    "n_head_pheno = 2\n",
    "n_layer_pheno = 2\n",
    "instance_size_pheno = 10\n",
    "Embedding_size_pheno = 10\n",
    "embedding_method_pheno = None\n",
    "proj_embed_pheno = False\n",
    "freeze_embed_pheno = False\n",
    "loss_version_pheno = 'cross_entropy'\n",
    "p_dropout = 0.1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pheno_method = 'Abby'\n",
    "embedding_method_pheno = None\n",
    "embedding_method_SNPS = None\n",
    "freeze_embed_SNPS = False\n",
    "nb_phenos = patient_list.get_nb_distinct_diseases_tot()\n",
    "nb_SNPS = 2\n",
    "Embedding_size_SNPS = 10\n",
    "n_head_SNPS = 2\n",
    "Head_size_SNPS = 4\n",
    "loss_version_SNPS = 'cross_entropy'\n",
    "n_layer_SNPS = 2\n",
    "instance_size_SNPS = 10\n",
    "mask_padding = True\n",
    "#multi\n",
    "n_head_cross = 2\n",
    "Head_size_cross = 4\n",
    "n_layer_cross = 2\n",
    "instance_size_cross = 10\n",
    "\n",
    "nb_phenos_possible = patient_list.get_nb_distinct_diseases_tot()\n",
    "vocab_size = nb_phenos_possible + 1 # masking\n",
    "##### training constants\n",
    "total_epochs = 100# number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max\n",
    "warm_up_size = int(warm_up_frac*total_epochs)\n",
    "padding_masking = True\n",
    "\n",
    "eval_batch_interval = 40\n",
    "eval_epochs_interval = 1\n",
    "\n",
    "#################### generate the ouptut files and dirs ############################################\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "#check test name\n",
    "model_dir = path + f'logs/runs/SNPS/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}'\n",
    "model_plot_dir = path + f'logs/plots/tests/SNP/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}/'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(model_plot_dir, exist_ok=True)\n",
    "#check number tests\n",
    "test_dir = f'{model_dir}/{test_name}/'\n",
    "print(test_dir)\n",
    "log_data_dir = f'{test_dir}/data/'\n",
    "log_tensorboard_dir = f'{test_dir}/tensorboard/'\n",
    "log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "os.makedirs(log_data_dir, exist_ok=True)\n",
    "os.makedirs(log_tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(log_slurm_outputs_dir, exist_ok=True)\n",
    "os.makedirs(log_slurm_errors_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "log_data_path_pickle = f'{test_dir}/data/{test_name}.pkl'\n",
    "log_tensorboard_path = f'{test_dir}/tensorboard/{test_name}'\n",
    "log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{test_name}.txt'\n",
    "log_slurm_error_path = f'{test_dir}/Slurm/Errors/{test_name}.txt'\n",
    "model_plot_path = path + f'logs/plots/tests/SNP/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}/{test_name}.png'\n",
    "\n",
    "sys.stdrerr = log_slurm_error_path\n",
    "\n",
    "############ generate the masked list of diseases #############################################\n",
    "start_time = time.time()\n",
    "print('generating the data files')\n",
    "list_pheno_truth = []\n",
    "list_labels = []\n",
    "list_diseases_sentence_masked = []\n",
    "for patient in patient_list:\n",
    "    diseases_sentence = torch.tensor(patient.diseases_sentence)\n",
    "    nb_diseases = len(diseases_sentence)\n",
    "    masks = np.zeros((nb_diseases, nb_diseases)).astype(bool)\n",
    "    np.fill_diagonal(masks,True)\n",
    "    diseases_sentence_masked = np.tile(diseases_sentence, nb_diseases).reshape(nb_diseases, nb_diseases)\n",
    "    pheno_Truth = diseases_sentence_masked[masks]\n",
    "    labels = [np.array([patient.SNP_label])]*nb_diseases\n",
    "    diseases_sentence_masked[masks] = nb_phenos \n",
    "\n",
    "    list_pheno_truth.extend(pheno_Truth)\n",
    "    list_labels.extend(labels)\n",
    "    list_diseases_sentence_masked.extend(diseases_sentence_masked)\n",
    "print(f'generated files in {time.time() - start_time} seconds')\n",
    "\n",
    "################################### padding the data ###################################################\n",
    "list_diseases_new = []\n",
    "nb_max_distinct_diseases_patient= patient_list.get_nb_max_distinct_diseases_patient() \n",
    "for list_diseases in list_diseases_sentence_masked:\n",
    "    padd = np.zeros(nb_max_distinct_diseases_patient- len(list_diseases), dtype=int)\n",
    "    list_diseases_new.append(np.concatenate([list_diseases, padd]).astype(int))\n",
    "list_diseases_sentence_masked = list_diseases_new\n",
    "\n",
    "\n",
    "list_data_gen = list(zip(list_diseases_sentence_masked, list_pheno_truth, list_labels))\n",
    "indices= np.arange(len(list_data_gen))\n",
    "np.random.shuffle(indices)\n",
    "indices_train= indices[:int(prop_train_test * len(list_data_gen))]\n",
    "indices_test = indices[int(prop_train_test * len(list_data_gen)):]\n",
    "\n",
    "\n",
    "data_training = [list_data_gen[i] for i in indices_train]\n",
    "data_test = [list_data_gen[i] for i in indices_test]\n",
    "\n",
    "print(len(data_training), flush=True)\n",
    "dataloader_train = DataLoader(data_training, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "Embedding_pheno = EmbeddingPheno(method=embedding_method_pheno, vocab_size=vocab_size, Embedding_size=Embedding_size_pheno,\n",
    "     rollup_depth=rollup_depth, freeze_embed=freeze_embed_pheno, dicts=dataT.dicts)\n",
    "\n",
    "Embedding_SNPS = EmbeddingSNPS(method=embedding_method_SNPS, nb_SNPS=nb_SNPS, Embedding_size=Embedding_size_SNPS, freeze_embed=freeze_embed_SNPS)\n",
    "    \n",
    "\n",
    "model = GenerativeModelPheWasV1(n_head_pheno=n_head_pheno, Head_size_pheno=Head_size_pheno, Embedding_pheno=Embedding_pheno, Embedding_SNPS=Embedding_SNPS,\n",
    "    instance_size_pheno=instance_size_pheno, n_layer_pheno=n_layer_pheno,  nb_SNPS=nb_SNPS, n_layer_SNPS=n_layer_SNPS, n_head_SNPS=n_head_SNPS, mask_padding=mask_padding,\n",
    "    Head_size_SNPS=Head_size_SNPS, instance_size_SNPS=instance_size_SNPS, nb_phenos_possible=nb_phenos_possible,\n",
    "    n_head_cross=n_head_cross, Head_size_cross=Head_size_cross, n_layer_cross=n_layer_cross, p_dropout=p_dropout, device=device,\n",
    "    loss_version_pheno=loss_version_pheno, loss_version_SNPS=loss_version_SNPS, gamma=2, alpha=1, padding_token=padding_token)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate_max)\n",
    "lr_scheduler_warm_up = LinearLR(optimizer, start_factor=start_factor_lr , end_factor=1, total_iters=warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=total_epochs-warm_up_size, end_factor=end_factor_lr)\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[warm_up_size])\n",
    "\n",
    "\n",
    "######################################################## Training Loop ###################################################\n",
    "output_file = log_slurm_outputs_path\n",
    "writer = SummaryWriter(log_tensorboard_path)\n",
    "\n",
    "## Open tensor board writer\n",
    "dic_features_list = {\n",
    "'list_training_loss' : [],\n",
    "'list_validation_loss' : [],\n",
    "'list_proba_avg_zero' : [],\n",
    "'list_proba_avg_one' : [],\n",
    "'list_auc_validation' : [],\n",
    "'list_accuracy_validation' : [],\n",
    "'list_f1_validation' : [],\n",
    "'epochs' : [] }\n",
    "\n",
    "# Training Loop\n",
    "start_time_training = time.time()\n",
    "print_file(output_file, f'Beginning of the program for {total_epochs} epochs', new_line=True)\n",
    "# Training Loop\n",
    "plot_ini_infos(model, output_file, dataloader_test, dataloader_train, writer, dic_features_list)\n",
    "for epoch in range(1, total_epochs+1):\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    total_loss = 0.0  \n",
    "    \n",
    "    #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "    for k, (batch_sentences_pheno, batch_labels_pheno, batch_sentences_SNPS) in enumerate(dataloader_train):\n",
    "        start_time_batch = time.time()\n",
    "        \n",
    "        batch_sentences_pheno = batch_sentences_pheno.to(device)\n",
    "        batch_labels_pheno = batch_labels_pheno.to(device)\n",
    "        batch_sentences_SNPS = batch_sentences_SNPS.to(device)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(batch_sentences_pheno, batch_sentences_SNPS,value='pheno', targets= batch_labels_pheno)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "    \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if k % eval_batch_interval == 0:\n",
    "            clear_last_line(output_file)\n",
    "            print_file(output_file, f'Progress in epoch {epoch}  = {round(k / len(dataloader_train)*100, 2)} %, time batch : {time.time() - start_time_epoch}', new_line=False)\n",
    "\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        dic_features = plot_infos(model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, model_plot_path)\n",
    "\n",
    "    \n",
    "    \n",
    "    lr_scheduler.step()\n",
    "\n",
    "dic_features = dic_features\n",
    "model.to('cpu')\n",
    "#model.write_embedding(writer)\n",
    "# Print time\n",
    "print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Add hyper parameters to tensorboard\n",
    "hyperparams = {\"CHR\" : CHR, \"SNP\" : SNP, \"ROLLUP LEVEL\" : rollup_depth,\n",
    "            'PHENO_METHOD': pheno_method, 'EMBEDDING_METHOD': embedding_method_pheno,\n",
    "            'EMBEDDING SIZE' : Embedding_size_pheno, 'ATTENTION HEADS' : n_head_pheno, 'BLOCKS' : n_layer_pheno,\n",
    "            'LR':1 , 'DROPOUT' : p_dropout, 'NUM_EPOCHS' : total_epochs, \n",
    "            'BATCH_SIZE' : batch_size, \n",
    "            'PADDING_MASKING': padding_masking,\n",
    "            'VERSION' : model_version,\n",
    "            'NB_Patients'  : len(patient_list),\n",
    "            'LOSS_VERSION'  : loss_version_pheno,\n",
    "            }\n",
    "\n",
    "writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Embedding_pheno.similarities_tab.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Embedding_pheno.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
