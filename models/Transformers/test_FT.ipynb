{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functionnal version with optionnal mask padding and dropouts, see Transformer_V1.ipynb for example\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "\n",
    "### imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList, Patient\n",
    "\n",
    "from codes.models.Transformers.Embedding import EmbeddingPhenoCat\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "from torch.utils.data import DataLoader\n",
    "### Transformer's instance\n",
    "# B, S, E, H, HN, MH = Batch_len, Sentence_len, Embedding_len, Head_size, Head number, MultiHead size.\n",
    "class TabTransformerGeneModel_V2(nn.Module):\n",
    "    def __init__(self, pheno_method, Embedding, instance_size, proj_embed, list_env_features, Head_size, binary_classes, n_head, n_layer, mask_padding=False, padding_token=None, p_dropout=0, device='cpu', loss_version='cross_entropy', gamma=2, alpha=1):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.Embedding_size = Embedding.Embedding_size\n",
    "        \n",
    "\n",
    "        self.mask_padding = mask_padding\n",
    "        self.padding_token = padding_token\n",
    "        self.padding_mask = None\n",
    "        self.device = device\n",
    "        self.pheno_method = pheno_method\n",
    "        self.binary_classes = binary_classes\n",
    "        self.Classes_nb = 2 if self.binary_classes else 3\n",
    "        self.loss_version = loss_version\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.list_env_features = list_env_features\n",
    "        self.nb_env = len(self.list_env_features)\n",
    "        self.instance_size = instance_size\n",
    "        self.Embedding = Embedding\n",
    "        self.proj_embed = proj_embed\n",
    "        if not self.proj_embed:\n",
    "            self.instance_size = self.Embedding_size\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size)\n",
    "        \n",
    "        self.blocks =PadMaskSequential(*[Block(self.instance_size, n_head=n_head, Head_size=Head_size, p_dropout=p_dropout, nb_env=self.nb_env) for _ in range(n_layer)]) #Block(self.instance_size, n_head=n_head, Head_size=Head_size) \n",
    "        self.ln_f = nn.LayerNorm(self.instance_size) # final layer norm\n",
    "        self.lm_head_logits = nn.Linear(self.instance_size, self.Classes_nb) \n",
    "        self.lm_head_proba = nn.Linear(self.instance_size,1) # plus one for the probabilities\n",
    "\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, input_dict):\n",
    "        diseases_sentence = input_dict['diseases']\n",
    "        B, S = np.shape(diseases_sentence)[0], np.shape(diseases_sentence)[1]\n",
    "        mask = torch.where(diseases_sentence==self.padding_token)\n",
    "        padding_mask_mat = torch.ones((B, S+self.nb_env, S+self.nb_env))\n",
    "        padding_mask_mat[mask] = 0\n",
    "        padding_mask_mat.transpose(-2,-1)[mask] = 0\n",
    "\n",
    "        padding_mask_probas = torch.ones((B, S+self.nb_env))\n",
    "        padding_mask_probas[mask] = 0\n",
    "        padding_mask_probas = padding_mask_probas.view(B, S+self.nb_env)\n",
    "        return padding_mask_mat.to(self.device), padding_mask_probas.to(self.device) # 1 if masked, 0 else\n",
    "\n",
    "    def set_padding_mask_transformer(self, padding_mask_mat, padding_mask_probas):\n",
    "        self.padding_mask_mat = padding_mask_mat\n",
    "        self.padding_mask_probas = padding_mask_probas\n",
    "    \n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        for key, value in input_dict.items():\n",
    "            input_dict[key] = value.to(self.device)\n",
    "\n",
    "        if 'SNP_label' in list(input_dict.keys()):\n",
    "            targets = input_dict.pop('SNP_label')\n",
    "        else:\n",
    "            targets = None\n",
    "        input_embedded = self.Embedding(input_dict)\n",
    "        Batch_len, Sentence_len, _ = input_embedded.shape\n",
    "\n",
    "   \n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask_mat, padding_mask_probas = self.create_padding_mask(input_dict)\n",
    "            self.set_padding_mask_transformer(padding_mask_mat, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask_mat)\n",
    "\n",
    "        \n",
    "        x = self.blocks(input_embedded) # shape B, S, E\n",
    "        x = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        probas = probas * self.padding_mask_probas\n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.Classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, targets, self.loss_version, self.gamma, self.alpha)\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def forward_decomposed(self, input_dict):\n",
    "        for key, value in input_dict.items():\n",
    "            input_dict[key] = value.to(self.device)\n",
    "            \n",
    "        if 'SNP_label' in list(input_dict.keys()):\n",
    "            targets = input_dict.pop('SNP_label')\n",
    "        else:\n",
    "            targets = None\n",
    "        input_embedded = self.Embedding(input_dict)\n",
    "        Batch_len, Sentence_len, _ = input_embedded.shape\n",
    "\n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask_mat, padding_mask_probas = self.create_padding_mask(input_dict)\n",
    "            self.set_padding_mask_transformer(padding_mask_mat, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask_mat)\n",
    "\n",
    "        \n",
    "        x, attention_probas = self.blocks.forward_decompose(input_embedded) # shape B, S, E\n",
    "        x = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        probas = probas * self.padding_mask_probas\n",
    "\n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.Classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, targets, self.loss_version, self.gamma, self.alpha)\n",
    "        return logits, loss, input_embedded, attention_probas, probas\n",
    "\n",
    "    def predict(self,input_dict):\n",
    "        if 'SNP_label' in list(input_dict.keys()):\n",
    "            input_dict.pop('SNP_label')\n",
    "        logits, _ = self(input_dict) # shape B, Classes_nb\n",
    "        return torch.argmax(logits, dim=1)  # (B,)\n",
    "        \n",
    "    def predict_proba(self, input_dict):\n",
    "        if 'SNP_label' in list(input_dict.keys()):\n",
    "            input_dict.pop('SNP_label')\n",
    "        logits, _ = self(input_dict)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate(self, dataloader_test):\n",
    "        print('beginning inference evaluation')\n",
    "        start_time_inference = time.time()\n",
    "        predicted_labels_list = []\n",
    "        predicted_probas_list = []\n",
    "        true_labels_list = []\n",
    "\n",
    "        total_loss = 0.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_dicts in dataloader_test:\n",
    "\n",
    "                batch_labels = input_dicts['SNP_label']\n",
    "\n",
    "                logits, loss = self(input_dicts)\n",
    "                total_loss += loss.item()\n",
    "                predicted_labels = self.predict(input_dicts)\n",
    "                predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "                predicted_probas = self.predict_proba(input_dicts)\n",
    "                predicted_probas_list.extend(predicted_probas.cpu().numpy())\n",
    "                true_labels_list.extend(batch_labels.cpu().numpy())\n",
    "        f1 = f1_score(true_labels_list, predicted_labels_list, average='macro')\n",
    "        accuracy = accuracy_score(true_labels_list, predicted_labels_list)\n",
    "        auc_score = calculate_roc_auc(true_labels_list, np.array(predicted_probas_list)[:, 1], return_nan=True)\n",
    "        proba_avg_zero, proba_avg_one = get_proba(true_labels_list, predicted_probas_list)\n",
    "    \n",
    "        self.train()\n",
    "        print(f'end inference evaluation in {time.time() - start_time_inference}s')\n",
    "        return f1, accuracy, auc_score, total_loss/len(dataloader_test), proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list\n",
    "\n",
    "\n",
    "    def write_embedding(self, writer):\n",
    "        if self.proj_embed:\n",
    "            embedding_tensor = self.projection_embed(self.Embedding.dic_embedding_cat['diseases'].weight).detach().cpu().numpy()\n",
    "        else:\n",
    "            embedding_tensor = self.Embedding.dic_embedding_cat['diseases'].weight.detach().cpu().numpy()\n",
    "        writer.add_embedding(embedding_tensor, metadata=self.Embedding.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class PadMaskSequential(nn.Sequential):\n",
    "    def __init__(self, *args):\n",
    "        super(PadMaskSequential, self).__init__(*args)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sequential(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module.forward_decompose(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, instance_size, n_head, Head_size, p_dropout, nb_env):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadSelfAttention(n_head, Head_size, instance_size, p_dropout,  nb_env=nb_env)\n",
    "        self.ffwd = FeedForward(instance_size, p_dropout)\n",
    "        self.ln1 = nn.LayerNorm(instance_size)\n",
    "        self.ln2 = nn.LayerNorm(instance_size)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_block(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        #x = self.ln1(x)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        out_sa, attention_probas= self.sa.forward_decompose(x)\n",
    "        x = out_sa + x\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x, attention_probas\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, Head_size, instance_size, p_dropout, nb_env):\n",
    "        super().__init__()\n",
    "        self.q_network = nn.Linear(instance_size, Head_size, bias = False) \n",
    "        self.k_network =  nn.Linear(instance_size, Head_size, bias = False)\n",
    "        self.v_network =  nn.Linear(instance_size, Head_size, bias = False)\n",
    "        self.proj = nn.Linear(Head_size, instance_size)\n",
    "        self.attention_dropout = nn.Dropout(p_dropout)\n",
    "        self.resid_dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "        self.multi_head_size = Head_size // n_head\n",
    "        self.flash = False\n",
    "        self.n_head = n_head\n",
    "        self.Head_size = Head_size\n",
    "        self.padding_mask = None\n",
    "        self.nb_env = nb_env\n",
    "\n",
    "    def set_padding_mask_sa(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "\n",
    "        q = self.q_network(x)\n",
    "        k = self.k_network(x)\n",
    "        v = self.v_network(x)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas = self.attention_dropout(attention_probas)\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas @ v_multi_head # shape B, S-Env, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out        \n",
    "\n",
    "    def forward_decompose(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "\n",
    "        q = self.q_network(x)\n",
    "        k = self.k_network(x)\n",
    "        v = self.v_network(x)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            ### padding mask #####\n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas = self.attention_dropout(attention_probas)\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out, attention_probas     \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear( instance_size, 4 * instance_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * instance_size, instance_size),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding')\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SineCosineEncoding(nn.Module):\n",
    "    def __init__(self, Embedding_size, max_len=1000):\n",
    "        super(SineCosineEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, Embedding_size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, Embedding_size, 2).float() * -(np.log(10000.0) / Embedding_size))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding.to(x.device)[x]\n",
    "\n",
    "class ZeroEmbedding(nn.Module):\n",
    "    def __init__(self, Embedding_size, max_len=1000):\n",
    "        super(ZeroEmbedding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, Embedding_size)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding.to(x.device)[x]\n",
    "\n",
    "\n",
    "class EmbeddingPheno(nn.Module):\n",
    "    def __init__(self, method=None, counts_method=None, vocab_size=None, max_count_same_disease=None, Embedding_size=None, rollup_depth=4, freeze_embed=False, dicts=None):\n",
    "        super(EmbeddingPheno, self).__init__()\n",
    "\n",
    "        self.dicts = dicts\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.nb_distinct_diseases_patient = vocab_size\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.metadata = None\n",
    "        self.counts_method = counts_method\n",
    "\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        \n",
    "        if method == None:\n",
    "            self.distinct_diseases_embeddings = nn.Embedding(vocab_size, Embedding_size)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, Embedding_size)\n",
    "            torch.nn.init.normal_(self.distinct_diseases_embeddings.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.normal_(self.counts_embeddings.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        elif method == 'Abby':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "\n",
    "\n",
    "\n",
    "        elif method=='Paul':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            if self.counts_method == 'SineCosine':\n",
    "                self.counts_embeddings = SineCosineEncoding(self.Embedding_size, max_count_same_disease)\n",
    "            elif self.counts_method == 'no_counts':\n",
    "                self.counts_embeddings = ZeroEmbedding(self.Embedding_size, max_count_same_disease )\n",
    "            else:\n",
    "\n",
    "                self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "    def write_embedding(self, writer):\n",
    "            embedding_tensor = self.distinct_diseases_embeddings.weight.data.detach().cpu().numpy()\n",
    "            writer.add_embedding(embedding_tensor, metadata=self.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class EmbeddingPhenoCat(nn.Module):\n",
    "    def __init__(self, pheno_method=None,  method=None, proj_embed=None, counts_method=None, Embedding_size=10, instance_size=10, rollup_depth=4, freeze_embed=False, dic_embedding_cat_params={}, dicts=None, device='cpu'):\n",
    "        super(EmbeddingPhenoCat, self).__init__()\n",
    "\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.dic_embedding_cat_params = dic_embedding_cat_params\n",
    "        dic_embedding_cat = {}\n",
    "        self.method = method\n",
    "        self.pheno_method = pheno_method\n",
    "        self.dicts = dicts\n",
    "        self.proj_embed = proj_embed\n",
    "        self.projection_embed = None\n",
    "        self.instance_size = instance_size\n",
    "        self.counts_method = counts_method\n",
    "\n",
    "        self.device = device\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        for cat, max_number  in self.dic_embedding_cat_params.items():\n",
    "        \n",
    "            if cat=='diseases':\n",
    "                if self.method == None:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, Embedding_size)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                elif self.method == 'Abby':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed).to(self.device)\n",
    "\n",
    "            \n",
    "\n",
    "                elif self.method=='Paul':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed).to(self.device)\n",
    "                    \n",
    "            elif cat == 'counts':\n",
    "                if self.pheno_method == 'Paul':\n",
    "                    if self.counts_method[cat] == 'SineCosine':\n",
    "                        dic_embedding_cat[cat] = SineCosineEncoding(self.instance_size, max_number).to(self.device)\n",
    "                    elif self.counts_method[cat] == 'no_counts':\n",
    "                        dic_embedding_cat[cat] = ZeroEmbedding(self.instance_size, max_number).to(self.device)\n",
    "                    else:\n",
    "                        dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                        torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "            elif cat == 'age':\n",
    "                if self.counts_method[cat] == 'SineCosine':\n",
    "                    dic_embedding_cat[cat] = SineCosineEncoding(self.instance_size, max_number).to(self.device)\n",
    "                elif self.counts_method[cat] == 'no_counts':\n",
    "                    dic_embedding_cat[cat] = ZeroEmbedding(self.instance_size, max_number).to(self.device)\n",
    "                else:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size).to(self.device)\n",
    "\n",
    "        self.dic_embedding_cat = dic_embedding_cat\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        list_env_embedded = []\n",
    "        for key, value in input_dict.items():\n",
    "            \n",
    "            batch_len = len(value)\n",
    "\n",
    "            if key=='diseases':\n",
    "                diseases_sentences_embedded = self.dic_embedding_cat[key](value)\n",
    "                if self.proj_embed:\n",
    "                    diseases_sentences_embedded = self.projection_embed(diseases_sentences_embedded)\n",
    "\n",
    "            elif key=='counts':\n",
    "                if self.pheno_method == 'Paul':\n",
    "                    counts_sentence_embedded = self.dic_embedding_cat[key](value)\n",
    "                    diseases_sentences_embedded = diseases_sentences_embedded + counts_sentence_embedded\n",
    "            \n",
    "\n",
    "            else:\n",
    "                list_env_embedded.append(self.dic_embedding_cat[key](value).view(batch_len, 1, self.instance_size))\n",
    "\n",
    "        env_embedded = torch.concat(list_env_embedded, dim=1)\n",
    "\n",
    "        return torch.concat([diseases_sentences_embedded, env_embedded], dim=1)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creation of the reference model\n",
    "#### framework constants:\n",
    "model_type = 'tab_transformer'\n",
    "model_version = 'transformer_V2'\n",
    "test_name = 'baseline_model_focal'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "tryout = False # True if we are doing a tryout, False otherwise \n",
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "rollup_depth = 4\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "compute_features = False\n",
    "padding = True\n",
    "list_env_features = ['age', 'sex']\n",
    "### data format\n",
    "batch_size = 200\n",
    "data_share = 1/1000#402555\n",
    "seuil_diseases = 600\n",
    "equalize_label = False\n",
    "decorelate = False\n",
    "threshold_corr = 1\n",
    "threshold_rare = 1000\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "##### model constants\n",
    "embedding_method = 'Abby' #None, Paul, Abby\n",
    "freeze_embedding = True\n",
    "Embedding_size = 4 # Size of embedding.\n",
    "proj_embed = True\n",
    "instance_size = 10\n",
    "n_head = 2 # number of SA heads\n",
    "n_layer = 1 # number of blocks in parallel\n",
    "Head_size = 4 # size of the \"single Attention head\", which is the sum of the size of all multi Attention heads\n",
    "eval_epochs_interval = 5 # number of epoch between each evaluation print of the model (no impact on results)\n",
    "eval_batch_interval = 40\n",
    "p_dropout = 0.3 # proba of dropouts in the model\n",
    "masking_padding = True # do we include padding masking or not\n",
    "loss_version = 'cross_entropy' #cross_entropy or focal_loss\n",
    "gamma = 2\n",
    "alpha = 63\n",
    "##### training constants\n",
    "total_epochs = 100 # number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=True,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share)\n",
    "#patient_list = dataT.get_patientlist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.models.data_form.DataSets import TabDictDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_data= dataT.get_data_tabtransfo(actualise_phenos=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train, indices_test = dataT.get_indices_train_test(nb_data=len(dic_data['diseases']))\n",
    "\n",
    "dic_data_train = {key: np.array(dic_data[key])[indices_train] for key in dic_data.keys()}\n",
    "dic_data_test = {key: np.array(dic_data[key])[indices_test] for key in dic_data.keys()}\n",
    "\n",
    "max_number_diseases = len(dataT.dicts['id'] ) \n",
    "max_number_counts = np.max([np.max(dic_data['counts'][k]) for k in range(len(dic_data['counts']))]) + 1\n",
    "max_number_age = np.max(np.array(dic_data['age'])) + 1\n",
    "max_number_sex = 2\n",
    "dic_embedding_cat_params = {'diseases':max_number_diseases, 'counts':max_number_counts, 'age':max_number_age, 'sex':max_number_sex} \n",
    "\n",
    "dataset_train = TabDictDataset(dic_data_train)\n",
    "dataset_test = TabDictDataset(dic_data_test)\n",
    "\n",
    "dataloader_train =  DataLoader(dataset_train, batch_size = batch_size, shuffle=True)\n",
    "dataloader_test =  DataLoader(dataset_test, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "dataloader_train = dataloader_train\n",
    "dataloader_test = dataloader_test\n",
    "vocab_size = max_number_diseases\n",
    "max_count_same_disease =max_number_diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_embedding_cat_params = {'diseases':max_number_diseases, 'counts':max_number_counts, 'age':max_number_age, 'sex':max_number_sex}\n",
    "counts_method = {'age':'SineCos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding = EmbeddingPhenoCat(\n",
    "    pheno_method=pheno_method,\n",
    "    instance_size=instance_size,\n",
    "    proj_embed=proj_embed,\n",
    "    method=embedding_method,\n",
    "    Embedding_size=Embedding_size,\n",
    "    rollup_depth=rollup_depth, \n",
    "    freeze_embed=freeze_embedding,\n",
    "    dic_embedding_cat_params=dic_embedding_cat_params,\n",
    "    dicts=dataT.dicts,\n",
    "    counts_method=counts_method\n",
    ")\n",
    "\n",
    "model = TabTransformerGeneModel_V2(\n",
    "    pheno_method=pheno_method,\n",
    "    Embedding=Embedding,\n",
    "    list_env_features=list_env_features,\n",
    "    Head_size=Head_size,\n",
    "    binary_classes=True,\n",
    "    instance_size=instance_size,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    mask_padding=masking_padding,\n",
    "    padding_token=padding_token,\n",
    "    p_dropout=p_dropout,\n",
    "    device='cpu',\n",
    "    loss_version=loss_version,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    proj_embed=proj_embed,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_52905/4153207007.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.6939e-03, -5.6683e-03],\n",
       "         [ 1.3835e-03, -4.6257e-03],\n",
       "         [-8.0239e-04,  3.8687e-05],\n",
       "         [ 8.8392e-04, -1.4570e-03],\n",
       "         [ 5.6030e-03, -1.1007e-02],\n",
       "         [ 1.1980e-02, -6.1403e-03],\n",
       "         [ 2.2064e-03, -3.6595e-03],\n",
       "         [-4.0777e-04, -1.4163e-03],\n",
       "         [-1.2971e-03, -1.4790e-03],\n",
       "         [ 4.6057e-03, -8.1748e-03],\n",
       "         [ 9.4777e-04, -1.6527e-03],\n",
       "         [-6.5837e-04, -1.8705e-03],\n",
       "         [ 1.2858e-02, -1.4519e-02],\n",
       "         [ 1.3729e-02, -1.3402e-02],\n",
       "         [ 1.3167e-03, -1.8621e-03],\n",
       "         [ 1.4153e-02, -2.0337e-02],\n",
       "         [ 8.2598e-03, -1.2816e-02],\n",
       "         [-7.3151e-04, -3.5170e-03],\n",
       "         [ 8.4189e-03, -1.1280e-02],\n",
       "         [ 1.4097e-04, -9.1331e-04],\n",
       "         [-1.3298e-03, -3.1648e-03],\n",
       "         [ 9.4121e-03, -1.0915e-02],\n",
       "         [ 1.3282e-03, -1.4887e-03],\n",
       "         [ 1.5908e-03, -3.9806e-03],\n",
       "         [ 2.5790e-03,  1.1578e-03],\n",
       "         [ 7.0528e-03, -9.9995e-03],\n",
       "         [-9.9785e-04, -2.2508e-04],\n",
       "         [ 9.1973e-04, -3.1565e-04],\n",
       "         [-8.3966e-04,  1.2163e-03],\n",
       "         [-6.7807e-04, -3.2074e-03],\n",
       "         [ 3.8790e-03, -6.8343e-03],\n",
       "         [ 1.6263e-03, -5.6004e-03],\n",
       "         [-7.3166e-05,  1.0528e-03],\n",
       "         [ 1.1568e-02, -1.8880e-02],\n",
       "         [ 1.8676e-03, -1.8913e-03],\n",
       "         [ 2.1802e-03, -3.9117e-03],\n",
       "         [ 5.3363e-03, -4.6237e-03],\n",
       "         [ 3.3577e-03, -6.7986e-03],\n",
       "         [ 3.2190e-03, -2.6742e-03],\n",
       "         [ 5.2626e-03, -5.5571e-03],\n",
       "         [ 2.4215e-03, -6.8167e-04],\n",
       "         [ 8.1351e-03, -6.0984e-03],\n",
       "         [ 4.4705e-03, -7.3661e-03],\n",
       "         [ 5.4030e-03, -7.6317e-03],\n",
       "         [ 2.8183e-04, -2.5062e-03],\n",
       "         [-2.6383e-04, -1.0857e-03],\n",
       "         [-3.0624e-04,  1.4986e-04],\n",
       "         [ 2.1319e-03, -4.2590e-03],\n",
       "         [ 1.1675e-02, -1.7206e-02],\n",
       "         [ 1.6743e-03, -2.4922e-03],\n",
       "         [ 5.0305e-03, -5.7722e-03],\n",
       "         [ 1.1027e-03, -3.3376e-04],\n",
       "         [ 5.5277e-03, -6.0681e-03],\n",
       "         [ 2.4326e-03, -1.4475e-03],\n",
       "         [ 3.8016e-03, -1.9012e-03],\n",
       "         [ 1.6615e-03, -3.0929e-03],\n",
       "         [ 1.2198e-03, -2.2316e-03],\n",
       "         [ 6.0754e-04,  4.2005e-04],\n",
       "         [ 3.5577e-03, -9.4434e-03],\n",
       "         [ 2.8754e-03, -3.9427e-03],\n",
       "         [ 1.9453e-02, -1.9238e-02],\n",
       "         [ 1.0082e-03, -1.2134e-03],\n",
       "         [ 2.5546e-03, -2.1981e-03],\n",
       "         [ 9.3740e-03, -4.8101e-03],\n",
       "         [ 1.0054e-02, -1.4709e-02],\n",
       "         [ 3.7930e-03, -8.2645e-03],\n",
       "         [ 4.1063e-04, -1.1192e-03],\n",
       "         [ 2.7531e-03, -7.5340e-03],\n",
       "         [ 6.4111e-03, -7.4173e-03],\n",
       "         [ 2.9193e-03, -1.8487e-03],\n",
       "         [ 1.2659e-03, -6.7745e-03],\n",
       "         [ 4.0980e-03, -3.7555e-03],\n",
       "         [ 8.0983e-03, -9.0391e-03],\n",
       "         [ 2.0993e-03, -2.8859e-03],\n",
       "         [ 3.2506e-03, -6.5744e-03],\n",
       "         [ 5.8515e-03, -5.2008e-03],\n",
       "         [ 6.2877e-04,  7.2896e-05],\n",
       "         [ 9.2220e-04, -2.6149e-03],\n",
       "         [ 1.2752e-03,  2.5225e-04],\n",
       "         [ 6.0124e-04, -1.9756e-03],\n",
       "         [ 1.4384e-03, -5.9259e-03]], grad_fn=<ViewBackward0>),\n",
       " tensor(0.6975, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.padding_mask_probas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_52905/4153207007.py:133: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n"
     ]
    }
   ],
   "source": [
    "logits, loss, input_embedded, attention_probas, probas = model.forward_decomposed(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0115, 0.0115, 0.0115, 0.0000, 0.0115, 0.0115, 0.0000, 0.0115,\n",
       "        0.0115, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_probas[0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1590e-03,  1.0172e-03],\n",
       "        [ 2.3007e-03, -1.3708e-03],\n",
       "        [ 3.3515e-03, -5.7096e-04],\n",
       "        [ 1.5905e-03,  1.8928e-03],\n",
       "        [ 3.4379e-03,  3.7935e-04],\n",
       "        [-3.1777e-04, -3.2896e-04],\n",
       "        [ 4.4466e-04, -4.1487e-04],\n",
       "        [ 1.1240e-03,  1.7581e-04],\n",
       "        [ 6.1040e-03, -3.4345e-03],\n",
       "        [ 3.2322e-03, -1.8210e-03],\n",
       "        [ 4.5344e-04,  7.2853e-04],\n",
       "        [ 8.2971e-04,  3.4760e-03],\n",
       "        [ 1.2453e-03,  2.6368e-03],\n",
       "        [ 3.6828e-03,  3.3782e-04],\n",
       "        [ 7.6898e-05, -1.5002e-03],\n",
       "        [ 3.4331e-04,  1.2215e-03],\n",
       "        [ 3.6066e-03,  1.3114e-03],\n",
       "        [ 2.0417e-04, -2.3432e-04],\n",
       "        [ 1.1820e-03, -7.7444e-05],\n",
       "        [ 1.4891e-03,  1.2438e-03],\n",
       "        [ 5.2570e-03,  8.8205e-04],\n",
       "        [ 1.8317e-03,  6.1368e-04],\n",
       "        [ 6.6612e-04,  2.6851e-04],\n",
       "        [ 4.6232e-03,  2.0571e-04],\n",
       "        [ 8.5909e-04,  1.6742e-03],\n",
       "        [ 2.0903e-03,  7.1877e-05],\n",
       "        [ 1.1389e-03, -1.8890e-03],\n",
       "        [ 3.4398e-03, -1.4074e-04],\n",
       "        [-1.7680e-03,  1.1188e-03],\n",
       "        [-1.8584e-04,  1.5057e-03],\n",
       "        [ 5.2282e-03, -1.4076e-03],\n",
       "        [ 3.4292e-03, -1.0547e-03],\n",
       "        [-1.0730e-03,  1.9067e-03],\n",
       "        [ 7.4294e-04, -3.3977e-04],\n",
       "        [ 6.9870e-03, -3.5905e-03],\n",
       "        [ 2.8229e-03, -1.0367e-03],\n",
       "        [ 5.5337e-03, -1.5224e-03],\n",
       "        [ 4.3845e-03, -1.1964e-03],\n",
       "        [ 6.5675e-04, -4.8354e-04],\n",
       "        [-2.4357e-04,  2.2984e-03],\n",
       "        [ 1.4005e-03,  3.1197e-04],\n",
       "        [-5.7842e-04,  1.4046e-03],\n",
       "        [ 1.1177e-03, -6.4184e-04],\n",
       "        [ 9.2635e-04,  4.9833e-04],\n",
       "        [ 1.3771e-03,  2.6964e-04],\n",
       "        [ 7.8829e-04,  1.6274e-04],\n",
       "        [ 5.0669e-03, -1.7346e-03],\n",
       "        [ 2.0211e-03, -7.1419e-04],\n",
       "        [-6.2896e-04,  3.9899e-04],\n",
       "        [ 2.3048e-03,  7.7766e-04],\n",
       "        [ 2.3369e-03, -4.0815e-04],\n",
       "        [ 7.6616e-03, -2.4802e-03],\n",
       "        [ 1.3740e-03,  6.2691e-04],\n",
       "        [ 2.3478e-03,  1.8616e-04],\n",
       "        [ 7.1715e-04,  6.0319e-04],\n",
       "        [ 9.3327e-03, -2.9294e-03],\n",
       "        [ 3.5356e-03,  2.0631e-04],\n",
       "        [ 8.2795e-03,  8.1019e-04],\n",
       "        [ 8.1296e-03, -8.7162e-04],\n",
       "        [ 1.9840e-04, -4.7974e-04],\n",
       "        [ 5.7159e-06,  1.2837e-03],\n",
       "        [-4.2370e-04, -1.0627e-04],\n",
       "        [ 1.3986e-03, -3.1657e-04],\n",
       "        [ 5.9574e-03, -1.4142e-03],\n",
       "        [ 1.0627e-03, -9.5442e-04],\n",
       "        [ 1.3407e-03,  3.5469e-04],\n",
       "        [ 2.1194e-03, -2.4729e-04],\n",
       "        [ 5.1948e-03,  7.6561e-04],\n",
       "        [ 1.8065e-03, -3.5126e-04],\n",
       "        [ 5.4868e-03, -1.6300e-03],\n",
       "        [-1.4214e-04,  1.4843e-04],\n",
       "        [ 4.3003e-03, -2.7313e-03],\n",
       "        [ 3.5872e-03, -5.2912e-04],\n",
       "        [ 3.4251e-03, -2.3044e-04],\n",
       "        [-8.2175e-04, -3.2860e-04],\n",
       "        [ 5.8740e-03, -4.9888e-04],\n",
       "        [ 4.7799e-03,  5.0665e-05],\n",
       "        [-1.7984e-03,  2.9461e-03],\n",
       "        [ 2.1314e-03,  1.4713e-03],\n",
       "        [-5.4403e-04,  6.2404e-04],\n",
       "        [ 2.1632e-03, -7.8331e-04]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_23174/2410537298.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time_epoch = time.time()\n",
    "total_loss = 0.0  \n",
    "\n",
    "#with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "for k, input_dict in enumerate(dataloader_train):\n",
    "    \n",
    "\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(input_dict)\n",
    "    #optimizer.zero_grad(set_to_none=True)\n",
    "    #loss.backward()\n",
    "\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    #optimizer.step()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diseases': Embedding(627, 10),\n",
       " 'age': Embedding(71, 10),\n",
       " 'sex': Embedding(2, 10)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding.dic_embedding_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (24400x10 and 128x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 162\u001b[0m, in \u001b[0;36mEmbeddingPhenoCat.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    160\u001b[0m     diseases_sentences_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic_embedding_cat[key](value)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_embed:\n\u001b[0;32m--> 162\u001b[0m         diseases_sentences_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiseases_sentences_embedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcounts\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpheno_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaul\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (24400x10 and 128x10)"
     ]
    }
   ],
   "source": [
    "Embedding(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_24335/888009741.py:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([89, 253, 255])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.padding_mask[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0422,  0.0020, -0.0045, -0.0077], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat(Embedding.list_env_embedded).view(2, 89, 4).transpose(0, 1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0422,  0.0020, -0.0045, -0.0077], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding.list_env_embedded[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch for key 'a': {'a': tensor([4, 5]), 'b': tensor([4, 5])}\n",
      "Batch for key 'b': {'a': tensor([3, 6]), 'b': tensor([3, 6])}\n",
      "Batch for key 'a': {'a': tensor([1, 2]), 'b': tensor([1, 2])}\n",
      "Batch for key 'b': {'a': tensor([1, 4]), 'b': tensor([1, 4])}\n",
      "Batch for key 'a': {'a': tensor([6, 3]), 'b': tensor([6, 3])}\n",
      "Batch for key 'b': {'a': tensor([5, 2]), 'b': tensor([5, 2])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDictDataset(Dataset):\n",
    "    def __init__(self, data_dict, key):\n",
    "        self.data_dict = data_dict\n",
    "        self.keys = list(data_dict.keys())\n",
    "        self.key = key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict[self.key])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = {k: self.data_dict[k][index] for k in self.keys}\n",
    "        return batch\n",
    "\n",
    "# Example huge dictionary\n",
    "huge_dict = {\n",
    "    'a': [1, 2, 3, 4, 5, 6],  # List of 100 values for key 'a'\n",
    "    'b':  [1, 2, 3, 4, 5, 6] # List of 100 values for key 'b'\n",
    "}\n",
    "\n",
    "# Create separate datasets for keys 'a' and 'b'\n",
    "dataset_a = MyDictDataset(huge_dict, key='a')\n",
    "dataset_b = MyDictDataset(huge_dict, key='b')\n",
    "\n",
    "# Create separate DataLoaders for keys 'a' and 'b'\n",
    "batch_size = 2\n",
    "dataloader_a = DataLoader(dataset_a, batch_size=batch_size, shuffle=True)\n",
    "dataloader_b = DataLoader(dataset_b, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoaders\n",
    "for batch_a, batch_b in zip(dataloader_a, dataloader_b):\n",
    "    print(\"Batch for key 'a':\", batch_a)\n",
    "    print(\"Batch for key 'b':\", batch_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': ['b', 'c'], 'value': [tensor([4, 7]), tensor([5, 8]), tensor([6, 9])]}\n",
      "{'key': ['a'], 'value': [tensor([1]), tensor([2]), tensor([3])]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDictDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.keys = list(data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        return {'key': key, 'value': self.data[key]}\n",
    "\n",
    "# Example dictionary\n",
    "my_data = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "my_dataset = MyDictDataset(my_data)\n",
    "\n",
    "# Create a DataLoader using the custom dataset\n",
    "batch_size = 2\n",
    "my_dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in my_dataloader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = EmbeddingPheno(method=None, vocab_size=10, Embedding_size=E, max_count_same_disease=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =TransformerGeneModel_V2(pheno_method=None,Embedding=embedding, Head_size=5,  Classes_nb=2, n_head=1, n_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.rand(4, 3, 6)\n",
    "v = torch.rand(4, 6).view(4, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.concat([u, v], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([[1, 2, 3, 0, 0]])\n",
    "counts = torch.tensor([1, 1, 1, 0, 0])\n",
    "labels = torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_10686/1781061347.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax( self.lm_head_proba(x).view(Batch_len, Sentence_len)) # shape B, S, 1(represent the probas to be chosen)\n"
     ]
    }
   ],
   "source": [
    "logits, probas, attention_probas = model.forward_decomposed(data,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1938, 0.2221, 0.1944, 0.1949, 0.1949]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.rand(2, 3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.rand(2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "logt = logits.transpose(1, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "probt = probas.view(2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = (logt @ probt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = logits[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1960)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(n* probas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1960],\n",
       "         [0.1987]],\n",
       "\n",
       "        [[0.2959],\n",
       "         [0.2656]]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.rand(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
