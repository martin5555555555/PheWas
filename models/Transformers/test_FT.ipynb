{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functionnal version with optionnal mask padding and dropouts, see Transformer_V1.ipynb for example\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "\n",
    "### imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList, Patient\n",
    "\n",
    "from codes.models.Transformers.Embedding import EmbeddingPhenoCat\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "from torch.utils.data import DataLoader\n",
    "### Transformer's instance\n",
    "# B, S, E, H, HN, MH = Batch_len, Sentence_len, Embedding_len, Head_size, Head number, MultiHead size.\n",
    "class TabTransformerGeneModel_V2(nn.Module):\n",
    "    def __init__(self, pheno_method, Embedding, instance_size, proj_embed, list_env_features, Head_size, binary_classes, n_head, n_layer, mask_padding=False, padding_token=None, p_dropout=0, device='cpu', loss_version='cross_entropy', gamma=2, alpha=1):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.Embedding_size = Embedding.Embedding_size\n",
    "        \n",
    "\n",
    "        self.mask_padding = mask_padding\n",
    "        self.padding_token = padding_token\n",
    "        self.padding_mask = None\n",
    "        self.device = device\n",
    "        self.pheno_method = pheno_method\n",
    "        self.binary_classes = binary_classes\n",
    "        self.Classes_nb = 2 if self.binary_classes else 3\n",
    "        self.loss_version = loss_version\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.list_env_features = list_env_features\n",
    "        self.nb_env = len(self.list_env_features)\n",
    "        self.instance_size = instance_size\n",
    "        self.Embedding = Embedding\n",
    "        self.proj_embed = proj_embed\n",
    "        if not self.proj_embed:\n",
    "            self.instance_size = self.Embedding_size\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size)\n",
    "        \n",
    "        self.blocks =PadMaskSequential(*[Block(self.instance_size, n_head=n_head, Head_size=Head_size, p_dropout=p_dropout, nb_env=self.nb_env) for _ in range(n_layer)]) #Block(self.instance_size, n_head=n_head, Head_size=Head_size) \n",
    "        self.ln_f = nn.LayerNorm(self.instance_size) # final layer norm\n",
    "        self.lm_head_logits = nn.Linear(self.instance_size, self.Classes_nb) \n",
    "        self.lm_head_proba = nn.Linear(self.instance_size,1) # plus one for the probabilities\n",
    "\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, input_dict):\n",
    "        diseases_sentence = input_dict['diseases']\n",
    "        B, S = np.shape(diseases_sentence)[0], np.shape(diseases_sentence)[1]\n",
    "        mask = torch.where(diseases_sentence==self.padding_token)\n",
    "        padding_mask_mat = torch.ones((B, S+self.nb_env, S+self.nb_env))\n",
    "        padding_mask_mat[mask] = 0\n",
    "        padding_mask_mat.transpose(-2,-1)[mask] = 0\n",
    "\n",
    "        padding_mask_probas = torch.ones((B, S+self.nb_env))\n",
    "        padding_mask_probas[mask] = 0\n",
    "        padding_mask_probas = padding_mask_probas.view(B, S+self.nb_env)\n",
    "        return padding_mask_mat.to(self.device), padding_mask_probas.to(self.device) # 1 if masked, 0 else\n",
    "\n",
    "    def set_padding_mask_transformer(self, padding_mask_mat, padding_mask_probas):\n",
    "        self.padding_mask_mat = padding_mask_mat\n",
    "        self.padding_mask_probas = padding_mask_probas\n",
    "    \n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        for key, value in input_dict.items():\n",
    "            input_dict[key] = value.to(self.device)\n",
    "\n",
    "        if 'SNP_label' in list(input_dict.keys()):\n",
    "            targets = input_dict.pop('SNP_label')\n",
    "        else:\n",
    "            targets = None\n",
    "        input_embedded = self.Embedding(input_dict)\n",
    "        Batch_len, Sentence_len, _ = input_embedded.shape\n",
    "\n",
    "   \n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask_mat, padding_mask_probas = self.create_padding_mask(input_dict)\n",
    "            self.set_padding_mask_transformer(padding_mask_mat, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask_mat)\n",
    "\n",
    "        \n",
    "        x = self.blocks(input_embedded) # shape B, S, E\n",
    "        x = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        probas = probas * self.padding_mask_probas\n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.Classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, targets, self.loss_version, self.gamma, self.alpha)\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def forward_decomposed(self, input_dict):\n",
    "        for key, value in input_dict.items():\n",
    "            input_dict[key] = value.to(self.device)\n",
    "            \n",
    "        if 'SNP_label' in list(input_dict.keys()):\n",
    "            targets = input_dict.pop('SNP_label')\n",
    "        else:\n",
    "            targets = None\n",
    "        input_embedded = self.Embedding(input_dict)\n",
    "        Batch_len, Sentence_len, _ = input_embedded.shape\n",
    "\n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask_mat, padding_mask_probas = self.create_padding_mask(input_dict)\n",
    "            self.set_padding_mask_transformer(padding_mask_mat, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask_mat)\n",
    "\n",
    "        \n",
    "        x, attention_probas = self.blocks.forward_decompose(input_embedded) # shape B, S, E\n",
    "        x = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        probas = probas * self.padding_mask_probas\n",
    "\n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.Classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, targets, self.loss_version, self.gamma, self.alpha)\n",
    "        return logits, loss, input_embedded, attention_probas, probas\n",
    "\n",
    "    def predict(self,input_dict):\n",
    "        if 'SNP_label' in list(input_dict.keys()):\n",
    "            input_dict.pop('SNP_label')\n",
    "        logits, _ = self(input_dict) # shape B, Classes_nb\n",
    "        return torch.argmax(logits, dim=1)  # (B,)\n",
    "        \n",
    "    def predict_proba(self, input_dict):\n",
    "        if 'SNP_label' in list(input_dict.keys()):\n",
    "            input_dict.pop('SNP_label')\n",
    "        logits, _ = self(input_dict)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate(self, dataloader_test):\n",
    "        print('beginning inference evaluation')\n",
    "        start_time_inference = time.time()\n",
    "        predicted_labels_list = []\n",
    "        predicted_probas_list = []\n",
    "        true_labels_list = []\n",
    "\n",
    "        total_loss = 0.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_dicts in dataloader_test:\n",
    "\n",
    "                batch_labels = input_dicts['SNP_label']\n",
    "\n",
    "                logits, loss = self(input_dicts)\n",
    "                total_loss += loss.item()\n",
    "                predicted_labels = self.predict(input_dicts)\n",
    "                predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "                predicted_probas = self.predict_proba(input_dicts)\n",
    "                predicted_probas_list.extend(predicted_probas.cpu().numpy())\n",
    "                true_labels_list.extend(batch_labels.cpu().numpy())\n",
    "        f1 = f1_score(true_labels_list, predicted_labels_list, average='macro')\n",
    "        accuracy = accuracy_score(true_labels_list, predicted_labels_list)\n",
    "        auc_score = calculate_roc_auc(true_labels_list, np.array(predicted_probas_list)[:, 1], return_nan=True)\n",
    "        proba_avg_zero, proba_avg_one = get_proba(true_labels_list, predicted_probas_list)\n",
    "    \n",
    "        self.train()\n",
    "        print(f'end inference evaluation in {time.time() - start_time_inference}s')\n",
    "        return f1, accuracy, auc_score, total_loss/len(dataloader_test), proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list\n",
    "\n",
    "\n",
    "    def write_embedding(self, writer):\n",
    "        if self.proj_embed:\n",
    "            embedding_tensor = self.projection_embed(self.Embedding.dic_embedding_cat['diseases'].weight).detach().cpu().numpy()\n",
    "        else:\n",
    "            embedding_tensor = self.Embedding.dic_embedding_cat['diseases'].weight.detach().cpu().numpy()\n",
    "        writer.add_embedding(embedding_tensor, metadata=self.Embedding.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class PadMaskSequential(nn.Sequential):\n",
    "    def __init__(self, *args):\n",
    "        super(PadMaskSequential, self).__init__(*args)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sequential(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module.forward_decompose(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, instance_size, n_head, Head_size, p_dropout, nb_env):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadSelfAttention(n_head, Head_size, instance_size, p_dropout,  nb_env=nb_env)\n",
    "        self.ffwd = FeedForward(instance_size, p_dropout)\n",
    "        self.ln1 = nn.LayerNorm(instance_size)\n",
    "        self.ln2 = nn.LayerNorm(instance_size)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_block(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        #x = self.ln1(x)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        out_sa, attention_probas= self.sa.forward_decompose(x)\n",
    "        x = out_sa + x\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x, attention_probas\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, Head_size, instance_size, p_dropout, nb_env):\n",
    "        super().__init__()\n",
    "        self.q_network = nn.Linear(instance_size, Head_size, bias = False) \n",
    "        self.k_network =  nn.Linear(instance_size, Head_size, bias = False)\n",
    "        self.v_network =  nn.Linear(instance_size, Head_size, bias = False)\n",
    "        self.proj = nn.Linear(Head_size, instance_size)\n",
    "        self.attention_dropout = nn.Dropout(p_dropout)\n",
    "        self.resid_dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "        self.multi_head_size = Head_size // n_head\n",
    "        self.flash = False\n",
    "        self.n_head = n_head\n",
    "        self.Head_size = Head_size\n",
    "        self.padding_mask = None\n",
    "        self.nb_env = nb_env\n",
    "\n",
    "    def set_padding_mask_sa(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "\n",
    "        q = self.q_network(x)\n",
    "        k = self.k_network(x)\n",
    "        v = self.v_network(x)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas = self.attention_dropout(attention_probas)\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas @ v_multi_head # shape B, S-Env, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out        \n",
    "\n",
    "    def forward_decompose(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "\n",
    "        q = self.q_network(x)\n",
    "        k = self.k_network(x)\n",
    "        v = self.v_network(x)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            ### padding mask #####\n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas = self.attention_dropout(attention_probas)\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out, attention_probas     \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear( instance_size, 4 * instance_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * instance_size, instance_size),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding')\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SineCosineEncoding(nn.Module):\n",
    "    def __init__(self, Embedding_size, max_len=1000):\n",
    "        super(SineCosineEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, Embedding_size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, Embedding_size, 2).float() * -(np.log(10000.0) / Embedding_size))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding.to(x.device)[x]\n",
    "\n",
    "class ZeroEmbedding(nn.Module):\n",
    "    def __init__(self, Embedding_size, max_len=1000):\n",
    "        super(ZeroEmbedding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, Embedding_size)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding.to(x.device)[x]\n",
    "\n",
    "\n",
    "class EmbeddingPheno(nn.Module):\n",
    "    def __init__(self, method=None, counts_method=None, vocab_size=None, max_count_same_disease=None, Embedding_size=None, rollup_depth=4, freeze_embed=False, dicts=None):\n",
    "        super(EmbeddingPheno, self).__init__()\n",
    "\n",
    "        self.dicts = dicts\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.nb_distinct_diseases_patient = vocab_size\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.metadata = None\n",
    "        self.counts_method = counts_method\n",
    "\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        \n",
    "        if method == None:\n",
    "            self.distinct_diseases_embeddings = nn.Embedding(vocab_size, Embedding_size)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, Embedding_size)\n",
    "            torch.nn.init.normal_(self.distinct_diseases_embeddings.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.normal_(self.counts_embeddings.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        elif method == 'Abby':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "\n",
    "\n",
    "\n",
    "        elif method=='Paul':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            if self.counts_method == 'SineCosine':\n",
    "                self.counts_embeddings = SineCosineEncoding(self.Embedding_size, max_count_same_disease)\n",
    "            elif self.counts_method == 'no_counts':\n",
    "                self.counts_embeddings = ZeroEmbedding(self.Embedding_size, max_count_same_disease )\n",
    "            else:\n",
    "\n",
    "                self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "    def write_embedding(self, writer):\n",
    "            embedding_tensor = self.distinct_diseases_embeddings.weight.data.detach().cpu().numpy()\n",
    "            writer.add_embedding(embedding_tensor, metadata=self.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class EmbeddingPhenoCat(nn.Module):\n",
    "    def __init__(self, pheno_method=None,  method=None, proj_embed=None, counts_method=None, Embedding_size=10, instance_size=10, rollup_depth=4, freeze_embed=False, dic_embedding_cat_params={}, dicts=None, device='cpu'):\n",
    "        super(EmbeddingPhenoCat, self).__init__()\n",
    "\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.dic_embedding_cat_params = dic_embedding_cat_params\n",
    "        dic_embedding_cat = {}\n",
    "        self.method = method\n",
    "        self.pheno_method = pheno_method\n",
    "        self.dicts = dicts\n",
    "        self.proj_embed = proj_embed\n",
    "        self.projection_embed = None\n",
    "        self.instance_size = instance_size\n",
    "        self.counts_method = counts_method\n",
    "\n",
    "        self.device = device\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        for cat, max_number  in self.dic_embedding_cat_params.items():\n",
    "        \n",
    "            if cat=='diseases':\n",
    "                if self.method == None:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, Embedding_size)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                elif self.method == 'Abby':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed).to(self.device)\n",
    "\n",
    "            \n",
    "\n",
    "                elif self.method=='Paul':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed).to(self.device)\n",
    "                    \n",
    "            elif cat == 'counts':\n",
    "                if self.pheno_method == 'Paul':\n",
    "                    if self.counts_method[cat] == 'SineCosine':\n",
    "                        dic_embedding_cat[cat] = SineCosineEncoding(self.instance_size, max_number).to(self.device)\n",
    "                    elif self.counts_method[cat] == 'no_counts':\n",
    "                        dic_embedding_cat[cat] = ZeroEmbedding(self.instance_size, max_number).to(self.device)\n",
    "                    else:\n",
    "                        dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                        torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "            elif cat == 'age':\n",
    "                if self.counts_method[cat] == 'SineCosine':\n",
    "                    dic_embedding_cat[cat] = SineCosineEncoding(self.instance_size, max_number).to(self.device)\n",
    "                elif self.counts_method[cat] == 'no_counts':\n",
    "                    dic_embedding_cat[cat] = ZeroEmbedding(self.instance_size, max_number).to(self.device)\n",
    "                else:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size).to(self.device)\n",
    "\n",
    "        self.dic_embedding_cat = dic_embedding_cat\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        list_env_embedded = []\n",
    "        for key, value in input_dict.items():\n",
    "            \n",
    "            batch_len = len(value)\n",
    "\n",
    "            if key=='diseases':\n",
    "                diseases_sentences_embedded = self.dic_embedding_cat[key](value)\n",
    "                if self.proj_embed:\n",
    "                    diseases_sentences_embedded = self.projection_embed(diseases_sentences_embedded)\n",
    "\n",
    "            elif key=='counts':\n",
    "                if self.pheno_method == 'Paul':\n",
    "                    counts_sentence_embedded = self.dic_embedding_cat[key](value)\n",
    "                    diseases_sentences_embedded = diseases_sentences_embedded + counts_sentence_embedded\n",
    "            \n",
    "\n",
    "            else:\n",
    "                list_env_embedded.append(self.dic_embedding_cat[key](value).view(batch_len, 1, self.instance_size))\n",
    "\n",
    "        env_embedded = torch.concat(list_env_embedded, dim=1)\n",
    "\n",
    "        return torch.concat([diseases_sentences_embedded, env_embedded], dim=1)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creation of the reference model\n",
    "#### framework constants:\n",
    "model_type = 'tab_transformer'\n",
    "model_version = 'transformer_V2'\n",
    "test_name = 'baseline_model_focal'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "tryout = False # True if we are doing a tryout, False otherwise \n",
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "rollup_depth = 4\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "compute_features = False\n",
    "padding = True\n",
    "list_env_features = ['age', 'sex']\n",
    "### data format\n",
    "batch_size = 200\n",
    "data_share = 1/1000#402555\n",
    "seuil_diseases = 600\n",
    "equalize_label = False\n",
    "decorelate = False\n",
    "threshold_corr = 1\n",
    "threshold_rare = 1000\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "##### model constants\n",
    "embedding_method = 'Abby' #None, Paul, Abby\n",
    "freeze_embedding = True\n",
    "Embedding_size = 4 # Size of embedding.\n",
    "proj_embed = True\n",
    "instance_size = 10\n",
    "n_head = 2 # number of SA heads\n",
    "n_layer = 1 # number of blocks in parallel\n",
    "Head_size = 4 # size of the \"single Attention head\", which is the sum of the size of all multi Attention heads\n",
    "eval_epochs_interval = 5 # number of epoch between each evaluation print of the model (no impact on results)\n",
    "eval_batch_interval = 40\n",
    "p_dropout = 0.3 # proba of dropouts in the model\n",
    "masking_padding = True # do we include padding masking or not\n",
    "loss_version = 'cross_entropy' #cross_entropy or focal_loss\n",
    "gamma = 2\n",
    "alpha = 63\n",
    "##### training constants\n",
    "total_epochs = 100 # number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=True,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share)\n",
    "#patient_list = dataT.get_patientlist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.models.data_form.DataSets import TabDictDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_data= dataT.get_data_tabtransfo(actualise_phenos=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train, indices_test = dataT.get_indices_train_test(nb_data=len(dic_data['diseases']))\n",
    "\n",
    "dic_data_train = {key: np.array(dic_data[key])[indices_train] for key in dic_data.keys()}\n",
    "dic_data_test = {key: np.array(dic_data[key])[indices_test] for key in dic_data.keys()}\n",
    "\n",
    "max_number_diseases = len(dataT.dicts['id'] ) \n",
    "max_number_counts = np.max([np.max(dic_data['counts'][k]) for k in range(len(dic_data['counts']))]) + 1\n",
    "max_number_age = np.max(np.array(dic_data['age'])) + 1\n",
    "max_number_sex = 2\n",
    "dic_embedding_cat_params = {'diseases':max_number_diseases, 'counts':max_number_counts, 'age':max_number_age, 'sex':max_number_sex} \n",
    "\n",
    "dataset_train = TabDictDataset(dic_data_train)\n",
    "dataset_test = TabDictDataset(dic_data_test)\n",
    "\n",
    "dataloader_train =  DataLoader(dataset_train, batch_size = batch_size, shuffle=True)\n",
    "dataloader_test =  DataLoader(dataset_test, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "dataloader_train = dataloader_train\n",
    "dataloader_test = dataloader_test\n",
    "vocab_size = max_number_diseases\n",
    "max_count_same_disease =max_number_diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_embedding_cat_params = {'diseases':max_number_diseases, 'counts':max_number_counts, 'age':max_number_age, 'sex':max_number_sex}\n",
    "counts_method = {'age':'SineCos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding = EmbeddingPhenoCat(\n",
    "    pheno_method=pheno_method,\n",
    "    instance_size=instance_size,\n",
    "    proj_embed=proj_embed,\n",
    "    method=embedding_method,\n",
    "    Embedding_size=Embedding_size,\n",
    "    rollup_depth=rollup_depth, \n",
    "    freeze_embed=freeze_embedding,\n",
    "    dic_embedding_cat_params=dic_embedding_cat_params,\n",
    "    dicts=dataT.dicts,\n",
    "    counts_method=counts_method\n",
    ")\n",
    "\n",
    "model = TabTransformerGeneModel_V2(\n",
    "    pheno_method=pheno_method,\n",
    "    Embedding=Embedding,\n",
    "    list_env_features=list_env_features,\n",
    "    Head_size=Head_size,\n",
    "    binary_classes=True,\n",
    "    instance_size=instance_size,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    mask_padding=masking_padding,\n",
    "    padding_token=padding_token,\n",
    "    p_dropout=p_dropout,\n",
    "    device='cpu',\n",
    "    loss_version=loss_version,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    proj_embed=proj_embed,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.padding_mask_probas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss, input_embedded, attention_probas, probas = model.forward_decomposed(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas[0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time_epoch = time.time()\n",
    "total_loss = 0.0  \n",
    "\n",
    "#with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "for k, input_dict in enumerate(dataloader_train):\n",
    "    \n",
    "\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(input_dict)\n",
    "    #optimizer.zero_grad(set_to_none=True)\n",
    "    #loss.backward()\n",
    "\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    #optimizer.step()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding.dic_embedding_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.padding_mask[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat(Embedding.list_env_embedded).view(2, 89, 4).transpose(0, 1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding.list_env_embedded[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDictDataset(Dataset):\n",
    "    def __init__(self, data_dict, key):\n",
    "        self.data_dict = data_dict\n",
    "        self.keys = list(data_dict.keys())\n",
    "        self.key = key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict[self.key])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = {k: self.data_dict[k][index] for k in self.keys}\n",
    "        return batch\n",
    "\n",
    "# Example huge dictionary\n",
    "huge_dict = {\n",
    "    'a': [1, 2, 3, 4, 5, 6],  # List of 100 values for key 'a'\n",
    "    'b':  [1, 2, 3, 4, 5, 6] # List of 100 values for key 'b'\n",
    "}\n",
    "\n",
    "# Create separate datasets for keys 'a' and 'b'\n",
    "dataset_a = MyDictDataset(huge_dict, key='a')\n",
    "dataset_b = MyDictDataset(huge_dict, key='b')\n",
    "\n",
    "# Create separate DataLoaders for keys 'a' and 'b'\n",
    "batch_size = 2\n",
    "dataloader_a = DataLoader(dataset_a, batch_size=batch_size, shuffle=True)\n",
    "dataloader_b = DataLoader(dataset_b, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoaders\n",
    "for batch_a, batch_b in zip(dataloader_a, dataloader_b):\n",
    "    print(\"Batch for key 'a':\", batch_a)\n",
    "    print(\"Batch for key 'b':\", batch_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDictDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.keys = list(data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        return {'key': key, 'value': self.data[key]}\n",
    "\n",
    "# Example dictionary\n",
    "my_data = {'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]}\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "my_dataset = MyDictDataset(my_data)\n",
    "\n",
    "# Create a DataLoader using the custom dataset\n",
    "batch_size = 2\n",
    "my_dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in my_dataloader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = EmbeddingPheno(method=None, vocab_size=10, Embedding_size=E, max_count_same_disease=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =TransformerGeneModel_V2(pheno_method=None,Embedding=embedding, Head_size=5,  Classes_nb=2, n_head=1, n_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.rand(4, 3, 6)\n",
    "v = torch.rand(4, 6).view(4, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.concat([u, v], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([[1, 2, 3, 0, 0]])\n",
    "counts = torch.tensor([1, 1, 1, 0, 0])\n",
    "labels = torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, probas, attention_probas = model.forward_decomposed(data,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.rand(2, 3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.rand(2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "logt = logits.transpose(1, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "logt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "probt = probas.view(2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = (logt @ probt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = logits[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(n* probas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.rand(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
