{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from codes.models.metrics import calculate_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "rollup_depth = 4\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = False\n",
    "save_data = False\n",
    "remove_none = True\n",
    "decorelate = False\n",
    "equalize_label = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 50\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "compute_features = True\n",
    "padding = False\n",
    "list_env_features = ['age', 'sex']\n",
    "### data format\n",
    "batch_size = 20\n",
    "data_share = 1/10000\n",
    "\n",
    "eval_epochs_interval = 2\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=True,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share)\n",
    "#patient_list = dataT.get_patientlist()\n",
    "data, labels, indices_env, name_envs = dataT.get_tree_data(with_env=False)\n",
    "data, labels = DataTransfo_1SNP.equalize_label(data, labels)\n",
    "nb_phenos = data.shape[1]\n",
    "phenos = np.arange(nb_phenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_pheno(data, labels, pheno_nb):\n",
    "    labels_ac = labels[data[:,pheno_nb]==1]\n",
    "    labels_deac = labels[data[:,pheno_nb]==0]\n",
    "    proba_mut_ac = np.sum(labels_ac==1)/len(labels_ac)\n",
    "    proba_mut_deac = np.sum(labels_deac==1)/len(labels_deac)\n",
    "    ratio  = proba_mut_ac / proba_mut_deac\n",
    "    return ratio\n",
    "def get_pred_naive(data, labels, pheno_nb, proba=False):\n",
    "    labels_ac = labels[data[:,pheno_nb]==1]\n",
    "    nb_ones_ac = np.sum(labels_ac==1)\n",
    "    nb_zeros_ac = np.sum(labels_ac==0)\n",
    "    proba = nb_zeros_ac / len(labels_ac)\n",
    "    label = (1 if nb_ones_ac > nb_zeros_ac else 0)\n",
    "    return proba, label\n",
    "get_risk_pheno = partial(get_risk_pheno, data, labels)\n",
    "get_pred_naive = partial(get_pred_naive, data, labels)\n",
    "\n",
    "odds_ratios = list(map(get_risk_pheno, phenos))\n",
    "pred_naive = np.array(list(map(get_pred_naive, phenos)))\n",
    "probas_pred_naive = pred_naive[:, 0]\n",
    "labels_pred_naive = pred_naive[:, 1]\n",
    "\n",
    "def get_pred_sentence(probas_pred_naive, labels_pred_naive, sentence, method='max'):\n",
    "    sentence = sentence.astype(bool)\n",
    "    labels_naive = labels_pred_naive[sentence].astype(bool)\n",
    "    probas_naive = probas_pred_naive[sentence].astype(bool)\n",
    "\n",
    "    if method=='mean':\n",
    "        if np.mean(probas_naive)>0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    if method=='max':\n",
    "        argmax = np.argmax((probas_naive-0.5)**2)\n",
    "        return labels_naive[argmax]\n",
    "    \n",
    "get_pred_sentence = partial(get_pred_sentence, probas_pred_naive, labels_pred_naive)\n",
    "\n",
    "frequencies = np.sum(data, axis=0)\n",
    "labels_pred  = np.apply_along_axis(get_pred_sentence, arr=data, axis=1)\n",
    "np.sum(labels_pred == labels)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from codes.models.metrics import calculate_roc_auc, get_proba\n",
    "class NaiveModelWeights(nn.Module):\n",
    "    def __init__(self, pheno_nb):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_weights_predictor = nn.Linear(pheno_nb, pheno_nb,bias=True,  dtype=float)\n",
    "        self.logits_predictor = nn.Linear(pheno_nb, 2 *pheno_nb, bias=True, dtype=float)\n",
    "\n",
    "    def forward(self, x, labels_target=None):\n",
    "        B, P = x.shape\n",
    "        weights = self.linear_weights_predictor(x)\n",
    "        prob_weights = F.softmax(weights).view(B, P, 1)\n",
    "        logits = self.logits_predictor(x).view(B, P, 2)\n",
    "        logits = (logits.transpose(1, 2)) @ prob_weights\n",
    "        \n",
    "\n",
    "        if labels_target != None:\n",
    "            err = F.cross_entropy(logits, labels_target.view(B, 1))#torch.sqrt(torch.sum((pred_probas - labels_target)**2)/len(x)) \n",
    "        return logits, err\n",
    "\n",
    "\n",
    "    def eval_model(self, dataloader_test):\n",
    "        self.eval()\n",
    "        print('beginning inference evaluation')\n",
    "        start_time_inference = time.time()\n",
    "        predicted_labels_list = []\n",
    "        predicted_probas_list = []\n",
    "        true_labels_list = []\n",
    "\n",
    "        total_loss = 0.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for k, batch in enumerate(dataloader_test):\n",
    "                data_train= batch['data']\n",
    "                labels_train = batch['label']\n",
    "\n",
    "                logits, loss = self(data_train, labels_train)\n",
    "                total_loss += loss.item()\n",
    "                predicted_probas = F.softmax(logits).detach().numpy()\n",
    "                predicted_probas_reduced = predicted_probas[:, 1]\n",
    "                predicted_labels = (predicted_probas_reduced > 0.5).astype(int)\n",
    "\n",
    "                #predicted_labels = self.predict(batch_sentences, batch_counts)\n",
    "                predicted_labels_list.extend(predicted_labels)\n",
    "                predicted_probas_list.extend(predicted_probas)\n",
    "                true_labels_list.extend(labels_train.cpu().numpy())\n",
    "        f1 = f1_score(true_labels_list, predicted_labels_list, average='macro')\n",
    "        accuracy = accuracy_score(true_labels_list, predicted_labels_list)\n",
    "        auc_score = calculate_roc_auc(true_labels_list, np.array(predicted_probas_list), return_nan=True)\n",
    "        proba_avg_zero, proba_avg_one = get_proba(true_labels_list, predicted_probas_list)\n",
    "        self.train()\n",
    "        print(f'end inference evaluation in {time.time() - start_time_inference}s')\n",
    "        return f1, accuracy, auc_score, total_loss/len(dataloader_test), proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list\n",
    "\n",
    "    \n",
    "\n",
    "class CustomDatasetWithLabels(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size = 1-prop_train_test, random_state=42)\n",
    "data_train = CustomDatasetWithLabels(data_train, labels)\n",
    "dataloader_train = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "data_test = CustomDatasetWithLabels(data_test, labels)\n",
    "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "naive_model = NaiveModelWeights(pheno_nb=nb_phenos)\n",
    "optimizer = torch.optim.AdamW(naive_model.parameters(), lr=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in naive_model.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    start_time = time.time()\n",
    "    loss_tot = 0\n",
    "    for k, batch in enumerate(dataloader_train):\n",
    "        data_train= batch['data']\n",
    "        labels_train = batch['label']\n",
    "        # evaluate the loss\n",
    "        pred_probas, loss = naive_model(data_train, labels_train)\n",
    "        loss_tot += loss\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch%eval_epochs_interval==0:\n",
    "        \n",
    "        f1, accuracy, auc_score, loss_val, proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list= naive_model.eval_model(dataloader_test)\n",
    "        print(f'loss_val = {loss_val}')\n",
    "\n",
    "    print(f'epoch {epoch} ended in {time.time() - start_time}')\n",
    "    print(f'loss_train = {loss_tot / len(dataloader_train)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_nb = 10\n",
    "linear_weights_predictor = nn.Linear(pheno_nb, pheno_nb,bias=True,  dtype=float)\n",
    "logits_predictor = nn.Linear(pheno_nb, 2 *pheno_nb, bias=True, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 10, dtype=float)\n",
    "B, P = x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = linear_weights_predictor(x)\n",
    "prob_weights = F.softmax(weights).view(B, P, 1)\n",
    "weights = linear_weights_predictor(x)\n",
    "prob_weights = F.softmax(weights).view(B, P, 1)\n",
    "\n",
    "logits = logits_predictor(x).view(B, P, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = (logits.transpose(1, 2)) @ prob_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logits = (logits.transpose(1, 2)) @ prob_weights\n",
    "pred_probas = F.softmax(logits.view(5, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
