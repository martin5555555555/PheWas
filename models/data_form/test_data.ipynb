{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vcfpy\n",
    "import pickle\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "from codes.models.data_form.DataSets import TabDictDataset\n",
    "from codes.Data_Files.Geno.generate_SNP_file import generate_geno_file\n",
    "\n",
    "def get_paths_pheno_dicts(method, rollup_depth=None):\n",
    "    if method == 'Paul':\n",
    "        path_pheno_id_dict = f\"/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding/vocab_dict/code2id_ukbb_omop_rolled_up_depth_{rollup_depth}_closest_ancestor.pickle\"\n",
    "        path_pheno_name_dict = f\"/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding/vocab_dict/code2name_ukbb_omop_rolled_up_depth_{rollup_depth}_closest_ancestor.pickle\"\n",
    "        path_pheno_cat_dict = f\"/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding/vocab_dict/code2cat_ukbb_omop_rolled_up_depth_{rollup_depth}_closest_ancestor.pickle\"\n",
    "    elif method == 'Abby':\n",
    "        path_pheno_id_dict = '/gpfs/commons/groups/gursoy_lab/anewbury/embeddings/data/cohortId2ind.pickle'\n",
    "        path_pheno_name_dict = '/gpfs/commons/groups/gursoy_lab/anewbury/embeddings/data/cohortId2name.pickle'\n",
    "        path_pheno_cat_dict = '/gpfs/commons/groups/gursoy_lab/anewbury/embeddings/data/cohortId2name.pickle' #no separation between name and cat for Abby's method\n",
    "    return path_pheno_id_dict, path_pheno_name_dict, path_pheno_cat_dict\n",
    "    \n",
    "def get_paths_pheno_file(method, rollup_depth=None, with_counts=False):\n",
    "        if method == 'Paul':\n",
    "            pheno_file  = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Pheno/Paul/ukbb_omop_rolled_up_depth_{rollup_depth}_closest_ancestor.csv'\n",
    "            pheno_file_tree = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Pheno/Paul/ukbb_omop_rolled_up_depth_{rollup_depth}_closest_ancestor_tree.npy'\n",
    "            pheno_file_tree_counts= f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Pheno/Paul/ukbb_omop_rolled_up_depth_{rollup_depth}_closest_ancestor_tree_counts.npy'\n",
    "\n",
    "        elif method == 'Abby':\n",
    "            pheno_file = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Pheno/Abby/phenotype_embedding_df_abby.csv'\n",
    "            pheno_file_tree = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Pheno/Abby/phenotype_embedding_df_abby_tree.npy'\n",
    "            pheno_file_tree_counts = None\n",
    "\n",
    "        return pheno_file, pheno_file_tree, pheno_file_tree_counts\n",
    "\n",
    "def get_paths_env_file():\n",
    "        env_file = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/environmental_features/env_features.csv'\n",
    "        return env_file\n",
    "def get_paths_geno_file(CHR, SNP, binary_classes, ld):\n",
    "        label_dict = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Geno/{CHR}/{SNP}/label_dict_bclasses={binary_classes}_ld={ld}.pkl'\n",
    "        return label_dict\n",
    "\n",
    "def map_array(tab1, tab2, i):\n",
    "    indices = np.where(tab2==i)\n",
    "    if len(indices[0]) == 0:\n",
    "        return i\n",
    "    else:\n",
    "        return tab1[indices]\n",
    "\n",
    "class DataTransfo_1SNP:\n",
    "    def __init__(self, SNP, CHR, method='Paul', binary_classes=True, rollup_depth=4, pad_token='<PAD>', padding=True, load_data=True, save_data=False, \n",
    "                 remove_none=True, compute_features=True, data_share=1, prop_train_test=0.8, equalize_label=False, seuil_diseases=None,\n",
    "                 decorelate=False, threshold_corr=1, threshold_rare=0, remove_rare=None, list_env_features=[], test=False, indices=None, list_pheno_ids=None, ld=False):\n",
    "        self.SNP = SNP\n",
    "        self.CHR = CHR\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.pad_token = pad_token\n",
    "        self.label_dict = None\n",
    "        self.padding = padding\n",
    "        self.path = \"/gpfs/commons/datasets/controlled/ukbb-gursoylab/\"\n",
    "        self.load_data = load_data\n",
    "        self.save_data = save_data\n",
    "        self.remove_none = remove_none\n",
    "        self.compute_features = compute_features\n",
    "        self.data_share = data_share\n",
    "        self.indices_train = None\n",
    "        self.indices_test = None\n",
    "        self.prop_train_test = prop_train_test\n",
    "        self.method = method\n",
    "        self.seuil_diseases = seuil_diseases\n",
    "        self.equalize_label = equalize_label\n",
    "        self.decorelate = decorelate\n",
    "        self.threshold_corr = threshold_corr\n",
    "        self.binary_classes = binary_classes\n",
    "        self.threshold_rare = threshold_rare\n",
    "        self.remove_rare = remove_rare\n",
    "        self.ld = ld\n",
    "        path_pheno_id_dict, path_pheno_name_dict, path_pheno_cat_dict = get_paths_pheno_dicts(method, rollup_depth)\n",
    "        self.pheno_file, self.pheno_file_tree, self.pheno_file_tree_counts = get_paths_pheno_file(method, rollup_depth)\n",
    "        self.env_file = get_paths_env_file()\n",
    "        self.geno_file = get_paths_geno_file(self.CHR, self.SNP, self.binary_classes, self.ld)\n",
    "        self.test = test\n",
    "        with open(path_pheno_id_dict, \"rb\") as f:\n",
    "            self.pheno_id_dict = pickle.load(f)\n",
    "        with open(path_pheno_name_dict, \"rb\") as f:\n",
    "            self.name_dict = pickle.load(f)\n",
    "        with open(path_pheno_cat_dict, \"rb\") as f:\n",
    "            self.cat_dict = pickle.load(f)\n",
    "\n",
    "        self.pheno_id_dict[0] = self.pad_token\n",
    "        self.name_dict[0] = 'pad'\n",
    "        self.cat_dict[0] = 'pad'\n",
    "        self.vocab_size = len(self.pheno_id_dict)\n",
    "\n",
    "        self.pheno_id_dict[self.pad_token]= 0\n",
    "        self.name_dict[self.pad_token]= self.pad_token\n",
    "        self.cat_dict[self.pad_token]= self.pad_token\n",
    "        self.dicts = {'id':self.pheno_id_dict, 'name': self.name_dict, 'cat': self.cat_dict}\n",
    "        self.list_env_features = list_env_features\n",
    "        self.list_pheno_ids=list_pheno_ids\n",
    "        self.indices = indices\n",
    "        self.eid_list = None\n",
    "        if self.indices != None:\n",
    "            self.indices_test = indices[0]\n",
    "            self.indices_train = indices[1]\n",
    "        \n",
    "\n",
    "\n",
    "    def build_name_file(self):\n",
    "        decorelate_part = f'decorelate={self.decorelate}' if (not self.decorelate) else f'decorelate={self.decorelate}_thcorr={self.threshold_corr}_thrare={self.threshold_rare}_rmrare={self.remove_rare}'\n",
    "        pheno_filter_snp_part = '' if (self.list_pheno_ids == None) else f'_filter={self.SNP}'\n",
    "        \n",
    "        name_file = f'PatientList_{self.SNP}_{decorelate_part}{pheno_filter_snp_part}.pkl'\n",
    "        \n",
    "        return name_file\n",
    "        \n",
    "\n",
    "    def get_patientlist(self):\n",
    "        if self.load_data:\n",
    "            start_time = time.time()\n",
    "            print(f\"loading data\")\n",
    "            # Open the file in binary write mode\n",
    "            name_file = self.build_name_file()\n",
    "            data_file = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Training/SNPS/{str(self.CHR)}/{self.SNP}/{self.method}/{name_file}'\n",
    "           \n",
    "            with open(data_file, 'rb') as file:\n",
    "                patient_list = pickle.load(file)\n",
    "            print(f\"data_loaded in {time.time() - start_time} s\")\n",
    "\n",
    "            if  self.data_share != 1:\n",
    "                patient_list.keep_share(self.data_share)\n",
    "                self.indices_train = None\n",
    "                self.indices_test = None\n",
    "        else:\n",
    "            print(\"building data\")\n",
    "            genectic_data = self.get_genetic_data()\n",
    "            pheno_data_df = self.get_pheno_data()\n",
    "            env_data_df = self.get_env_data()\n",
    "            patient_list = PatientList(\n",
    "                [self.get_eid_data(eid, pheno_data_df.get_group(eid), self.method) \n",
    "                for eid in tqdm(list(pheno_data_df.groups.keys())[:int(self.nb_eid * self.data_share)], desc=\"Processing\", unit=\"group\")], \n",
    "                self.pad_token, self.pheno_id_dict, self.list_env_features)\n",
    "            \n",
    "            print(len(patient_list))\n",
    "            if self.remove_none:\n",
    "                print(\"removing None values\")\n",
    "            patient_list.remove_none()\n",
    "            if self.decorelate:\n",
    "                patient_list.decorelate(self.threshold_corr, self.threshold_rare, self.remove_rare)\n",
    "        \n",
    "        if self.remove_none:\n",
    "            print(\"removing None values\")\n",
    "            patient_list.remove_none()\n",
    "        \n",
    "        if self.padding:\n",
    "            print(\"padding data\")\n",
    "            patient_list.padd_data()\n",
    "\n",
    "        if self.seuil_diseases != None:\n",
    "            patient_list.set_seuil_data(self.seuil_diseases)\n",
    "            self.indices_train = None\n",
    "            self.indices_test = None\n",
    "        if self.equalize_label:\n",
    "            patient_list.equalize_label()\n",
    "        \n",
    "        if self.compute_features:\n",
    "            print(\"computing features\")\n",
    "            patient_list.get_nb_distinct_diseases_tot()\n",
    "            patient_list.get_nb_max_distinct_diseases_patient()\n",
    "            patient_list.get_max_count_same_disease()\n",
    "            self.get_indices_train_test(nb_data=len(patient_list), prop_train_test=self.prop_train_test)\n",
    "        if self.save_data and self.data_share==1: # saves only if all data have been processed\n",
    "            print(\"saving data\")\n",
    "            data_dir = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Training/SNPS/{str(self.CHR)}/{self.SNP}/{self.method}/'\n",
    "            name_file = self.build_name_file()\n",
    "            data_file = os.path.join(data_dir, name_file)\n",
    "            if not os.path.exists(data_dir):\n",
    "                os.makedirs(data_dir)\n",
    "\n",
    "            with open(data_file, 'wb') as file:\n",
    "                # Use pickle.dump() to serialize and save the object to the file\n",
    "                pickle.dump(patient_list, file)\n",
    "        \n",
    "        #self.actualise_phenos(type='patient_list', patient_list=patient_list)\n",
    "\n",
    "        \n",
    "        return patient_list\n",
    "\n",
    "    def get_pheno_data(self):\n",
    "        start_time = time.time()\n",
    "        if self.method == 'Paul':\n",
    "            print('loading df Paul')\n",
    "            if self.test:\n",
    "                df = pd.read_csv(self.pheno_file, nrows=100)\n",
    "                \n",
    "            else:\n",
    "                df = pd.read_csv(self.pheno_file)\n",
    "            if self.list_pheno_ids != None:\n",
    "                df = df[df['concept_id'].isin(self.list_pheno_ids)] # filter the dataFrame, only keep the eids that has one of the diseases\n",
    "            #df['concept_id'] = df['concept_id'].map(self.pheno_id_dict) already done\n",
    "            self.eid_list = list(df['eid'].unique())\n",
    "            self.nb_eid = len(self.eid_list)\n",
    "            df_grouped = df.groupby('eid')\n",
    "        elif self.method == 'Abby':\n",
    "            print('loading df Abby')\n",
    "            if self.test:\n",
    "                pheno_df = pd.read_csv(self.pheno_file, nrows=100)\n",
    "            else:\n",
    "                pheno_df = pd.read_csv(self.pheno_file)\n",
    "            pheno_df.set_index(pheno_df.subject_id, inplace=True)\n",
    "            pheno_df.drop('subject_id', axis=1, inplace=True)\n",
    "            self.eid_list = list(pheno_df.index)\n",
    "            self.nb_eid = len(self.eid_list)\n",
    "            pheno_df.columns = pheno_df.columns.map(self.pheno_id_dict)\n",
    "            df_grouped = pheno_df.groupby(pheno_df.index)\n",
    "        print(f'df loaded in {time.time() - start_time} s for creating data')\n",
    "        return df_grouped\n",
    "\n",
    "\n",
    "    def get_env_data(self):\n",
    "        df_env = pd.read_csv(self.env_file)\n",
    "        self.env_df = df_env\n",
    "        return df_env\n",
    "    \n",
    "    def get_eid_data(self, eid, df, method):\n",
    "        if method == 'Paul':\n",
    "            unique_codes = list(df['concept_id'].values)\n",
    "            occurrences = list(df['condition_occurrence_count'].values)\n",
    "\n",
    "            #disease_sentence = [code for code in unique_codes]\n",
    "            #counts_sentence = [count for count in occurrences]\n",
    "\n",
    "            def get_sum_counts(group, i):\n",
    "                return np.sum(group['condition_occurrence_count'][group['concept_id']==i])\n",
    "\n",
    "            \n",
    "            get_sum_counts_par = np.vectorize(partial(get_sum_counts, df))\n",
    "            disease_sentence = np.array(df['concept_id'].unique())\n",
    "            counts_sentence = get_sum_counts_par(disease_sentence)\n",
    "            \n",
    "\n",
    "               \n",
    "            #print(str(eid) in list(self.label_dict.keys()))\n",
    "        elif method == 'Abby':\n",
    "            df_t = df.transpose()\n",
    "            df_t = df_t[df_t[eid]==1]\n",
    "            disease_sentence = np.array(df_t.index)\n",
    "            counts_sentence = np.ones(len(disease_sentence)) # put all the counts at one in Abby's case.\n",
    "        label = self.label_dict.get(str(eid))\n",
    "        #env features:\n",
    "        dic_env_features_patient = {}            \n",
    "        for feature in self.list_env_features:\n",
    "            if np.sum(self.env_df['eid']==eid)==0:\n",
    "                dic_env_features_patient[feature]=None\n",
    "            else:\n",
    "                dic_env_features_patient[feature] = self.env_df[self.env_df['eid']==eid].iloc[0][feature]\n",
    "            \n",
    "\n",
    "        patient = Patient(disease_sentence, counts_sentence, label, dic_env_features_patient)\n",
    "        \n",
    "        \n",
    "        return patient\n",
    "\n",
    "    def get_genetic_data(self, mut = None):\n",
    "        if mut == None:\n",
    "            CHR, SNP = self.CHR, self.SNP\n",
    "        else:\n",
    "            CHR, SNP = mut\n",
    "        geno_file = get_paths_geno_file(CHR, SNP, self.binary_classes, self.ld)\n",
    "        if os.path.exists(self.geno_file):\n",
    "            with open(self.geno_file, 'rb') as file:\n",
    "                label_dict = pickle.load(file)\n",
    "        else:\n",
    "            label_dict = generate_geno_file(self.CHR, self.SNP, self.binary_classes, self.ld, self.save_data)\n",
    "        self.label_dict = label_dict\n",
    "        return label_dict\n",
    "       \n",
    "\n",
    "    def get_indices_train_test(self, nb_data=None, prop_train_test=0.8):\n",
    "        if type(self.indices_train) != np.ndarray:\n",
    "            indices = np.arange(nb_data)\n",
    "            np.random.shuffle(indices)\n",
    "            self.indices_train = indices[:int(nb_data*prop_train_test)]\n",
    "            self.indices_test = indices[int(nb_data*prop_train_test):]\n",
    "        return self.indices_train, self.indices_test\n",
    "    \n",
    "    def get_tree_data(self, with_env=True, with_counts=False, load_possible=True, only_relevant=False, mut=None):\n",
    "        if self.list_pheno_ids == None:\n",
    "            if self.method == 'Abby':\n",
    "                pheno_data = np.load(self.pheno_file_tree)\n",
    "                eids = pheno_data[:,0]\n",
    "                nb_phenos = pheno_data.shape[1] - 1\n",
    "                pheno_data_tot = np.zeros((pheno_data.shape[0], nb_phenos+1+len(self.list_env_features))) if with_env else np.zeros((pheno_data.shape[0], nb_phenos+1))\n",
    "                pheno_data_tot[:, :pheno_data.shape[1]] = pheno_data\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "            if self.method == 'Paul':\n",
    "                if self.list_env_features == []:\n",
    "                    with_env = False\n",
    "                pheno_file_tree = self.pheno_file_tree_counts if with_counts else self.pheno_file_tree\n",
    "                if load_possible and os.path.exists(pheno_file_tree):\n",
    "                    pheno_data = np.load(self.pheno_file_tree)\n",
    "                    eids = pheno_data[:,0]\n",
    "                    nb_phenos = pheno_data.shape[1] - 1\n",
    "                    \n",
    "                    pheno_data_tot = np.zeros((pheno_data.shape[0], nb_phenos+1+len(self.list_env_features))) if with_env else np.zeros((pheno_data.shape[0], nb_phenos+1))\n",
    "                    pheno_data_tot[:, :pheno_data.shape[1]] = pheno_data\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    pheno_df = pd.read_csv(self.pheno_file)\n",
    "                    eids = np.unique( np.array(list(pheno_df['eid'])))\n",
    "                    nb_phenos = np.max(pheno_df['concept_id'])\n",
    "                    pheno_data_tot = np.zeros((len(eids), nb_phenos+1+len(self.list_env_features)), dtype=int) if with_env else  np.zeros((len(eids), nb_phenos+1), dtype=int) \n",
    "\n",
    "                    grouped = pheno_df.groupby('eid')\n",
    "\n",
    "                    def get_sum_counts(group, i):\n",
    "                        return np.sum(group['condition_occurrence_count'][group['concept_id']==i])\n",
    "\n",
    "                    def get_data(with_counts, group):\n",
    "                        if with_counts:\n",
    "                            get_sum_counts_par = np.vectorize(partial(get_sum_counts, group))\n",
    "                            arr = np.array(group['concept_id'].unique())\n",
    "                            return arr, get_sum_counts_par(arr)\n",
    "                        else:\n",
    "                            return np.array(group['concept_id'].unique())\n",
    "                        \n",
    "                    get_data = partial(get_data, with_counts)       \n",
    "                    data_list = grouped.apply(get_data)\n",
    "\n",
    "                    if with_counts:\n",
    "                        for k, (diseases, counts) in enumerate(data_list):\n",
    "                            pheno_data_tot[k,np.array(diseases)] = counts\n",
    "                    else:\n",
    "                        for k, diseases in enumerate(data_list):\n",
    "                            pheno_data_tot[k,np.array(diseases)] = 1\n",
    "                    \n",
    "\n",
    "\n",
    "                    if self.save_data:\n",
    "                        data = pheno_data_tot\n",
    "                        np.save(self.pheno_file_tree, arr = data[:, :1+ nb_phenos])\n",
    "\n",
    "            if with_env:\n",
    "                env_df = pd.read_csv(self.env_file)[['eid'] + self.list_env_features]\n",
    "                env_df.set_index('eid', inplace=True)\n",
    "                env_df = env_df[env_df.notna().all(axis=1)]\n",
    "\n",
    "            if mut == None:\n",
    "                labels_dict = [self.get_genetic_data()]\n",
    "            else:\n",
    "                labels_dict = [self.get_genetic_data(mut=gene) for gene in mut]\n",
    "\n",
    "            indices = np.ones(len(eids)).astype(bool)\n",
    "            for label_dict in labels_dict:\n",
    "                indices = indices & (np.isin(eids, list(label_dict.keys())))\n",
    "            indices = np.isin(eids,env_df.index) & (indices) if with_env else indices\n",
    "\n",
    "            \n",
    "\n",
    "            env = []\n",
    "            labels = []\n",
    "            for eid in eids[indices]:\n",
    "                if with_env:\n",
    "                    env.append(np.array(env_df.loc[eid]))\n",
    "                labels.append(np.array([label_dict[str(eid)] for label_dict in labels_dict]))\n",
    "\n",
    "\n",
    "            env = np.array(env)\n",
    "            labels = np.array(labels)\n",
    "            pheno_data_tot = pheno_data_tot[indices]\n",
    "            if with_env :\n",
    "                pheno_data_tot[:, 1 + nb_phenos:] = env\n",
    "\n",
    "            if mut == None:\n",
    "                labels = labels[:,0]\n",
    "            else:\n",
    "                labels = labels\n",
    "            return pheno_data_tot[:, 1:], labels, 1+nb_phenos, self.list_env_features, eids\n",
    "        else:\n",
    "            if self.method == 'Paul':\n",
    "                print('youpi')\n",
    "                pheno_df = pd.read_csv(self.pheno_file)\n",
    "                pheno_df_filtered = pheno_df[pheno_df['concept_id'].isin(self.list_pheno_ids)]\n",
    "                eid_list_filtered = np.unique( np.array(list(pheno_df_filtered['eid'])))\n",
    "                eid_list = np.unique( np.array(list(pheno_df['eid'])))\n",
    "\n",
    "                nb_phenos = np.max(pheno_df['concept_id'])\n",
    "\n",
    "                grouped = pheno_df_filtered.groupby('eid') if only_relevant else pheno_df.groupby('eid')\n",
    "\n",
    "                pheno_id_dict = self.pheno_id_dict\n",
    "                pheno_id_reverse = {value: key for key, value in pheno_id_dict.items()}\n",
    "\n",
    "                dic_map_pheno_old_new = { self.list_pheno_ids[i]:i+1 for i in range(len(self.list_pheno_ids))}\n",
    "\n",
    "                pheno_id_dict_new = {pheno_id_reverse[pheno_id]: i+1 for i,pheno_id in enumerate(self.list_pheno_ids)}\n",
    "                pheno_id_dict_new[max(pheno_id_dict_new.keys())+1] = len(self.list_pheno_ids)+1\n",
    "                self.name_dict[max(pheno_id_dict_new.keys())] = 'additional disease'\n",
    "                self.name_dict = {key:self.name_dict[key] for key in pheno_id_dict_new.keys()}\n",
    "                self.cat_dict[max(pheno_id_dict_new.keys())] = 'additional disease'\n",
    "                self.cat_dict = {key:self.cat_dict[key] for key in pheno_id_dict_new.keys()}\n",
    "                \n",
    "                self.pheno_dict = pheno_id_dict_new\n",
    "\n",
    "                def map_values(val):\n",
    "                    return dic_map_pheno_old_new[val]\n",
    "                map_values = np.vectorize(map_values)\n",
    "\n",
    "                nb_phenos_interest = len(self.list_pheno_ids)\n",
    "                if only_relevant:\n",
    "                    res = np.zeros((len(eid_list), nb_phenos_interest), dtype=int)\n",
    "                else:\n",
    "                    res = np.zeros((len(eid_list), nb_phenos), dtype=int)\n",
    "                print(nb_phenos)\n",
    "                \n",
    "                data_list  = [np.array(grouped.get_group(eid)['concept_id'].unique())\n",
    "                        for eid in eid_list_filtered]\n",
    "                for k, diseases in enumerate(data_list):\n",
    "                    if only_relevant:\n",
    "                        res[k,map_values(diseases)-1] = 1\n",
    "                    else:\n",
    "                        res[k, diseases-1] = 1\n",
    "                if only_relevant:\n",
    "                    columns = np.arange(1, nb_phenos_interest+1)\n",
    "                else:\n",
    "                    columns = np.arange(1, nb_phenos+1)\n",
    "                print(len(columns))\n",
    "                print(res.shape)\n",
    "                pheno_df = pd.DataFrame(data=res, columns=columns)\n",
    "                pheno_df.insert(0, 'eid', eid_list)\n",
    "                if only_relevant:\n",
    "                    pheno_df[nb_phenos_interest+1] = np.ones(len(pheno_df), dtype=int)\n",
    "                \n",
    "\n",
    "        env_df = pd.read_csv(self.env_file)\n",
    "        env_df.set_index('eid', inplace=True)\n",
    "        pheno_df.set_index('eid', inplace=True)\n",
    "        list_features_env = self.list_env_features if with_env else []\n",
    "        df_tot = pheno_df.join(env_df[list_features_env], on=pheno_df.index, rsuffix='') if with_env else pheno_df\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        label_dict = self.get_genetic_data()\n",
    "        df_labels = pd.DataFrame(list(label_dict.items()), columns=['eid', 'label'])\n",
    "        df_labels.set_index('eid', inplace=True)\n",
    "        df_labels.index = df_labels.index.astype(int)\n",
    "        df_tot_label = df_tot.join(df_labels, on = df_tot.index, rsuffix='')\n",
    "\n",
    "        if self.remove_none:\n",
    "            df_tot_label = df_tot_label[df_tot_label['label'].notna()]\n",
    "        columns = df_tot_label.columns\n",
    "        indice_env = len(pheno_df.columns)\n",
    "        cols_env = columns[indice_env:]\n",
    "        df_tot_label = df_tot_label.astype(int)\n",
    "        pheno_data = df_tot_label.to_numpy()\n",
    "\n",
    "\n",
    "        if self.list_pheno_ids != None:\n",
    "            return pheno_data[:,:-1], pheno_data[:, -1], indice_env, cols_env, len(eid_list_filtered)\n",
    "    \n",
    "\n",
    "        return pheno_data[:,:-1], pheno_data[:, -1], indice_env, cols_env, df_tot_label\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def get_data_tabtransfo(self, train_test=False, patient_list=None, actualise_phenos=True):\n",
    "        keys = ['diseases', 'counts', 'SNP_label'] + self.list_env_features\n",
    "        self.dic_list_patients = {key:[] for key in keys }\n",
    "\n",
    "        if self.load_data:\n",
    "            data_dir = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Training/SNPS/{str(self.CHR)}/{self.SNP}/{self.method}/'\n",
    "            if self.decorelate:\n",
    "                name_file = f'dic_data_{self.SNP}_decorelate={self.decorelate}_thcorr={self.threshold_corr}_thrare={self.threshold_rare}_rmrare={self.remove_rare}.pkl'\n",
    "            else:\n",
    "                name_file = f'dic_data_{self.SNP}_decorelate={self.decorelate}.pkl'\n",
    "            filename = data_dir + name_file\n",
    "            with open(filename, 'rb') as file:\n",
    "                dic_list_patients_loaded = pickle.load(file)\n",
    "\n",
    "            n_size = int(len(dic_list_patients_loaded['diseases'])*self.data_share)\n",
    "\n",
    "            for key in keys:\n",
    "                self.dic_list_patients[key] = np.array(dic_list_patients_loaded[key])[:n_size]\n",
    "\n",
    "            if self.equalize_label:\n",
    "                self.tab_transfo_equalize_label()\n",
    "            \n",
    "\n",
    "        else:\n",
    "            \n",
    "            if patient_list == None:\n",
    "                patient_list = self.get_patientlist()\n",
    "\n",
    "            \n",
    "            for patient in tqdm(patient_list, desc='Processing', unit='group') :\n",
    "                self.dic_list_patients['diseases'].append(patient.diseases_sentence)\n",
    "                self.dic_list_patients['counts'].append(patient.counts_sentence)\n",
    "                self.dic_list_patients['SNP_label'].append(patient.SNP_label)\n",
    "                for key, value in patient.dic_env_features.items():\n",
    "                    self.dic_list_patients[key].append(value)\n",
    "\n",
    "\n",
    "            if self.save_data and self.data_share==1:\n",
    "                data_dir = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Training/SNPS/{str(self.CHR)}/{self.SNP}/{self.method}/'\n",
    "                if self.decorelate:\n",
    "                    name_file = f'dic_data_{self.SNP}_decorelate={self.decorelate}_thcorr={self.threshold_corr}_thrare={self.threshold_rare}_rmrare={self.remove_rare}.pkl'\n",
    "                else:\n",
    "                    name_file = f'dic_data_{self.SNP}_decorelate={self.decorelate}.pkl'\n",
    "                filename = data_dir + name_file\n",
    "                with open(filename, 'wb') as file:\n",
    "                    pickle.dump(self.dic_list_patients, file)\n",
    "\n",
    "        if actualise_phenos:\n",
    "            self.actualise_phenos(type='dic_tab', dic_list_patients=self.dic_list_patients)\n",
    "        return self.dic_list_patients\n",
    "\n",
    "    def tab_transfo_equalize_label(self):\n",
    "        \n",
    "        indices_labels_ones = np.where(self.dic_list_patients['SNP_label'] == 1)[0]\n",
    "        indices_labels_zeros = np.where(self.dic_list_patients['SNP_label'] == 0)[0]\n",
    "        nb_zeros =len(indices_labels_zeros)\n",
    "        nb_ones = len(indices_labels_ones)\n",
    "        minor_allele = 0 if nb_zeros <= nb_ones else 1\n",
    "        major_allele = 1 - minor_allele\n",
    "        indices_labels_minor_allele = indices_labels_ones if minor_allele ==1 else indices_labels_zeros\n",
    "        indices_labels_major_allele = indices_labels_ones if major_allele ==1 else indices_labels_zeros\n",
    "\n",
    "        indices_selected_major = indices_labels_major_allele[:len(indices_labels_minor_allele)]\n",
    "        for key in self.dic_list_patients.keys():\n",
    "            data_minor = np.array(self.dic_list_patients[key])[indices_labels_minor_allele]\n",
    "            data_major= np.array(self.dic_list_patients[key])[indices_selected_major]\n",
    "            self.dic_list_patients[key] = np.concatenate([data_minor, data_major])\n",
    "            np.random.shuffle(self.dic_list_patients[key])\n",
    "\n",
    "    def actualise_phenos(self, type='patient_list', patient_list=None, dic_list_patients=None):\n",
    "        diseases_present = np.zeros(len(self.pheno_id_dict), dtype=bool)\n",
    "\n",
    "        if type=='patient_list':\n",
    "            for patient in patient_list:\n",
    "                diseases_present[patient.diseases_sentence] = True\n",
    "        elif type=='dic_tab':\n",
    "            for list_diseases in dic_list_patients['diseases']:\n",
    "                diseases_present[np.array(list_diseases)] = True\n",
    "        phenos_ex = np.where(diseases_present==1)[0]\n",
    "        phenos_new = np.zeros(len(diseases_present), dtype=int)\n",
    "        phenos_new[diseases_present] = np.arange(len(phenos_ex))\n",
    "        if type=='patient_list':\n",
    "            for patient in patient_list:\n",
    "                patient.diseases_sentence = phenos_new[patient.diseases_sentence]\n",
    "        elif type=='dic_tab':\n",
    "            for k, list_diseases in enumerate(dic_list_patients['diseases']):\n",
    "                dic_list_patients['diseases'][k] = phenos_new[np.array(list_diseases)]\n",
    "        id_dict = {0:self.pad_token}\n",
    "        cat_dict = {0 : 'pad'}\n",
    "        name_dict = {0 : 'pad'}\n",
    "        for key in self.pheno_id_dict:\n",
    "            pheno_id = self.pheno_id_dict[key]\n",
    "            if phenos_new[pheno_id] != 0 :\n",
    "                id_dict[key] = phenos_new[pheno_id] \n",
    "                cat_dict[key] = self.cat_dict[key]\n",
    "                name_dict[key] = self.name_dict[key]\n",
    "        self.pheno_id_dict = id_dict\n",
    "        self.cat_dict = cat_dict\n",
    "        self.name_dict = name_dict\n",
    "        self.dicts = {'id':self.pheno_id_dict, 'name': self.name_dict, 'cat': self.cat_dict, 'diseases_present':diseases_present}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def equalize_label(data, labels):\n",
    "        nb_zeros = np.sum(labels==0)\n",
    "        nb_ones = len(labels) - nb_zeros\n",
    "        minor_label = 1 if nb_ones <= nb_zeros else 0\n",
    "        major_label = 1 - minor_label\n",
    "        data_minor = data[labels==minor_label]\n",
    "        data_major = data[labels ==major_label][:len(data_minor)]\n",
    "        data = np.concatenate([data_minor, data_major], axis=0)\n",
    "        labels = np.array([minor_label]*len(data_minor) + [major_label]*len(data_major))\n",
    "        return data, labels\n",
    "        \n",
    "\n",
    "class Patient:\n",
    "    def __init__(self, diseases_sentence, counts_sentence, label, dic_env_features=None):\n",
    "        self.diseases_sentence = diseases_sentence\n",
    "        self.counts_sentence = counts_sentence\n",
    "        self.SNP_label = label\n",
    "        self.dic_env_features = dic_env_features\n",
    "\n",
    "        self.nb_max_distinct_diseases_patient = None\n",
    "        self.nb_max_counts_same_disease = None\n",
    "        self.nb_distinct_diseases_ini = len(self.diseases_sentence)\n",
    "        self.nb_distinct_diseases = self.get_nb_distinct_diseases\n",
    "        self.nb_counts_distinct_diseases = len(self.counts_sentence)\n",
    "        self.hasnone = False\n",
    "        self.see_hasnone()\n",
    "\n",
    "    def see_hasnone(self):\n",
    "        if self.SNP_label==None:\n",
    "            self.hasnone = True\n",
    "        else:\n",
    "            for feature in self.dic_env_features.keys():\n",
    "                if self.dic_env_features[feature]==None:\n",
    "                    self.hasnone=True\n",
    "\n",
    "    @property\n",
    "    def nb_distinct_diseases_actual(self):\n",
    "        return len(self.diseases_sentence)\n",
    "    \n",
    "    @property\n",
    "    def nb_counts_distinct_diseases_actual(self):\n",
    "        return len(self.counts_sentence)\n",
    "\n",
    "    @property\n",
    "    def get_nb_distinct_diseases(self):\n",
    "        diseases_sentence = np.array(self.diseases_sentence)\n",
    "        self.nb_distinct_diseases = len(diseases_sentence[diseases_sentence != 0])\n",
    "        return self.nb_distinct_diseases\n",
    "    \n",
    "    def padd_patient(self, nb_max_distinct_disease, padding_item):\n",
    "        self.diseases_sentence = np.concatenate([self.diseases_sentence, np.full(nb_max_distinct_disease-self.nb_distinct_diseases_actual, padding_item)])\n",
    "        self.counts_sentence = np.concatenate([self.counts_sentence, np.zeros(nb_max_distinct_disease-self.nb_counts_distinct_diseases_actual)]).astype(int)\n",
    "        return nb_max_distinct_disease-self.nb_distinct_diseases# to inform on sparcity\n",
    "    def unpadd_patient(self, padding_item):\n",
    "        self.diseases_sentence = self.diseases_sentence[self.diseases_sentence!= padding_item]\n",
    "        self.counts_sentence = self.counts_sentence[self.counts_sentence!= padding_item]\n",
    "\n",
    "    def get_vector(self, nb_max_diseases_sentence):\n",
    "        patient_grouped = list(zip(self.diseases_sentence, self.counts_sentence))\n",
    "\n",
    "        # Sort element according to the first list\n",
    "        patient_group_sorted = sorted(patient_grouped, key=lambda x: x[0])\n",
    "        # Retrieve the two sorted list\n",
    "        diseases_sentence_sorted, counts_sentence_sorted = map(np.array, zip(*patient_group_sorted))\n",
    "        # create patient vector\n",
    "        vector_patient = np.zeros(nb_max_diseases_sentence)\n",
    "        vector_patient[diseases_sentence_sorted-1] = counts_sentence_sorted\n",
    "        self.vector_patient = vector_patient\n",
    "        return vector_patient\n",
    "\n",
    "    def get_tree_data(self, nb_max_diseases_sentence, list_env_features=[]):\n",
    "        res_diseases = np.zeros(nb_max_diseases_sentence)\n",
    "        res_diseases[self.diseases_sentence] = 1\n",
    "        if len(list_env_features) > 0:\n",
    "            print(self.dic_env_features['age'])\n",
    "            tab_env = np.array([self.dic_env_features[key] for key in list_env_features])\n",
    "            return np.concatenate([res_diseases, tab_env])\n",
    "        return res_diseases\n",
    "    \n",
    "    def delete_phenos(self, pheno_map_final, tab_reverse_pheno_ex_new):\n",
    "        diseases_sentence_ini = self.diseases_sentence\n",
    "        self.diseases_sentence =  np.unique(pheno_map_final(np.array(diseases_sentence_ini)))\n",
    "        self.diseases_sentence = self.diseases_sentence[self.diseases_sentence !=0] # to get rid of the zeros(padding or removal)\n",
    "        group_pheno_id_list = [tab_reverse_pheno_ex_new[i] for i in self.diseases_sentence]\n",
    "        self.counts_sentence = [np.max(np.array(self.counts_sentence)[np.where(np.isin(diseases_sentence_ini,group_pheno_id_list[0][0]))[0]]) for group_pheno_id in group_pheno_id_list]\n",
    "\n",
    "            \n",
    "class PatientList:\n",
    "    def __init__(self, list_patients, padding_item=0, pheno_id_dict=None, list_env_features=[]):\n",
    "        self.patients_list = list_patients\n",
    "        self.padding_item = padding_item\n",
    "        self.nb_distinct_diseases_tot = None\n",
    "        self.nb_max_counts_same_disease = None\n",
    "        self.nb_max_distinct_diseases_patient = None\n",
    "        self.label_vector = None\n",
    "        self.is_padded = False\n",
    "        self.pheno_id_dict = pheno_id_dict\n",
    "        self.sparsity = None\n",
    "        self.seuil = None\n",
    "        self.share = 1\n",
    "        self.list_env_features = list_env_features\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients_list)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.patients_list[idx]\n",
    "    \n",
    "    def keep_share(self, share):\n",
    "        if self.share == share:\n",
    "            pass\n",
    "        else:\n",
    "            n = int(share * len(self))\n",
    "            self.patients_list = self.patients_list[:n]\n",
    "            self.share = share\n",
    "    \n",
    "    def get_nb_max_distinct_diseases_patient(self):\n",
    "        self.nb_max_distinct_diseases_patient = np.max(np.array([patient.nb_distinct_diseases for patient in self.patients_list]))\n",
    "        return self.nb_max_distinct_diseases_patient\n",
    "  \n",
    "    def get_max_count_same_disease(self):\n",
    "        self.nb_max_counts_same_disease = max([max(patient.counts_sentence) for patient in self.patients_list])+1 #+1 because of zero counts (padding)\n",
    "        return self.nb_max_counts_same_disease\n",
    "    \n",
    "    def get_nb_distinct_diseases_tot(self):\n",
    "        self.nb_distinct_diseases_tot = max([max(patient.diseases_sentence) for patient in self.patients_list])+1 #+1 because of the zero padding\n",
    "\n",
    "        return self.nb_distinct_diseases_tot\n",
    "    def get_labels_vector(self):\n",
    "        self.label_vector = np.array([patient.SNP_label for patient in self.patients_list])\n",
    "        return self.label_vector\n",
    "    \n",
    "    def remove_none(self):\n",
    "        patient_list = []\n",
    "        for patient in self.patients_list:\n",
    "            if not patient.hasnone:\n",
    "                patient_list.append(patient)\n",
    "            \n",
    "        self.patients_list = patient_list\n",
    "\n",
    "\n",
    "    def get_transformer_data(self, indices_train, indices_test): # transform the patients list in a tuple (distinct_diseases, counts_sentences) list\n",
    "        transformer_dataset_train = [(patient.diseases_sentence, patient.counts_sentence, patient.SNP_label) for patient in np.array(self.patients_list)[indices_train]]\n",
    "        transformer_dataset_test = [(patient.diseases_sentence, patient.counts_sentence, patient.SNP_label) for patient in np.array(self.patients_list)[indices_test]]\n",
    "\n",
    "        return transformer_dataset_train,transformer_dataset_test\n",
    "    \n",
    "    def padd_data(self):\n",
    "        if self.is_padded == True:\n",
    "            pass\n",
    "        else:\n",
    "            if self.nb_max_distinct_diseases_patient==None:\n",
    "                self.get_nb_max_distinct_diseases_patient()\n",
    "            padded_data_count = 0\n",
    "            for patient in self.patients_list:\n",
    "                padded_data_count += patient.padd_patient(self.nb_max_distinct_diseases_patient, self.padding_item)\n",
    "            self.sparsity = padded_data_count / (len(self)*self.nb_max_distinct_diseases_patient)\n",
    "            self.is_padded = True\n",
    "    def unpad_data(self):\n",
    "        for patient in self.patients_list:\n",
    "            patient.unpadd_patient(padding_item=self.padding_item)\n",
    "    def get_matrix_data(self):\n",
    "        if self.nb_max_distinct_diseases_patient==None:\n",
    "            nb_distinct_diseases_patient = self.get_nb_max_distinct_diseases_patient()\n",
    "        patient_list_matrix = np.zeros((len(self),nb_distinct_diseases_patient))\n",
    "        for i,patient in enumerate(self.patients_list):\n",
    "            patient_list_matrix[i,:] = patient.get_vector(nb_distinct_diseases_patient)\n",
    "        \n",
    "        return patient_list_matrix, self.get_labels_vector()\n",
    "    \n",
    "    def get_tree_data(self, with_env=False):\n",
    "        if self.nb_distinct_diseases_tot==None:\n",
    "            nb_distinct_diseases_tot = self.get_nb_distinct_diseases_tot()\n",
    "        label_vector = self.get_labels_vector()\n",
    "        if with_env:\n",
    "            return ([patient.get_tree_data(self.nb_distinct_diseases_tot, list_env_features=self.list_env_features) for patient in self.patients_list], label_vector)\n",
    "        else:\n",
    "            return ([patient.get_tree_data(self.nb_distinct_diseases_tot, list_env_features=[]) for patient in self.patients_list], label_vector)\n",
    "    \n",
    "    def compute_features(self):\n",
    "        self.get_nb_distinct_diseases_tot()\n",
    "        self.get_max_count_same_disease()\n",
    "        self.get_nb_max_distinct_diseases_patient()\n",
    "\n",
    "    def set_seuil_data(self, seuil):\n",
    "        new_patient_list = []\n",
    "        for patient in self.patients_list:\n",
    "            if patient.nb_counts_distinct_diseases <= seuil:\n",
    "                new_patient_list.append(patient)\n",
    "        self.patients_list = new_patient_list\n",
    "        self.compute_features()\n",
    "\n",
    "    def get_major_label(self):\n",
    "        label_vector = self.get_labels_vector()\n",
    "        nb_zeros = np.sum(label_vector==0)\n",
    "        nb_ones = np.sum(label_vector==1)\n",
    "        if nb_zeros >= nb_ones:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def equalize_label(self):\n",
    "        major_label = self.get_major_label()\n",
    "        nb_minor_label = np.sum(self.label_vector!=major_label)\n",
    "        counts_major_label = 0\n",
    "        new_patient_list = []\n",
    "        for patient in self.patients_list:\n",
    "            if patient.SNP_label != major_label:\n",
    "                new_patient_list.append(patient)\n",
    "            else:\n",
    "                if counts_major_label < nb_minor_label:\n",
    "                    new_patient_list.append(patient)\n",
    "                    counts_major_label +=1\n",
    "        self.patients_list = new_patient_list\n",
    "        self.compute_features()\n",
    "\n",
    "    def split_labels(self):\n",
    "        patient_label_zero = []\n",
    "        patient_label_one = []\n",
    "        for patient in self.patients_list:\n",
    "            if patient.SNP_label == 0:\n",
    "                patient_label_zero.append(patient)\n",
    "            else:\n",
    "                patient_label_one.append(patient)\n",
    "        return patient_label_zero, patient_label_one\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_pheno(indices_pheno_null_variance, indices_phenos_rare, indices_pheno_one_class, remove_rare=None):\n",
    "        if remove_rare==None:\n",
    "            return indices_pheno_null_variance\n",
    "        elif remove_rare == 'all':\n",
    "            return np.unique(np.concatenate([indices_pheno_null_variance, indices_phenos_rare]))\n",
    "        elif remove_rare == 'one_class':\n",
    "            tab_phenos_rare_and_one_class = np.intersect1d(indices_pheno_one_class, indices_phenos_rare) \n",
    "            return np.unique(np.concatenate([indices_pheno_null_variance, tab_phenos_rare_and_one_class]))\n",
    "\n",
    "    def decorelate(self, threshold_corr, threshold_rare, remove_rare, dicts=None):\n",
    "        print('decorelate the data set')\n",
    "        pheno_tree, labels = self.get_tree_data()\n",
    "        pheno_tree = np.array(pheno_tree)\n",
    "        nb_occ = np.sum(pheno_tree, axis=0)\n",
    "        self.indices_pheno_null_variance =np.argwhere(np.var(pheno_tree, axis=0) ==0)\n",
    "        self.indices_pheno_rare = np.argwhere(nb_occ < threshold_rare)\n",
    "        #pheno_tree = pheno_tree[ :, np.var(pheno_tree, axis=0)!=0] #get rid of the phenotypes that has a null variance\n",
    "        def get_labels(col):\n",
    "            return len(np.unique(labels[col==1]))-1\n",
    "        counts_labels = np.apply_along_axis(get_labels, axis =0, arr=pheno_tree)\n",
    "        self.indices_pheno_one_class = np.argwhere(counts_labels==0)\n",
    "\n",
    "        self.indices_pheno_remove = PatientList.remove_pheno(self.indices_pheno_null_variance, self.indices_pheno_rare, self.indices_pheno_one_class, remove_rare)\n",
    "        print('calcul_corr')\n",
    "        start_time_corr = time.time()\n",
    "        corr = np.corrcoef(pheno_tree, rowvar=False)\n",
    "        mask = np.tri(corr.shape[0], k=0, dtype=bool)\n",
    "        corr[mask] = None\n",
    "        print(f' corr computed in {time.time() - start_time_corr}s')\n",
    "        tab_phenos_delete, tab_phenos_keep= np.where(corr >= threshold_corr)\n",
    "        self.fully_correlated_phenos = list(zip(tab_phenos_delete, tab_phenos_keep))\n",
    "        list_indices_phenos_ex = np.arange(0, self.nb_distinct_diseases_tot+1)\n",
    "        list_indice_phenos_new = np.arange(0, self.nb_distinct_diseases_tot+1)\n",
    "        list_indice_phenos_new[self.padding_item] = 0\n",
    "        print('building correspondance')\n",
    "        ##building the correspondance between the phenotype that will be deleted and those associated who remains\n",
    "        for k, pheno_delete in enumerate(np.unique(tab_phenos_delete)):\n",
    "            pheno_keep = tab_phenos_keep[np.min(np.where(tab_phenos_delete==pheno_delete))]\n",
    "            list_indice_phenos_new[pheno_delete] = pheno_keep\n",
    "            list_indice_phenos_new[list_indice_phenos_new == pheno_delete] = pheno_keep\n",
    "\n",
    "        self.list_indice_phenos_new = np.array(list_indice_phenos_new)\n",
    "        self.list_indice_phenos_ex = np.array(list_indices_phenos_ex)\n",
    "        self.indices_phenos_present= np.arange(0, len(np.unique(self.list_indice_phenos_new)))\n",
    "        pheno_map_fun_new = np.vectorize( partial(map_array, self.list_indice_phenos_new, self.list_indice_phenos_ex))\n",
    "        pheno_map_fun_reordered = np.vectorize(partial(map_array, self.indices_phenos_present, np.unique(self.list_indice_phenos_new)))\n",
    "\n",
    "\n",
    "        self.indices_pheno_remove = pheno_map_fun_reordered(pheno_map_fun_new(self.indices_pheno_remove))\n",
    "        pheno_map_fun_delete = np.vectorize(partial(map_array, np.zeros(len(np.unique(self.indices_pheno_remove))), np.unique(self.indices_pheno_remove)))\n",
    "        self.list_indice_phenos_new = pheno_map_fun_reordered(self.list_indice_phenos_new)\n",
    "\n",
    "        self.list_indice_phenos_new = pheno_map_fun_delete(self.list_indice_phenos_new)\n",
    "        new_len = len(self.list_indice_phenos_new[self.list_indice_phenos_new != 0])\n",
    "        self.list_indice_phenos_new = pheno_map_fun_delete(self.list_indice_phenos_new)\n",
    "        self.indices_phenos_present= np.arange(0, len(np.unique(self.list_indice_phenos_new)))\n",
    "        pheno_map_fun_reordered_2 = np.vectorize(partial(map_array, self.indices_phenos_present, np.unique(self.list_indice_phenos_new)))\n",
    "        self.list_indice_phenos_new = pheno_map_fun_reordered_2(self.list_indice_phenos_new )\n",
    "\n",
    "        pheno_map_final = np.vectorize(partial(map_array, self.list_indice_phenos_new , self.list_indice_phenos_ex ))\n",
    "            \n",
    "        self.tab_reverse_pheno_ex_new = [np.where(self.list_indice_phenos_new==i) for i in np.unique(self.list_indice_phenos_new)]\n",
    "\n",
    "\n",
    "\n",
    "        print('update patient_list')\n",
    "        start_time_patient = time.time()\n",
    "        [patient.delete_phenos(pheno_map_final, self.tab_reverse_pheno_ex_new) for patient in \n",
    "            tqdm(self.patients_list, desc=\"Processing\", unit=\"group\")]\n",
    "        print(f'patients computed in {time.time() - start_time_patient}s')\n",
    "\n",
    "        self.is_padded = False\n",
    "        self.nb_max_distinct_diseases_patient = None\n",
    "\n",
    "        self.padd_data()\n",
    "\n",
    "        if dicts!= None:\n",
    "            pheno_id_dict = dicts['id']\n",
    "            pheno_name_dict = dicts['name']\n",
    "            pheno_cat_dict = dicts['cat']\n",
    "            pheno_id_dict_reverse = { pheno_id_dict[key]:key for key in pheno_id_dict.keys()}\n",
    "            keys = pheno_id_dict.keys()\n",
    "            for key in keys:\n",
    "                if pheno_id_dict[key] not in self.indices_pheno_remove:\n",
    "                    pheno_id_dict[key] = self.list_indice_phenos_new[pheno_id_dict[key]]\n",
    "                    pheno_name_dict[key] = key\n",
    "                    pheno_cat_dict[key] = key\n",
    "                else:\n",
    "                    pheno_id_dict.pop(key)\n",
    "                    pheno_name_dict.pop(key)\n",
    "                    pheno_cat_dict.pop(key)\n",
    "\n",
    "\n",
    "\n",
    "    def remove_zero_patients(self):\n",
    "        def lenp(patient):\n",
    "            return np.sum(patient.diseases_sentence !=0)\n",
    "        lenp = np.vectorize(lenp)\n",
    "        array_patient_list = np.array(self.patients_list)\n",
    "        self.patients_list =list(array_patient_list[np.where(lenp(array_patient_list)!=0)[0]])\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "rollup_depth = 4\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "ld = 'no'\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = False\n",
    "save_data = True\n",
    "remove_none = False\n",
    "decorelate = False\n",
    "equalize_label = False\n",
    "threshold_corr = 0.8\n",
    "threshold_rare = 1000\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "compute_features = True\n",
    "padding = True\n",
    "list_env_features = ['age', 'meat']\n",
    "list_phenos_ids = None#list(np.load('/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/phewas/list_associations_snps/rs673604_paul.npy'))\n",
    "### data format\n",
    "batch_size = 20\n",
    "data_share = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=remove_none,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share,\n",
    "                         list_pheno_ids=list_phenos_ids,\n",
    "                         ld=ld)\n",
    "#patient_list = dataT.get_patientlist()''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_list = dataT.get_patientlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mut = [(1, 'rs673604'), (1, 'rs673604')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, indices_env, env_features, eids = dataT.get_tree_data(with_env=True, with_counts=False, load_possible=True, only_relevant=True, mut=None)\n",
    "frequencies_ini = np.sum(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_equalized, labels_equalized = DataTransfo_1SNP.equalize_label(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save, labels_save = data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a specific disease name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 92\n",
    "dic_pheno_id = dataT.pheno_id_dict\n",
    "name_dict = dataT.name_dict\n",
    "reverse = {value:key for key, value in dic_pheno_id.items()}\n",
    "name_dict[reverse[id]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse of the correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = data, labels # keep only relevant samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_corr = np.array([np.corrcoef(labels, data[:,i])[0,1] for i in range(data.shape[1]-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_corr, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse of the F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_equalized = data_equalized[:, :-1]\n",
    "data = data[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F_pheno(data, labels, pheno_nb):    \n",
    "    labels_1 = labels[data[:,pheno_nb]==1]\n",
    "    labels_0 = labels[data[:,pheno_nb]==0]\n",
    "    P0 = np.sum(labels_0==1)/len(labels_0)\n",
    "    P1 = np.sum(labels_1==1)/len(labels_1)\n",
    "    F0 = max(P0, 1-P0)\n",
    "    F1 = max(P1, 1-P1)\n",
    "    return P0, P1\n",
    "def get_plots_F(data, labels):\n",
    "    \n",
    "    get_risk_pheno = partial(get_F_pheno, data, labels)\n",
    "    frequencies = np.sum(data, axis=0) / len(data)\n",
    "    seuil_frequencies = -1\n",
    "    indices = frequencies*len(data) > seuil_frequencies\n",
    "    proba_mean = max(np.sum(labels==0)/len(labels), 1-np.sum(labels==0)/len(labels))\n",
    "    phenos = np.arange(len(data[0]))[indices]\n",
    "    Fs = np.array(list(map(get_risk_pheno, phenos)))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (10, 10))\n",
    "    axes[0].plot(Fs[:,0], 'o')\n",
    "    axes[0].plot(Fs[:, 1], 'o')\n",
    "    axes[0].axhline(proba_mean)\n",
    "    log_freq = np.log(frequencies*len(data)+1)[indices]\n",
    "    color_values = log_freq\n",
    "\n",
    "    diff_p = np.abs(Fs[:,0]-Fs[:,1]) *100\n",
    "    axes[1].scatter(np.arange(len(diff_p)), diff_p, c= color_values , cmap='viridis')\n",
    "\n",
    "    fig = plt.subplots(figsize=(10, 10))\n",
    "    plt.scatter(np.arange(len(diff_p)), diff_p, c=color_values, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    return Fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = get_plots_F(data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_p = np.abs(Fs[:,0] - Fs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_p[1537], frequencies_ini[1537]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "df = pd.read_csv(dataT.pheno_file, nrows=1000000)\n",
    "#df_filtered =  df[df['concept_id'].isin(dataT.list_pheno_ids)]\n",
    "\n",
    "#df['concept_id'] = df['concept_id'].map(dataT.pheno_id_dict) already done\n",
    "dataT.eid_list = list(df['eid'].unique())\n",
    "dataT.nb_eid = len(dataT.eid_list)\n",
    "df_grouped = df.groupby('eid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid = 1000035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_grouped.get_group(eid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#env features:\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences, unique_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
