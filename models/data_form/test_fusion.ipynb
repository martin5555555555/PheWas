{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import shap\n",
    "import tensorboard\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report\n",
    "from functools import partial\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP\n",
    "from codes.models.metrics import calculate_roc_auc\n",
    "\n",
    "import featurewiz as gwiz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "from codes.models.Decision_tree.utils import get_indice, get_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of fc1.weight: tensor([[-0.0193,  0.0146,  0.0230,  0.0026, -0.0158,  0.0272, -0.0056,  0.0026,\n",
      "         -0.0167, -0.0043],\n",
      "        [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.1777,  0.1340,  0.2113,  0.0243, -0.1454,  0.2507, -0.0512,  0.0236,\n",
      "         -0.1534, -0.0392],\n",
      "        [-0.0791,  0.0596,  0.0940,  0.0108, -0.0647,  0.1115, -0.0228,  0.0105,\n",
      "         -0.0682, -0.0174],\n",
      "        [-0.2129,  0.1605,  0.2531,  0.0291, -0.1742,  0.3003, -0.0613,  0.0283,\n",
      "         -0.1838, -0.0469]])\n",
      "Gradient of fc1.bias: tensor([-0.0187,  0.0000, -0.1719, -0.0765, -0.2060])\n",
      "Gradient of fc2.weight: tensor([[ 0.0980,  0.0000,  0.1750,  0.1817,  0.1875],\n",
      "        [-0.0980, -0.0000, -0.1750, -0.1817, -0.1875]])\n",
      "Gradient of fc2.bias: tensor([ 0.4002, -0.4002])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### framework constants:\n",
    "model_type = 'decision_tree'\n",
    "model_version = 'gradient_boosting'\n",
    "test_name = '1_test_train_transfo_V1'\n",
    "tryout = True # True if we are ding a tryout, False otherwise \n",
    "### data constants:\n",
    "### data constants:\n",
    "CHR = 6\n",
    "SNP = 'rs12203592'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "rollup_depth = 4\n",
    "ld = 'no'\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "decorelate = False\n",
    "equalize_label = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 50\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "compute_features = True\n",
    "padding = False\n",
    "list_env_features = []\n",
    "list_pheno_ids = None #list(np.load(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/phewas/list_associations_snps/{SNP}_paul.npy'))\n",
    "\n",
    "### data format\n",
    "batch_size = 20\n",
    "data_share = 1\n",
    "\n",
    "##### model constants\n",
    "\n",
    "\n",
    "##### training constants\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=remove_none,\n",
    "                         equalize_label=equalize_label,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare, \n",
    "                         list_env_features=list_env_features,\n",
    "                         data_share=data_share,\n",
    "                         list_pheno_ids=list_pheno_ids,\n",
    "                         ld=ld)\n",
    "#patient_list = dataT.get_patientlist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels_patients, indices_env, name_envs, eids = dataT.get_tree_data(with_env=False, load_possible=True, only_relevant=False)\n",
    "frequencies_ini = np.sum(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(labels_patients==1) / np.sum(labels_patients==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pheno_method == 'Abby':\n",
    "    embedding_file_diseases = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "    weight_diseases = torch.load(embedding_file_diseases)\n",
    "else:\n",
    "    embedding_file_diseases = '/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding/node2vec_embeddings/emb_4d_100_OMOP__ukbb_omop_rolled_up_depth_4.pickle'\n",
    "#weight_diseases = torch.load(embedding_file_diseases).detach().numpy()[1:]\n",
    "    with open(embedding_file_diseases, 'rb') as file:\n",
    "        weight_diseases = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_distance = cosine_distances(weight_diseases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather in clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_keep = frequencies_ini > 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 100\n",
    "# Vos données\n",
    "X =  weight_diseases\n",
    "\n",
    "# Définir une fonction de distance cosinus\n",
    "def custom_cosine_distance(X, Y=None):\n",
    "    return cosine_distances(X, Y)\n",
    "\n",
    "# Adapter KMeans pour utiliser la distance cosinus\n",
    "class CustomKMeans(KMeans):\n",
    "    def fit(self, X, y=None):\n",
    "        self.distance_func = custom_cosine_distance\n",
    "        return super().fit(X, y)\n",
    "\n",
    "# Utilisation de CustomKMeans avec la distance cosinus\n",
    "kmeans = CustomKMeans(n_clusters=n_clusters)\n",
    "\n",
    "# Ajustement du modèle aux données\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Obtention des étiquettes de cluster pour chaque vecteur\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Obtention des centres de cluster\n",
    "centers = kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 3\n",
    "indices_cluster = np.where(clusters==cluster_id)[0]\n",
    "get_name(dataT, indices_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = [np.sum(clusters==i) for i in range(n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nb, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[indices_keep] = np.arange(indices_keep.sum()) + n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'nombre clusters = {len(np.unique(clusters))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized = True\n",
    "interest = False\n",
    "keep = False\n",
    "scaled = True\n",
    "fusion = True\n",
    "random = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if fusion:\n",
    "    df = pd.DataFrame(data.T)    \n",
    "    df['cluster'] = clusters\n",
    "    grouped = df.groupby('cluster').sum()\n",
    "    data_fusion = grouped.T.to_numpy()\n",
    "    data_fusion[data_fusion!=0] = 1\n",
    "    data_use, labels_use = data_fusion, labels_patients\n",
    "else:\n",
    "    data_use, labels_use = data, labels_patients\n",
    "\n",
    "\n",
    "\n",
    "if interest:\n",
    "    data_use, labels_use = data_use[:nb_patients_interest, :-1], labels_use[:nb_patients_interest]\n",
    "else:\n",
    "    data_use, labels_use = data_use, labels_use\n",
    "\n",
    "if equalized:\n",
    "    pheno, labels = DataTransfo_1SNP.equalize_label(data=data_use, labels = labels_use)\n",
    "else:\n",
    "    pheno, labels = data_use, labels_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparcity_fusion = np.sum(data_fusion==0) / np.size(data_fusion)\n",
    "#sparcity_ini = np.sum(data==0) / np.size(data)\n",
    "#sparcity_ini, sparcity_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_patients_train, diseases_patients_test, label_patients_train, label_patients_test = train_test_split(pheno, labels, test_size = 1-prop_train_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: np.sum(label_patients_train == 1) / np.sum(label_patients_train == 0), 1: 1.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = np.sum(diseases_patients_train, axis=0)\n",
    "indices_keep = frequencies > 100\n",
    "print(f'nb phenos keep = {indices_keep.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_patients_train_keep, diseases_patients_test_keep = diseases_patients_train[:, indices_keep], diseases_patients_test[:, indices_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if keep:\n",
    "    diseases_patients_train_model = diseases_patients_train_keep\n",
    "    diseases_patients_test_model = diseases_patients_test_keep\n",
    "else:\n",
    "    diseases_patients_train_model = diseases_patients_train\n",
    "    diseases_patients_test_model = diseases_patients_test\n",
    "\n",
    " \n",
    "diseases_patients_train_model_unscaled = diseases_patients_train_model\n",
    "diseases_patients_test_model_unscaled = diseases_patients_test_model\n",
    "\n",
    "if scaled:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    diseases_patients_train_model= scaler.fit_transform(diseases_patients_train_model)\n",
    "    diseases_patients_test_model = scaler.fit_transform(diseases_patients_test_model)\n",
    "\n",
    "if random:\n",
    "    np.random.shuffle(label_patients_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingClassifier(class_weight=class_weight)\n",
    "\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "model.fit(diseases_patients_train_model, label_patients_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "labels_pred_test = model.predict(diseases_patients_test_model)\n",
    "labels_pred_train = model.predict(diseases_patients_train_model)\n",
    "proba_test = model.predict_proba(diseases_patients_test_model)[:, 1]\n",
    "proba_train = model.predict_proba(diseases_patients_train_model)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_positive_train = np.sum(labels_pred_train==0)\n",
    "nb_negative_train = np.sum(labels_pred_train==1)\n",
    "nb_positive_test = np.sum(labels_pred_test==0)\n",
    "nb_negative_test = np.sum(labels_pred_test==1)\n",
    "\n",
    "TP_test = np.sum((label_patients_test==0 )& (labels_pred_test == 0)) / nb_positive_test\n",
    "FP_test = np.sum((label_patients_test==1 )& (labels_pred_test == 0)) / nb_positive_test\n",
    "TN_test = np.sum((label_patients_test==1 )& (labels_pred_test == 1)) / nb_negative_test\n",
    "FN_test = np.sum((label_patients_test== 0)& (labels_pred_test == 1)) / nb_negative_test\n",
    "\n",
    "TP_train = np.sum((label_patients_train==0 )& (labels_pred_train == 0)) / nb_positive_train\n",
    "FP_train = np.sum((label_patients_train==1 )& (labels_pred_train == 0)) / nb_positive_train\n",
    "TN_train = np.sum((label_patients_train==1 )& (labels_pred_train == 1)) / nb_negative_train\n",
    "FN_train = np.sum((label_patients_train== 0)& (labels_pred_train == 1)) / nb_negative_train\n",
    "\n",
    "accuracy_train = accuracy_score(label_patients_train, labels_pred_train)\n",
    "accuracy_test = accuracy_score(label_patients_test, labels_pred_test)\n",
    "\n",
    "auc_test = calculate_roc_auc(label_patients_test, proba_test)\n",
    "auc_train = calculate_roc_auc(label_patients_train, proba_train)\n",
    "\n",
    "proba_avg_zero_test = 1- np.mean(proba_test[label_patients_test==0])\n",
    "proba_avg_zero_train = 1- np.mean(proba_train[label_patients_train==0])\n",
    "proba_avg_one_test = np.mean(proba_test[label_patients_test==1])\n",
    "proba_avg_one_train = np.mean(proba_train[label_patients_train==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{TP_test=}') \n",
    "print(f'{FP_test=}')\n",
    "print(f'{TN_test=}')\n",
    "print(f'{FN_test=}')\n",
    "print(f'{TP_train=}') \n",
    "print(f'{FP_train=}')\n",
    "print(f'{TN_train=}')\n",
    "print(f'{FN_train=}')\n",
    "print(' ')\n",
    "print(f'{auc_test=}')\n",
    "print(f'{auc_train=}')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(f'{accuracy_test=}')\n",
    "print(f'{accuracy_train=}')\n",
    "print(' ')\n",
    "print(f'{proba_avg_zero_test=}')\n",
    "print(f'{proba_avg_zero_train=}')\n",
    "print(f'{proba_avg_one_test=}')\n",
    "print(f'{proba_avg_one_train=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Plot according to frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_patients_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_patients_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = np.linspace(0, 4000, 20)\n",
    "aucs = []\n",
    "nb_phenos = []\n",
    "for frequency in frequencies:\n",
    "    print(frequency)\n",
    "    indices_keep = frequencies_ini <= frequency\n",
    "    nb_pheno = indices_keep.sum()\n",
    "    diseases_patients_train_keep, diseases_patients_test_keep = diseases_patients_train[:, indices_keep], diseases_patients_test[:, indices_keep]\n",
    "    model = HistGradientBoostingClassifier(class_weight=class_weight)\n",
    "\n",
    "\n",
    "    # Entraîner le modèle sur l'ensemble d'entraînement\n",
    "    model.fit(diseases_patients_train_keep, label_patients_train)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    labels_pred_test = model.predict(diseases_patients_test_keep)\n",
    "    labels_pred_train = model.predict(diseases_patients_train_keep)\n",
    "    proba_test = model.predict_proba(diseases_patients_test_keep)[:, 1]\n",
    "    proba_train = model.predict_proba(diseases_patients_train_keep)[:, 1]\n",
    "\n",
    "    aucs.append(calculate_roc_auc(label_patients_test, proba_test))\n",
    "    nb_phenos.append(nb_pheno)\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(frequencies, aucs, 'o')\n",
    "plt.xlabel('frequencies')\n",
    "plt.ylabel('auc val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs_ones = proba_train\n",
    "true_labels = np.array(label_patients_train)\n",
    "plt.hist(predicted_probs_ones, bins=100)\n",
    "plt.xlabel('proba')\n",
    "plt.ylabel('nb of predictions')\n",
    "plt.show()\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(true_labels, predicted_probs_ones, n_bins=80)\n",
    "auc = calculate_roc_auc(true_labels, predicted_probs_ones)\n",
    "# Tracer le graphique de calibration\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='--', label='Calibration Plot')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nb diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_maladies_train = diseases_patients_train_model_unscaled.sum(axis=1)\n",
    "nb_maladies_test = diseases_patients_test_model_unscaled.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_train = []\n",
    "nbs_train = []\n",
    "for diseases_nb in np.unique(nb_maladies_train):\n",
    "    patients_nb_diseases = diseases_patients_train_model[nb_maladies_train == diseases_nb]\n",
    "    labels_nb_diseases = label_patients_train[nb_maladies_train == diseases_nb]\n",
    "    labels_nb_diseases_pred = labels_pred_train[nb_maladies_train == diseases_nb]\n",
    "    acc = np.sum(labels_nb_diseases == labels_nb_diseases_pred ) / len(labels_nb_diseases)\n",
    "    accs_train.append(acc)\n",
    "    nbs_train.append(len(labels_nb_diseases))\n",
    "nb_diseases_mean_train = np.mean(nb_maladies_train)\n",
    "print(f'nb diseases mean train= {nb_diseases_mean_train}')\n",
    "\n",
    "accs_test = []\n",
    "nbs_test = []\n",
    "for diseases_nb in np.unique(nb_maladies_test):\n",
    "    patients_nb_diseases = diseases_patients_test_model[nb_maladies_test == diseases_nb]\n",
    "    labels_nb_diseases = label_patients_test[nb_maladies_test == diseases_nb]\n",
    "    labels_nb_diseases_pred = labels_pred_test[nb_maladies_test == diseases_nb]\n",
    "    acc = np.sum(labels_nb_diseases == labels_nb_diseases_pred ) / len(labels_nb_diseases)\n",
    "    accs_test.append(acc)\n",
    "    nbs_test.append(len(labels_nb_diseases))\n",
    "nb_diseases_mean_test = np.mean(nb_maladies_test)\n",
    "print(f'nb diseases mean test= {nb_diseases_mean_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_values_nbs_train = nbs_train\n",
    "color_values_nbs_test = nbs_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.unique(nb_maladies_train), accs_train,  c= color_values_nbs_train , cmap='viridis')\n",
    "plt.xlabel('nb of diseases')\n",
    "plt.ylabel('accuracy')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.unique(nb_maladies_test), accs_test,  c= color_values_nbs_test , cmap='viridis')\n",
    "plt.xlabel('nb of diseases')\n",
    "plt.ylabel('accuracy')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = np.sum(diseases_patients_train_model_unscaled, axis=0)\n",
    "log_freq = np.log(frequencies+1)\n",
    "color_values = log_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Compute SHAP values for a set of samples (e.g., X_test)\n",
    "shap_values = explainer.shap_values(diseases_patients_train_model)\n",
    "\n",
    "# Plot the SHAP values\n",
    "#shap.summary_plot(shap_values, diseases_patients_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaps = np.abs(shap_values).mean(axis=0)\n",
    "plt.scatter(np.arange(len(shaps)), shaps,  c= color_values , cmap='viridis')\n",
    "plt.xlabel('phenotypes')\n",
    "plt.ylabel('shap values')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(diseases_patients_train_keep==0, axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_keep = shaps> 0.004\n",
    "print(f'nb patients keep = {indices_keep.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(shaps)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_pheno_par(phenos)[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 297\n",
    "indices_cluster = np.where(clusters==cluster_id)[0]\n",
    "get_name(dataT, indices_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score phenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = np.sum(diseases_patients_train_model_unscaled, axis=0)\n",
    "log_freq = np.log(frequencies+1)\n",
    "color_values = log_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_pheno(data, labels_true, labels_pred, nb_pheno):\n",
    "    coherence = labels_true[data[:,nb_pheno-1]==1] == labels_pred[data[:,nb_pheno-1]==1]\n",
    "    accuracy_pheno = np.sum(coherence)/ len(coherence)\n",
    "    return accuracy_pheno\n",
    "accuracy_pheno_par = partial(get_accuracy_pheno, diseases_patients_test_model_unscaled,  label_patients_test, labels_pred_test)\n",
    "phenos = np.arange(1, diseases_patients_train_model.shape[1]+1)\n",
    "accuracy_pheno_par = np.vectorize(accuracy_pheno_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(phenos)), accuracy_pheno_par(phenos),  c= color_values , cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FT - Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import pandas as pd\n",
    "\n",
    "# Example transactions with integers\n",
    "transactions = [\n",
    "    [1, 2, 3, 4],\n",
    "    [1, 2],\n",
    "    [1, 2, 3, 4],\n",
    "    [1],\n",
    "    [1, 2, 5]\n",
    "]\n",
    "\n",
    "# Convert transactions to transaction encoding format\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(te_ary, columns=range(1, te_ary.shape[1]+1))\n",
    "\n",
    "# Find frequent itemsets using FP-Growth\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.4, use_colnames=True)\n",
    "\n",
    "print(\"Frequent itemsets:\")\n",
    "print(frequent_itemsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ass = data.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_keep.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ass_one = data_ass[labels_patients==1]\n",
    "data_ass_zero= data_ass[labels_patients==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ass_one_r = data_ass_one[:, indices_keep]\n",
    "data_ass_zero_r = data_ass_zero[:, indices_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_1 = pd.DataFrame(data_ass_one_r, columns=range(1, data_ass_one_r.shape[1]+1))\n",
    "df_0 = pd.DataFrame(data_ass_zero_r, columns=range(1, data_ass_zero_r.shape[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_1 = fpgrowth(df_1, min_support=0.001, use_colnames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_0 = fpgrowth(df_0, min_support=0.001, use_colnames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ass_one_r.shape, data_ass_zero_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_0[frequent_itemsets_0['support']> 0.05].sort_values('support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_1[frequent_itemsets_1['support']> 0.05].sort_values('support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = frequent_itemsets_1.merge(frequent_itemsets_0, left_on='itemsets', right_on='itemsets').sort_values('support_x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_zero, support_one = np.array(merged_df['support_y']), np.array(merged_df['support_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_zero, support_one   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = support_one / support_zero\n",
    "res = np.max(np.concatenate([res, res**-1], axis=0).reshape(2, res.shape[0]), axis=0)\n",
    "plt.plot(res, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[3972]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = res > 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = data_ass[:, [16, 296]].sum(axis=1) ==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_patients[indices]==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels_patients[indices]==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['itemsets'][arg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_one[arg], support_zero[arg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = frequent_itemsets_1.itemsets\n",
    "explode = items.explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_map = {}\n",
    "for item in items:\n",
    "    dic_map[item] = explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
