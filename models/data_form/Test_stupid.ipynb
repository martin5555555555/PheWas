{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report\n",
    "from codes.models.metrics import calculate_roc_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = 0.5\n",
    "P1 = 0.6\n",
    "PX = 0.1\n",
    "\n",
    "N = 100000\n",
    "N_variables = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.binomial(1, PX, N)\n",
    "nb_X_1 = np.sum(X == 1)\n",
    "Y = np.zeros(N)\n",
    "Y_1 = np.random.binomial(1, P1, nb_X_1)\n",
    "Y_0 = np.random.binomial(1, P0, N - nb_X_1)\n",
    "Y[X==1] = Y_1\n",
    "Y[X==0] = Y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X, Y)[0, 1]\n",
    "P0_calc = np.sum(Y[X==1]) / nb_X_1\n",
    "P1_calc = np.sum(Y[X==0]) / (N - nb_X_1)\n",
    "diff_p_calc = P0_calc - P1_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, diff_p_calc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model de prediciton\n",
    "X = X.reshape(N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = ['x']\n",
    "# Train a decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Faire des pr√©dictions sur l'ensemble de test\n",
    "labels_pred_test = model.predict(X)\n",
    "labels_pred_train = model.predict(X)\n",
    "proba_test = model.predict_proba(X)[:, 1]\n",
    "proba_train = model.predict_proba(X)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_positive_train = np.sum(labels_pred_train==0)\n",
    "nb_negative_train = np.sum(labels_pred_train==1)\n",
    "nb_positive_test = np.sum(labels_pred_test==0)\n",
    "nb_negative_test = np.sum(labels_pred_test==1)\n",
    "\n",
    "TP_test = np.sum((Y==0 )& (labels_pred_test == 0)) / nb_positive_test\n",
    "FP_test = np.sum((Y==1 )& (labels_pred_test == 0)) / nb_positive_test\n",
    "TN_test = np.sum((Y==1 )& (labels_pred_test == 1)) / nb_negative_test\n",
    "FN_test = np.sum((Y== 0)& (labels_pred_test == 1)) / nb_negative_test\n",
    "\n",
    "TP_train = np.sum((Y==0 )& (labels_pred_train == 0)) / nb_positive_train\n",
    "FP_train = np.sum((Y==1 )& (labels_pred_train == 0)) / nb_positive_train\n",
    "TN_train = np.sum((Y==1 )& (labels_pred_train == 1)) / nb_negative_train\n",
    "FN_train = np.sum((Y== 0)& (labels_pred_train == 1)) / nb_negative_train\n",
    "\n",
    "accuracy_train = accuracy_score(Y, labels_pred_train)\n",
    "accuracy_test = accuracy_score(Y, labels_pred_test)\n",
    "\n",
    "auc_test = calculate_roc_auc(Y, proba_test)\n",
    "auc_train = calculate_roc_auc(Y, proba_train)\n",
    "\n",
    "proba_avg_zero_test = 1- np.mean(proba_test[Y==0])\n",
    "proba_avg_zero_train = 1- np.mean(proba_train[Y==0])\n",
    "proba_avg_one_test = np.mean(proba_test[Y==1])\n",
    "proba_avg_one_train = np.mean(proba_train[Y==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{TP_test=}') \n",
    "print(f'{FP_test=}')\n",
    "print(f'{TN_test=}')\n",
    "print(f'{FN_test=}')\n",
    "print(f'{TP_train=}') \n",
    "print(f'{FP_train=}')\n",
    "print(f'{TN_train=}')\n",
    "print(f'{FN_train=}')\n",
    "print(' ')\n",
    "print(f'{auc_test=}')\n",
    "print(f'{auc_train=}')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(f'{accuracy_test=}')\n",
    "print(f'{accuracy_train=}')\n",
    "print(' ')\n",
    "print(f'{proba_avg_zero_test=}')\n",
    "print(f'{proba_avg_zero_train=}')\n",
    "print(f'{proba_avg_one_test=}')\n",
    "print(f'{proba_avg_one_train=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_predict = PX * max(P1, 1-P1) + (1 - PX) * max(P0, 1-P0)\n",
    "acc_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
