{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functionnal version with optionnal mask padding and dropouts, see Transformer_V1.ipynb for example\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "\n",
    "### imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from codes.models.Transformers.Embedding import EmbeddingPheno\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "\n",
    "### Transformer's instance\n",
    "# B, S, E, H, HN, MH = Batch_len, Sentence_len, Embedding_len, Head_size, Head number, MultiHead size.\n",
    "\n",
    "def custom_softmax(input_tensor, dim=-1):\n",
    "    # Calculer le softmax classique\n",
    "    softmax_output = F.softmax(input_tensor, dim)\n",
    "\n",
    "    # Trouver les colonnes avec que des -inf\n",
    "    nan_columns = torch.all(input_tensor == float('-inf'), dim=dim)\n",
    "\n",
    "    # Remplacer les valeurs de softmax par 0 pour les colonnes avec que des -inf\n",
    "    softmax_output[nan_columns] = 0\n",
    "\n",
    "    return softmax_output\n",
    "\n",
    "class TransformerGeneModel_V2(nn.Module):\n",
    "    def __init__(self, pheno_method, Embedding, Head_size, binary_classes, n_head, n_layer, mask_padding=False, padding_token=None, p_dropout=0, device='cpu', loss_version='cross_entropy', gamma=2, alpha=1, instance_size=None, proj_embed=True):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.Embedding_size = Embedding.Embedding_size\n",
    "        self.binary_classes = binary_classes\n",
    "        self.instance_size=instance_size\n",
    "        self.proj_embed = proj_embed\n",
    "        if not self.proj_embed:\n",
    "            self.instance_size = self.Embedding_size\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size)\n",
    "        self.classes_nb = 2 if self.binary_classes else 3\n",
    "        self.blocks =PadMaskSequential(*[Block(self.instance_size, n_head=n_head, Head_size=Head_size, p_dropout=p_dropout) for _ in range(n_layer)]) #Block(self.instance_size, n_head=n_head, Head_size=Head_size) \n",
    "        self.ln_f = nn.LayerNorm(self.instance_size) # final layer norm\n",
    "        self.lm_head_logits = nn.Linear(self.instance_size, self.classes_nb) \n",
    "        self.lm_head_proba = nn.Linear(self.instance_size,1) # plus one for the probabilities\n",
    "        self.Embedding = Embedding\n",
    "        self.mask_padding = mask_padding\n",
    "        self.padding_token = padding_token\n",
    "        self.padding_mask = None\n",
    "        self.device = device\n",
    "        self.pheno_method = pheno_method\n",
    "        \n",
    "        self.loss_version = loss_version\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.shap = False\n",
    "       \n",
    "        self.diseases_embedding_table = Embedding.distinct_diseases_embeddings\n",
    "        if self.pheno_method == 'Paul':\n",
    "            self.counts_embedding_table = Embedding.counts_embeddings\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, diseases_sentence):\n",
    "        B, S = np.shape(diseases_sentence)[0], np.shape(diseases_sentence)[1]\n",
    "        mask = torch.where(diseases_sentence==self.padding_token)\n",
    "        padding_mask_mat = torch.ones((B, S, S), dtype=torch.int)\n",
    "        padding_mask_mat[mask] = 0\n",
    "        padding_mask_mat.transpose(-2,-1)[mask] = 0\n",
    "\n",
    "        \n",
    "        padding_mask_probas = torch.zeros((B, S))\n",
    "        padding_mask_probas[mask] = -torch.inf\n",
    "        padding_mask_probas = padding_mask_probas.view(B, S)\n",
    "        return padding_mask_mat.to(self.device), padding_mask_probas.to(self.device) # 1 if masked, 0 else\n",
    "\n",
    "    def set_padding_mask_transformer(self, padding_mask, padding_mask_probas):\n",
    "        self.padding_mask = padding_mask\n",
    "        self.padding_mask_probas = padding_mask_probas\n",
    "\n",
    "    def forward(self, diseases_sentence, counts_diseases, targets=None):\n",
    "\n",
    "        diseases_sentence = diseases_sentence.to(torch.long)\n",
    "        counts_diseases = counts_diseases.to(torch.long)\n",
    "\n",
    "        Batch_len, Sentence_len = diseases_sentence.shape\n",
    "\n",
    "        diseases_sentence = diseases_sentence.to(self.device)\n",
    "        counts_diseases = counts_diseases.to(self.device)\n",
    "        \n",
    "        if targets!=None:\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask, padding_mask_probas = self.create_padding_mask(diseases_sentence)\n",
    "            self.set_padding_mask_transformer(padding_mask, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask)\n",
    "\n",
    "        diseases_sentences_embedded = self.diseases_embedding_table(diseases_sentence) # shape B, S, E\n",
    "\n",
    "        x = diseases_sentences_embedded \n",
    "    \n",
    "        if self.pheno_method == 'Paul':\n",
    "            counts_diseases_embedded = self.counts_embedding_table(counts_diseases) # shape B, S, E\n",
    "            x = x + counts_diseases_embedded # shape B, S, E \n",
    "        \n",
    "        if self.proj_embed:\n",
    "            x = self.projection_embed(x)\n",
    "        x = self.blocks(x) # shape B, S, E\n",
    "        x = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        if self.mask_padding:\n",
    "            weights_logits = weights_logits + self.padding_mask_probas\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        #if self.mask_padding:\n",
    "           # probas = probas * self.padding_mask_probas\n",
    "        \n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, targets, self.loss_version, self.gamma, self.alpha)\n",
    "\n",
    "        if self.shap:\n",
    "            return logits[:,0].view(Batch_len, 1)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def forward_decomposed(self, diseases_sentence, diseases_count):\n",
    "        self.list_attention_layers = []\n",
    "        Batch_len, Sentence_len = diseases_sentence.shape\n",
    "        print('coucou')\n",
    "        diseases_sentence = diseases_sentence.to(self.device)\n",
    "        counts_diseases = diseases_count.to(self.device)\n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask, padding_mask_probas = self.create_padding_mask(diseases_sentence)\n",
    "            self.set_padding_mask_transformer(padding_mask, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask)\n",
    "\n",
    "        diseases_sentences_embedded = self.diseases_embedding_table(diseases_sentence) # shape B, S, E\n",
    "\n",
    "        x = diseases_sentences_embedded \n",
    "        if self.pheno_method == 'Paul':\n",
    "            counts_diseases_embedded = self.counts_embedding_table(counts_diseases) # shape B, S, E\n",
    "            x = x + counts_diseases_embedded # shape B, S, E \n",
    "        if self.proj_embed:\n",
    "            x = self.projection_embed(x)\n",
    "        x= self.blocks.forward_decompose(x, self.list_attention_layers) # shape B, S, E\n",
    "        x_out = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x_out) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        if self.mask_padding:\n",
    "            weights_logits = weights_logits + self.padding_mask_probas\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        #if self.mask_padding:\n",
    "           # probas = probas * self.padding_mask_probas\n",
    "        \n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1))# (B,Classes_Numb) Weighted Average logits\n",
    "        return logits, probas, x_out\n",
    "    \n",
    "\n",
    "    def predict(self, diseases_sentence, diseases_count):\n",
    "        logits, _ = self(diseases_sentence, diseases_count) # shape B, Classes_nb\n",
    "        return torch.argmax(logits, dim=1)  # (B,)\n",
    "        \n",
    "    def predict_proba(self, diseases_sentence, diseases_count):\n",
    "        logits, _ = self(diseases_sentence, diseases_count)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate(self, dataloader_test):\n",
    "        print('beginning inference evaluation')\n",
    "        start_time_inference = time.time()\n",
    "        predicted_labels_list = []\n",
    "        predicted_probas_list = []\n",
    "        true_labels_list = []\n",
    "\n",
    "        total_loss = 0.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_sentences, batch_counts, batch_labels in dataloader_test:\n",
    "\n",
    "\n",
    "                logits, loss = self(batch_sentences, batch_counts,  batch_labels)\n",
    "                total_loss += loss.item()\n",
    "                predicted_labels = self.predict(batch_sentences, batch_counts)\n",
    "                predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "                predicted_probas = self.predict_proba(batch_sentences, batch_counts)\n",
    "                predicted_probas_list.extend(predicted_probas.cpu().numpy())\n",
    "                true_labels_list.extend(batch_labels.cpu().numpy())\n",
    "        f1 = f1_score(true_labels_list, predicted_labels_list, average='macro')\n",
    "        accuracy = accuracy_score(true_labels_list, predicted_labels_list)\n",
    "        auc_score = calculate_roc_auc(true_labels_list, np.array(predicted_probas_list)[:, 1], return_nan=True)\n",
    "        proba_avg_zero, proba_avg_one = get_proba(true_labels_list, predicted_probas_list)\n",
    "    \n",
    "        self.train()\n",
    "        print(f'end inference evaluation in {time.time() - start_time_inference}s')\n",
    "        return f1, accuracy, auc_score, total_loss/len(dataloader_test), proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list\n",
    "\n",
    "\n",
    "    def write_embedding(self, writer):\n",
    "        if self.proj_embed:\n",
    "            embedding_tensor = self.projection_embed(self.diseases_embedding_table.weight).detach().cpu().numpy()\n",
    "        else:\n",
    "            embedding_tensor = self.diseases_embedding_table.weight.detach().cpu().numpy()\n",
    "\n",
    "        writer.add_embedding(embedding_tensor, metadata=self.Embedding.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class PadMaskSequential(nn.Sequential):\n",
    "    def __init__(self, *args):\n",
    "        super(PadMaskSequential, self).__init__(*args)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sequential(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x, list_attention_layers):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module.forward_decompose(x, list_attention_layers)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, instance_size, n_head, Head_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadSelfAttention(n_head, Head_size, instance_size, p_dropout)\n",
    "        self.ffwd = FeedForward(instance_size, p_dropout)\n",
    "        self.ln1 = nn.LayerNorm(instance_size)\n",
    "        self.ln2 = nn.LayerNorm(instance_size)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_block(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        #x = self.ln1(x)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x, list_attention_layers=None):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        out_sa, attention_probas = self.sa.forward_decompose(x)\n",
    "        if list_attention_layers != None:\n",
    "            list_attention_layers.append(attention_probas)\n",
    "        x = out_sa + x\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, Head_size, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.qkv_network = nn.Linear(instance_size, Head_size * 3, bias = False) #group the computing of the nn.Linear for q, k and v, shape \n",
    "        self.proj = nn.Linear(Head_size, instance_size)\n",
    "        self.attention_dropout = nn.Dropout(p_dropout)\n",
    "        self.resid_dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "        self.multi_head_size = Head_size // n_head\n",
    "        self.flash = False\n",
    "        self.n_head = n_head\n",
    "        self.Head_size = Head_size\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sa(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "        q, k, v  = self.qkv_network(x).split(self.Head_size, dim=2) #q, k, v of shape (B, S, H)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            if self.padding_mask != None:\n",
    "                padding_mask_weights = -(1-self.padding_mask)*(10**10)\n",
    "                attention_weights = (attention_weights.transpose(0, 1)+padding_mask_weights).transpose(0, 1)\n",
    "            #print(f'wei0={attention_weights}')\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas = self.attention_dropout(attention_probas)\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out        \n",
    "    \n",
    "    def forward_decompose(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "        q, k, v  = self.qkv_network(x).split(self.Head_size, dim=2) #q, k, v of shape (B, S, H)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            if self.padding_mask != None:\n",
    "                padding_mask_weights = -(1-self.padding_mask)*(10**10)\n",
    "                attention_weights = (attention_weights.transpose(0, 1)+padding_mask_weights).transpose(0, 1)\n",
    "\n",
    "\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas_dropout = self.attention_dropout(attention_probas)\n",
    "\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas_dropout @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        #out = self.resid_dropout(out)\n",
    "        return out, attention_probas   \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear( instance_size, 4 * instance_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * instance_size, instance_size),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def calculate_roc_auc(y_true, predicted_probabilities, return_nan=False):\n",
    "    # Check the number of unique classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # Check if there is more than one class\n",
    "    if num_classes > 1:\n",
    "        # Compute ROC-AUC score\n",
    "        roc_auc = roc_auc_score(y_true, predicted_probabilities)\n",
    "        return roc_auc\n",
    "    else:\n",
    "        # Return NaN if there is only one class\n",
    "        if return_nan:\n",
    "            return np.nan\n",
    "        else:\n",
    "            print(\"Only one class present in y_true. ROC AUC score is not defined in that case.\")\n",
    "def calculate_classification_report(true_labels_list, predicted_labels_list, return_nan=True):\n",
    "    # Check the number of unique classes\n",
    "    num_classes = len(np.unique(true_labels_list))\n",
    "\n",
    "    # Check if there is more than one class\n",
    "    if num_classes > 1:\n",
    "        # Compute ROC-AUC score\n",
    "        report = classification_report(true_labels_list, predicted_labels_list)\n",
    "        return report\n",
    "    else:\n",
    "        # Return NaN if there is only one class\n",
    "        if return_nan:\n",
    "            return np.nan\n",
    "        else:\n",
    "            print(\"Only one class present in y_true. ROC AUC score is not defined in that case.\")\n",
    "\n",
    "def get_proba(true_labels_list, predicted_probas_list):\n",
    "    avg_proba_zero = np.mean(np.array(predicted_probas_list)[:,0][np.array(true_labels_list)==0])\n",
    "    avg_proba_one = np.mean(np.array(predicted_probas_list)[:,1][np.array(true_labels_list)==1])\n",
    "    return avg_proba_zero, avg_proba_one\n",
    "\n",
    "def calculate_loss(logits, targets=None, loss_type='cross_entropy', gamma=None, alpha=None):\n",
    "    if targets is None:\n",
    "            loss = None\n",
    "    else:\n",
    "        #target : shape B,\n",
    "        \n",
    "        if loss_type == 'cross_entropy':\n",
    "            cross_entropy = F.cross_entropy(logits, targets)\n",
    "            return cross_entropy\n",
    "        elif loss_type == 'focal_loss':\n",
    "            alphas = ((1 - targets) * (alpha-1)).to(torch.float) + 1\n",
    "            probas = F.softmax(logits)\n",
    "            certidude_factor =  (1-probas[[range(probas.shape[0]), targets]])**gamma * alphas\n",
    "            cross_entropy = F.cross_entropy(logits, targets, reduce=False)\n",
    "            loss = torch.dot(cross_entropy, certidude_factor)\n",
    "            return loss\n",
    "        elif loss_type == 'predictions':\n",
    "            probas = F.softmax(logits)\n",
    "            predictions = (probas[:,1] > 0.5).to(int)\n",
    "            return torch.tensor(torch.sum((predictions-targets)**2)/len(predictions), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding')\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingPheno(nn.Module):\n",
    "    def __init__(self, method=None, vocab_size=None, max_count_same_disease=None, Embedding_size=None, rollup_depth=4, freeze_embed=False, dicts=None):\n",
    "        super(EmbeddingPheno, self).__init__()\n",
    "\n",
    "        self.dicts = dicts\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.nb_distinct_diseases_patient = vocab_size\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.metadata = None\n",
    "\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        \n",
    "        if method == None:\n",
    "            self.distinct_diseases_embeddings = nn.Embedding(vocab_size, Embedding_size)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, Embedding_size)\n",
    "            torch.nn.init.normal_(self.distinct_diseases_embeddings.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.normal_(self.counts_embeddings.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        elif method == 'Abby':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "\n",
    "\n",
    "\n",
    "        elif method=='Paul':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "    def write_embedding(self, writer):\n",
    "            embedding_tensor = self.distinct_diseases_embeddings.weight.data.detach().cpu().numpy()\n",
    "            writer.add_embedding(embedding_tensor, metadata=self.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class EmbeddingPhenoCat(nn.Module):\n",
    "    def __init__(self, method=None, Embedding_size=10, rollup_depth=4, freeze_embed=False, dic_embedding_cat_params={}):\n",
    "        super(EmbeddingPheno, self).__init__()\n",
    "\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.dic_embedding_cat_params = dic_embedding_cat_params\n",
    "        dic_embedding_cat = {}\n",
    "        for cat, max_number  in self.dic_embedding_cat:\n",
    "        \n",
    "            if cat=='diseases':\n",
    "                if method == None:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, Embedding_size)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                elif method == 'Abby':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "\n",
    "                elif method=='Paul':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)\n",
    "                    dic_embedding_cat[cat] = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "                    self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            if cat=='counts':\n",
    "                if (method == None) or (method == 'Paul') :\n",
    "                    dic_embedding_cat['counts_embeddings'] = nn.Embedding(max_number, self.Embedding_size)\n",
    "                    torch.nn.init.normal_(self.counts_embeddings.weight, mean=0.0, std=0.02)\n",
    "\n",
    "            else:\n",
    "                dic_embedding_cat[cat] = nn.Embedding(max_number, Embedding_size)\n",
    "                torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "        self.dic_embedding_cat = dic_embedding_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patient = 1000\n",
    "val_possibles = np.arange(1, 4, dtype=int)\n",
    "indices_random = np.unique(np.random.randint(0,3, size=(3)))\n",
    "\n",
    "patient_list = np.zeros((n_patient, 3))\n",
    "indices_1 = np.random.randint(0,2, n_patient)\n",
    "indices_2 =np.random.randint(0,2, n_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_diseases_list = []\n",
    "for i in range(n_patient):\n",
    "    indices_random = np.unique(np.random.randint(0,3, size=(3)))\n",
    "    np.random.shuffle(indices_random)\n",
    "    patient_diseases_list.append(np.concatenate([val_possibles[indices_random], np.zeros(3-len(val_possibles[indices_random]),dtype=int)]))\n",
    "patient_diseases_list = np.array(patient_diseases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_counts_list = np.ones((n_patient,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = []\n",
    "for patient in patient_diseases_list:\n",
    "    res = np.isin(np.array([1, 2, 3]), patient)\n",
    "    if res[0] and not res[1]:\n",
    "        labels_list.append(1)\n",
    "    else:\n",
    "        labels_list.append(0)\n",
    "labels_list = np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding  = EmbeddingPheno(method=None, vocab_size=4, max_count_same_disease=1, Embedding_size=10, rollup_depth=4, freeze_embed=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerGeneModel_V2(pheno_method = 'Abby',\n",
    "                             Embedding = Embedding,\n",
    "                             Head_size=6,\n",
    "                             binary_classes=True,\n",
    "                             n_head=2,\n",
    "                             n_layer=2,\n",
    "                             mask_padding=True, \n",
    "                             padding_token=0, \n",
    "                             p_dropout=0, \n",
    "                             loss_version = 'cross_entropy',\n",
    "                             gamma = 0,\n",
    "                             instance_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equalize:\n",
    "patient_list_ones = patient_diseases_list[labels_list==1]\n",
    "patient_list_zeros = patient_diseases_list[labels_list==0][:len(patient_list_ones)]\n",
    "patient_diseases_list = np.concatenate([patient_list_ones, patient_list_zeros])\n",
    "labels_list = np.array([1]*len(patient_list_ones) + [0]*len(patient_list_zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_train = 0.8\n",
    "n_train = int(len(patient_diseases_list)*prop_train)\n",
    "data_train = list(zip(patient_diseases_list[:n_train],patient_counts_list[:n_train], labels_list[:n_train]))\n",
    "data_test = list(zip(patient_diseases_list[n_train:],patient_counts_list[n_train:], labels_list[n_train:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(data_train, batch_size=20, shuffle=True)\n",
    "dataloader_test = DataLoader(data_train, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentence, batch_counts, batch_labels = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, probas, x_out = model.forward_decomposed(batch_sentence, batch_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.padding_mask_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas = model.list_attention_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = torch.rand(20,2, 3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = model.padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### padding mask #####\n",
    "padding_mask_weights = -((1-padding_mask)*(10**10))\n",
    "attention_weights = (attention_weights.transpose(0, 1)+padding_mask_weights).transpose(0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "\n",
    "attention_probas = (attention_probas.transpose(0, 1)*padding_mask).transpose(0, 1)\n",
    "# attention_probas[attention_probas.isnan()]=0\n",
    "attention_probas_dropout = self.attention_dropout(attention_probas)\n",
    "\n",
    "\n",
    "#print(f'wei1={attention_probas}')\n",
    "#attention_probas = self.dropout(attention_probas)\n",
    "## weighted aggregation of the values\n",
    "out = attention_probas_dropout @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "out = self.proj(out)\n",
    "#out = self.resid_dropout(ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, probas, x_out = model.forward_decomposed(batch_sentence, batch_counts)\n",
    "attention_probas = model.list_attention_layers\n",
    "indice_sentence = 0\n",
    "indice_layer = 0\n",
    "indice_head = 0\n",
    "attention_probas = attention_probas[indice_layer].detach().numpy()[indice_sentence][indice_head]\n",
    "mask = model.padding_mask.detach().numpy()[indice_sentence]\n",
    "n_real = np.sum(mask[0])\n",
    "\n",
    "attention_probas_masked = attention_probas[mask].reshape(n_real, n_real)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_probas_masked, cmap=\"YlGnBu\", annot=True, fmt=\".2f\", cbar=False)\n",
    "\n",
    "# Ajoutez des Ã©tiquettes pour les axes\n",
    "plt.xlabel(\"Token\")\n",
    "plt.ylabel(\"Token\")\n",
    "plt.title(\"Self-Attention Matrix\")\n",
    "\n",
    "# Affichez le plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas[indice_sentence][indice_head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, total_epochs+1):\n",
    "\n",
    "    total_loss = 0.0  \n",
    "    \n",
    "    #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "    for k, (batch_sentences, batch_counts, batch_labels) in enumerate(dataloader_train):\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(batch_sentences, batch_counts, batch_labels)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "    \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, probas, attention_probas, attention_weights, attention_weights_bis,  x_out = model.forward_decomposed(batch_sentences, batch_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_probas[0] * (1-model.padding_mask[0].to(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.padding_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict(batch_sentences, batch_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, accuracy, auc_score, loss, proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list=model.evaluate(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_list = np.array(true_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas = np.array(predicted_probas_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = (pred_probas[:,0] < 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pred_labels==true_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, probas, attention_probas, x_out = model.forward_decomposed(batch_sentences, batch_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 3\n",
    "logits[ind], x_out[ind], batch_sentences[ind], probas[ind], attention_probas[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pred_labels==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.rand(2, 3, 1)\n",
    "mask = torch.zeros(2, 3, 1).to(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0][0][0] = True\n",
    "mask[0][1][0] = True\n",
    "mask[1][1][0] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas[mask]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.projection_embed(model.diseases_embedding_table.weight).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.diseases_embedding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
