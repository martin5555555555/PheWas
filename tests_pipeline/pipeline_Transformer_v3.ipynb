{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functionnal version with optionnal mask padding and dropouts, see Transformer_V1.ipynb for example\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "from torch.utils.data import DataLoader\n",
    "from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LambdaLR, LinearLR, SequentialLR\n",
    "from codes.models.utils import clear_last_line, print_file, number_tests, Unbuffered, plot_infos, plot_ini_infos\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList, Patient\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functionnal version with optionnal mask padding and dropouts, see Transformer_V1.ipynb for example\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "\n",
    "### imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from codes.models.Transformers.Embedding import EmbeddingPheno\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "\n",
    "### Transformer's instance\n",
    "# B, S, E, H, HN, MH = Batch_len, Sentence_len, Embedding_len, Head_size, Head number, MultiHead size.\n",
    "\n",
    "def custom_softmax(input_tensor, dim=-1):\n",
    "    # Calculer le softmax classique\n",
    "    softmax_output = F.softmax(input_tensor, dim)\n",
    "\n",
    "    # Trouver les colonnes avec que des -inf\n",
    "    nan_columns = torch.all(input_tensor == float('-inf'), dim=dim)\n",
    "\n",
    "    # Remplacer les valeurs de softmax par 0 pour les colonnes avec que des -inf\n",
    "    softmax_output[nan_columns] = 0\n",
    "\n",
    "    return softmax_output\n",
    "\n",
    "class TransformerGeneModel_V2(nn.Module):\n",
    "    def __init__(self, pheno_method, Embedding, Head_size, binary_classes, n_head, n_layer, mask_padding=False, padding_token=None, p_dropout=0, device='cpu', loss_version='cross_entropy', gamma=2, alpha=1, instance_size=None, proj_embed=True, L1=False):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.Embedding_size = Embedding.Embedding_size\n",
    "        self.binary_classes = binary_classes\n",
    "        self.instance_size=instance_size\n",
    "        self.proj_embed = proj_embed\n",
    "        if not self.proj_embed:\n",
    "            self.instance_size = self.Embedding_size\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size)\n",
    "        self.classes_nb = 2 if self.binary_classes else 3\n",
    "        self.blocks =PadMaskSequential(*[Block(self.instance_size, n_head=n_head, Head_size=Head_size, p_dropout=p_dropout) for _ in range(n_layer)]) #Block(self.instance_size, n_head=n_head, Head_size=Head_size) \n",
    "        self.ln_f = nn.LayerNorm(self.instance_size) # final layer norm\n",
    "        self.lm_head_logits = nn.Linear(self.instance_size, self.classes_nb) \n",
    "        self.lm_head_proba = nn.Linear(self.instance_size,1) # plus one for the probabilities\n",
    "        self.Embedding_gather = nn.Embedding(1000, self.instance_size)\n",
    "        self.Embedding = Embedding\n",
    "        self.mask_padding = mask_padding\n",
    "        self.padding_token = padding_token\n",
    "        self.gather_pheno_logits = nn.Linear(self.instance_size, 1)\n",
    "        self.padding_mask = None\n",
    "        self.device = device\n",
    "        self.pheno_method = pheno_method\n",
    "        \n",
    "        \n",
    "        self.loss_version = loss_version\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.shap = False\n",
    "        self.L1 = L1\n",
    "       \n",
    "        self.diseases_embedding_table = Embedding.distinct_diseases_embeddings\n",
    "        if self.pheno_method == 'Paul':\n",
    "            self.counts_embedding_table = Embedding.counts_embeddings\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, diseases_sentence):\n",
    "        B, S = np.shape(diseases_sentence)[0], np.shape(diseases_sentence)[1]\n",
    "        mask = torch.where(diseases_sentence==self.padding_token)\n",
    "        padding_mask_mat = torch.ones((B, S, S), dtype=torch.int)\n",
    "        padding_mask_mat[mask] = 0\n",
    "        padding_mask_mat.transpose(-2,-1)[mask] = 0\n",
    "\n",
    "        \n",
    "        padding_mask_probas = torch.zeros((B, S))\n",
    "        padding_mask_probas[mask] = -torch.inf\n",
    "        padding_mask_probas = padding_mask_probas.view(B, S)\n",
    "        return padding_mask_mat.to(self.device), padding_mask_probas.to(self.device) # 1 if masked, 0 else\n",
    "\n",
    "    def set_padding_mask_transformer(self, padding_mask, padding_mask_probas):\n",
    "        self.padding_mask = padding_mask\n",
    "        self.padding_mask_probas = padding_mask_probas\n",
    "\n",
    "    def forward(self, diseases_sentence, counts_diseases, targets=None):\n",
    "\n",
    "        diseases_sentence = diseases_sentence.to(torch.long)\n",
    "        counts_diseases = counts_diseases.to(torch.long)\n",
    "\n",
    "        Batch_len, Sentence_len = diseases_sentence.shape\n",
    "\n",
    "        diseases_sentence = diseases_sentence.to(self.device)\n",
    "        counts_diseases = counts_diseases.to(self.device)\n",
    "        \n",
    "        if targets!=None:\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask, padding_mask_probas = self.create_padding_mask(diseases_sentence)\n",
    "            self.set_padding_mask_transformer(padding_mask, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask)\n",
    "\n",
    "        diseases_sentences_embedded = self.diseases_embedding_table(diseases_sentence) # shape B, S, E\n",
    "\n",
    "        x = diseases_sentences_embedded \n",
    "\n",
    "        if self.pheno_method == 'Paul':\n",
    "            counts_diseases_embedded = self.counts_embedding_table(counts_diseases) # shape B, S, E\n",
    "            x = x + counts_diseases_embedded # shape B, S, E \n",
    "        \n",
    "        if self.proj_embed:\n",
    "            x = self.projection_embed(x)\n",
    "\n",
    "        print(x.shape)\n",
    "        gather_logits = self.gather_pheno_logits(x).view(Batch_len, Sentence_len)\n",
    "        print(gather_logits.shape)\n",
    "        \n",
    "        if self.mask_padding:\n",
    "            gather_logits = gather_logits + self.padding_mask_probas\n",
    "        gather_probas = F.softmax(gather_logits)\n",
    "\n",
    "        x = (self.Embedding_gather.weight.T * gather_probas).sum(axis = 1) # similar to retranspose and sum for axis=0\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        x = self.blocks(x) # shape B, S, E\n",
    "        x = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        if self.mask_padding:\n",
    "            weights_logits = weights_logits + self.padding_mask_probas\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        #if self.mask_padding:\n",
    "           # probas = probas * self.padding_mask_probas\n",
    "        \n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, probas, targets, self.loss_version, self.gamma, self.alpha, L1=self.L1)\n",
    "\n",
    "        if self.shap:\n",
    "            return logits[:,0].view(Batch_len, 1)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def forward_decomposed(self, diseases_sentence, diseases_count):\n",
    "        self.list_attention_layers = []\n",
    "        Batch_len, Sentence_len = diseases_sentence.shape\n",
    "        print('coucou')\n",
    "        diseases_sentence = diseases_sentence.to(self.device)\n",
    "        counts_diseases = diseases_count.to(self.device)\n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask, padding_mask_probas = self.create_padding_mask(diseases_sentence)\n",
    "            self.set_padding_mask_transformer(padding_mask, padding_mask_probas)\n",
    "            self.blocks.set_padding_mask_sequential(self.padding_mask)\n",
    "\n",
    "        diseases_sentences_embedded = self.diseases_embedding_table(diseases_sentence) # shape B, S, E\n",
    "\n",
    "        x = diseases_sentences_embedded \n",
    "        if self.pheno_method == 'Paul':\n",
    "            counts_diseases_embedded = self.counts_embedding_table(counts_diseases) # shape B, S, E\n",
    "            x = x + counts_diseases_embedded # shape B, S, E \n",
    "        if self.proj_embed:\n",
    "            x = self.projection_embed(x)\n",
    "        x= self.blocks.forward_decompose(x, self.list_attention_layers) # shape B, S, E\n",
    "        x_out = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x_out) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        if self.mask_padding:\n",
    "            weights_logits = weights_logits + self.padding_mask_probas\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        #if self.mask_padding:\n",
    "           # probas = probas * self.padding_mask_probas\n",
    "        \n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1))# (B,Classes_Numb) Weighted Average logits\n",
    "        return logits, probas, x_out\n",
    "    \n",
    "\n",
    "    def predict(self, diseases_sentence, diseases_count):\n",
    "        logits, _ = self(diseases_sentence, diseases_count) # shape B, Classes_nb\n",
    "        return torch.argmax(logits, dim=1)  # (B,)\n",
    "        \n",
    "    def predict_proba(self, diseases_sentence, diseases_count):\n",
    "        logits, _ = self(diseases_sentence, diseases_count)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate(self, dataloader_test):\n",
    "        print('beginning inference evaluation')\n",
    "        start_time_inference = time.time()\n",
    "        predicted_labels_list = []\n",
    "        predicted_probas_list = []\n",
    "        true_labels_list = []\n",
    "\n",
    "        total_loss = 0.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_sentences, batch_counts, batch_labels in dataloader_test:\n",
    "\n",
    "\n",
    "                logits, loss = self(batch_sentences, batch_counts,  batch_labels)\n",
    "                total_loss += loss.item()\n",
    "                predicted_labels = self.predict(batch_sentences, batch_counts)\n",
    "                predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "                predicted_probas = self.predict_proba(batch_sentences, batch_counts)\n",
    "                predicted_probas_list.extend(predicted_probas.cpu().numpy())\n",
    "                true_labels_list.extend(batch_labels.cpu().numpy())\n",
    "        f1 = f1_score(true_labels_list, predicted_labels_list, average='macro')\n",
    "        accuracy = accuracy_score(true_labels_list, predicted_labels_list)\n",
    "        auc_score = calculate_roc_auc(true_labels_list, np.array(predicted_probas_list)[:, 1], return_nan=True)\n",
    "        proba_avg_zero, proba_avg_one = get_proba(true_labels_list, predicted_probas_list)\n",
    "    \n",
    "        self.train()\n",
    "        print(f'end inference evaluation in {time.time() - start_time_inference}s')\n",
    "        return f1, accuracy, auc_score, total_loss/len(dataloader_test), proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list\n",
    "\n",
    "\n",
    "    def write_embedding(self, writer):\n",
    "        if self.proj_embed:\n",
    "            embedding_tensor = self.projection_embed(self.diseases_embedding_table.weight).detach().cpu().numpy()\n",
    "        else:\n",
    "            embedding_tensor = self.diseases_embedding_table.weight.detach().cpu().numpy()\n",
    "\n",
    "        writer.add_embedding(embedding_tensor, metadata=self.Embedding.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class PadMaskSequential(nn.Sequential):\n",
    "    def __init__(self, *args):\n",
    "        super(PadMaskSequential, self).__init__(*args)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sequential(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x, list_attention_layers):\n",
    "        for module in self:\n",
    "            module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module.forward_decompose(x, list_attention_layers)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, instance_size, n_head, Head_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadSelfAttention(n_head, Head_size, instance_size, p_dropout)\n",
    "        self.ffwd = FeedForward(instance_size, p_dropout)\n",
    "        self.ln1 = nn.LayerNorm(instance_size)\n",
    "        self.ln2 = nn.LayerNorm(instance_size)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_block(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        #x = self.ln1(x)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x, list_attention_layers=None):\n",
    "        self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        out_sa, attention_probas = self.sa.forward_decompose(x)\n",
    "        if list_attention_layers != None:\n",
    "            list_attention_layers.append(attention_probas)\n",
    "        x = out_sa + x\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, Head_size, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.qkv_network = nn.Linear(instance_size, Head_size * 3, bias = False) #group the computing of the nn.Linear for q, k and v, shape \n",
    "        self.proj = nn.Linear(Head_size, instance_size)\n",
    "        self.attention_dropout = nn.Dropout(p_dropout)\n",
    "        self.resid_dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "        self.multi_head_size = Head_size // n_head\n",
    "        self.flash = False\n",
    "        self.n_head = n_head\n",
    "        self.Head_size = Head_size\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sa(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "        q, k, v  = self.qkv_network(x).split(self.Head_size, dim=2) #q, k, v of shape (B, S, H)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            if self.padding_mask != None:\n",
    "                padding_mask_weights = -(1-self.padding_mask)*(10**10)\n",
    "                attention_weights = (attention_weights.transpose(0, 1)+padding_mask_weights).transpose(0, 1)\n",
    "            #print(f'wei0={attention_weights}')\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas = self.attention_dropout(attention_probas)\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out        \n",
    "    \n",
    "    def forward_decompose(self, x):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "        q, k, v  = self.qkv_network(x).split(self.Head_size, dim=2) #q, k, v of shape (B, S, H)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            if self.padding_mask != None:\n",
    "                padding_mask_weights = -(1-self.padding_mask)*(10**10)\n",
    "                attention_weights = (attention_weights.transpose(0, 1)+padding_mask_weights).transpose(0, 1)\n",
    "\n",
    "\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if self.padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*self.padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas_dropout = self.attention_dropout(attention_probas)\n",
    "\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas_dropout @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        #out = self.resid_dropout(out)\n",
    "        return out, attention_probas   \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear( instance_size, 4 * instance_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * instance_size, instance_size),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def calculate_roc_auc(y_true, predicted_probabilities, return_nan=False, multi_class=None):\n",
    "    # Check the number of unique classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    print(f'num classes = {num_classes}')\n",
    "    # Check if there is more than one class\n",
    "    if num_classes > 1:\n",
    "        if multi_class == None:\n",
    "        # Compute ROC-AUC score\n",
    "            roc_auc = roc_auc_score(y_true, predicted_probabilities)\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_true, predicted_probabilities, multi_class=multi_class)\n",
    "\n",
    "        return roc_auc\n",
    "    else:\n",
    "        # Return NaN if there is only one class\n",
    "        if return_nan:\n",
    "            return np.nan\n",
    "        else:\n",
    "            print(\"Only one class present in y_true. ROC AUC score is not defined in that case.\")\n",
    "\n",
    "def calculate_classification_report(true_labels_list, predicted_labels_list, return_nan=True):\n",
    "    # Check the number of unique classes\n",
    "    num_classes = len(np.unique(true_labels_list))\n",
    "\n",
    "    # Check if there is more than one class\n",
    "    if num_classes > 1:\n",
    "        # Compute ROC-AUC score\n",
    "        report = classification_report(true_labels_list, predicted_labels_list)\n",
    "        return report\n",
    "    else:\n",
    "        # Return NaN if there is only one class\n",
    "        if return_nan:\n",
    "            return np.nan\n",
    "        else:\n",
    "            print(\"Only one class present in y_true. ROC AUC score is not defined in that case.\")\n",
    "\n",
    "def get_proba(true_labels_list, predicted_probas_list):\n",
    "    avg_proba_zero = np.mean(np.array(predicted_probas_list)[:,0][np.array(true_labels_list)==0])\n",
    "    avg_proba_one = np.mean(np.array(predicted_probas_list)[:,1][np.array(true_labels_list)==1])\n",
    "    return avg_proba_zero, avg_proba_one\n",
    "\n",
    "def calculate_loss(logits, logits_relevant, targets=None, loss_type='cross_entropy', gamma=None, alpha=None, L1=True):\n",
    "    if targets is None:\n",
    "            loss = None\n",
    "    else:\n",
    "        #target : shape B,\n",
    "        \n",
    "        if loss_type == 'cross_entropy':\n",
    "            cross_entropy = F.cross_entropy(logits, targets)\n",
    "            loss = cross_entropy\n",
    "        elif loss_type == 'focal_loss':\n",
    "            alphas = ((1 - targets) * (alpha-1)).to(torch.float) + 1\n",
    "            probas = F.softmax(logits)\n",
    "            certidude_factor =  (1-probas[[range(probas.shape[0]), targets]])**gamma * alphas\n",
    "            cross_entropy = F.cross_entropy(logits, targets, reduce=False)\n",
    "            loss = torch.dot(cross_entropy, certidude_factor)\n",
    "        elif loss_type == 'predictions':\n",
    "            probas = F.softmax(logits)\n",
    "            predictions = (probas[:,1] > 0.5).to(int)\n",
    "            loss = torch.sum((predictions-targets)**2)/len(predictions)\n",
    "        \n",
    "        if L1:\n",
    "            loss_l1 = torch.norm(logits_relevant, p=1)\n",
    "            return loss + loss_l1/10\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding')\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SineCosineEncoding(nn.Module):\n",
    "    def __init__(self, Embedding_size, max_len=1000):\n",
    "        super(SineCosineEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, Embedding_size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, Embedding_size, 2).float() * -(np.log(10000.0) / Embedding_size))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding.to(x.device)[x]\n",
    "\n",
    "class ZeroEmbedding(nn.Module):\n",
    "    def __init__(self, Embedding_size, max_len=1000):\n",
    "        super(ZeroEmbedding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, Embedding_size)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding.to(x.device)[x]\n",
    "\n",
    "\n",
    "class EmbeddingPheno(nn.Module):\n",
    "    def __init__(self, method=None, counts_method=None, vocab_size=None, max_count_same_disease=None, Embedding_size=None, rollup_depth=4, freeze_embed=False, dicts=None):\n",
    "        super(EmbeddingPheno, self).__init__()\n",
    "\n",
    "        self.dicts = dicts\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.nb_distinct_diseases_patient = vocab_size\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.metadata = None\n",
    "        self.counts_method = counts_method\n",
    "\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        \n",
    "        if method == None:\n",
    "            self.distinct_diseases_embeddings = nn.Embedding(vocab_size, Embedding_size)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, Embedding_size)\n",
    "            torch.nn.init.normal_(self.distinct_diseases_embeddings.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.normal_(self.counts_embeddings.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        elif method == 'Abby':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "\n",
    "\n",
    "\n",
    "        elif method=='Paul':\n",
    "            embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            if self.counts_method == 'SineCosine':\n",
    "                self.counts_embeddings = SineCosineEncoding(self.Embedding_size, max_count_same_disease)\n",
    "            elif self.counts_method == 'no_counts':\n",
    "                self.counts_embeddings = ZeroEmbedding(self.Embedding_size, max_count_same_disease )\n",
    "            else:\n",
    "\n",
    "                self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "    def write_embedding(self, writer):\n",
    "            embedding_tensor = self.distinct_diseases_embeddings.weight.data.detach().cpu().numpy()\n",
    "            writer.add_embedding(embedding_tensor, metadata=self.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class EmbeddingPhenoCat(nn.Module):\n",
    "    def __init__(self, pheno_method=None,  method=None, proj_embed=None, counts_method=None, Embedding_size=10, instance_size=10, rollup_depth=4, freeze_embed=False, dic_embedding_cat_params={}, dicts=None, device='cpu'):\n",
    "        super(EmbeddingPhenoCat, self).__init__()\n",
    "\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.dic_embedding_cat_params = dic_embedding_cat_params\n",
    "        dic_embedding_cat = {}\n",
    "        self.method = method\n",
    "        self.pheno_method = pheno_method\n",
    "        self.dicts = dicts\n",
    "        self.proj_embed = proj_embed\n",
    "        self.projection_embed = None\n",
    "        self.instance_size = instance_size\n",
    "        self.counts_method = counts_method\n",
    "\n",
    "        self.device = device\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        for cat, max_number  in self.dic_embedding_cat_params.items():\n",
    "        \n",
    "            if cat=='diseases':\n",
    "                if self.method == None:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, Embedding_size)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                elif self.method == 'Abby':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed).to(self.device)\n",
    "\n",
    "            \n",
    "\n",
    "                elif self.method=='Paul':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed).to(self.device)\n",
    "                    \n",
    "            elif cat == 'counts':\n",
    "                if self.pheno_method == 'Paul':\n",
    "                    if self.counts_method[cat] == 'SineCosine':\n",
    "                        dic_embedding_cat[cat] = SineCosineEncoding(self.instance_size, max_number).to(self.device)\n",
    "                    elif self.counts_method[cat] == 'no_counts':\n",
    "                        dic_embedding_cat[cat] = ZeroEmbedding(self.instance_size, max_number).to(self.device)\n",
    "                    else:\n",
    "                        dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                        torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "            elif cat == 'age':\n",
    "                if self.counts_method[cat] == 'SineCosine':\n",
    "                    dic_embedding_cat[cat] = SineCosineEncoding(self.instance_size, max_number).to(self.device)\n",
    "                elif self.counts_method[cat] == 'no_counts':\n",
    "                    dic_embedding_cat[cat] = ZeroEmbedding(self.instance_size, max_number).to(self.device)\n",
    "                else:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size).to(self.device)\n",
    "\n",
    "        self.dic_embedding_cat = dic_embedding_cat\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        list_env_embedded = []\n",
    "        for key, value in input_dict.items():\n",
    "            \n",
    "            batch_len = len(value)\n",
    "\n",
    "            if key=='diseases':\n",
    "                diseases_sentences_embedded = self.dic_embedding_cat[key](value)\n",
    "                if self.proj_embed:\n",
    "                    diseases_sentences_embedded = self.projection_embed(diseases_sentences_embedded)\n",
    "\n",
    "            elif key=='counts':\n",
    "                if self.pheno_method == 'Paul':\n",
    "                    counts_sentence_embedded = self.dic_embedding_cat[key](value)\n",
    "                    diseases_sentences_embedded = diseases_sentences_embedded + counts_sentence_embedded\n",
    "            \n",
    "\n",
    "            else:\n",
    "                list_env_embedded.append(self.dic_embedding_cat[key](value).view(batch_len, 1, self.instance_size))\n",
    "\n",
    "        env_embedded = torch.concat(list_env_embedded, dim=1)\n",
    "\n",
    "        return torch.concat([diseases_sentences_embedded, env_embedded], dim=1)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creation of the reference model\n",
    "#### framework constants:\n",
    "model_type = 'transformer'\n",
    "model_version = 'transformer_V2'\n",
    "test_name = 'baseline_model'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "tryout = True # True if we are doing a tryout, False otherwise \n",
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "rollup_depth = 4\n",
    "binary_classes = False #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "compute_features = False\n",
    "indices=None\n",
    "padding = True\n",
    "list_env_features = ['age', 'sex']\n",
    "### data format\n",
    "batch_size = 200\n",
    "data_share = 1#402555\n",
    "seuil_diseases = 600\n",
    "equalize_label = True\n",
    "decorelate = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 1000\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "##### model constants\n",
    "embedding_method = 'Abby' #None, Paul, Abby\n",
    "counts_method = 'normal'#{'counts': 'SineCos', 'age':'SineCos'}\n",
    "freeze_embedding = True\n",
    "Embedding_size = 10 # Size of embedding.\n",
    "proj_embed = True\n",
    "instance_size = 10\n",
    "n_head = 2# number of SA heads\n",
    "n_layer = 1# number of blocks in parallel\n",
    "Head_size = 8 # size of the \"single Attention head\", which is the sum of the size of all multi Attention heads\n",
    "eval_epochs_interval = 5 # number of epoch between each evaluation print of the model (no impact on results)\n",
    "eval_batch_interval = 40\n",
    "p_dropout = 0.3 # proba of dropouts in the model\n",
    "masking_padding = True # do we include padding masking or not\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_version = 'cross_entropy' #cross_entropy or focal_loss\n",
    "gamma = 2\n",
    "alpha = 1\n",
    "L1 = True\n",
    "##### training constants\n",
    "total_epochs = 50 # number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max\n",
    "\n",
    "warm_up_size = int(warm_up_frac*total_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         binary_classes=binary_classes,\n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         data_share=data_share,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=remove_none,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         equalize_label=equalize_label,\n",
    "                         seuil_diseases=seuil_diseases,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare,\n",
    "                         list_env_features=list_env_features, \n",
    "                         indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_list = dataT.get_patientlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "#check test name\n",
    "model_dir = path + f'logs/runs/SNPS/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}'\n",
    "model_plot_dir = path + f'logs/plots/tests/SNP/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}/'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(model_plot_dir, exist_ok=True)\n",
    "#check number tests\n",
    "test_dir = f'{model_dir}/{test_name}/'\n",
    "print(test_dir)\n",
    "log_data_dir = f'{test_dir}/data/'\n",
    "log_tensorboard_dir = f'{test_dir}/tensorboard/'\n",
    "log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "os.makedirs(log_data_dir, exist_ok=True)\n",
    "os.makedirs(log_tensorboard_dir, exist_ok=True)\n",
    "os.makedirs(log_slurm_outputs_dir, exist_ok=True)\n",
    "os.makedirs(log_slurm_errors_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "log_data_path_pickle = f'{test_dir}/data/{test_name}.pkl'\n",
    "log_tensorboard_path = f'{test_dir}/tensorboard/{test_name}'\n",
    "log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{test_name}.txt'\n",
    "log_slurm_error_path = f'{test_dir}/Slurm/Errors/{test_name}.txt'\n",
    "model_plot_path = path + f'logs/plots/tests/SNP/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}/{test_name}.png'\n",
    "\n",
    "sys.stdrerr = log_slurm_error_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_train, indices_test = dataT.get_indices_train_test(nb_data=len(patient_list),prop_train_test=prop_train_test)\n",
    "patient_list_transformer_train, patient_list_transformer_test = patient_list.get_transformer_data(indices_train.astype(int), indices_test.astype(int))\n",
    "#creation of torch Datasets:\n",
    "dataloader_train = DataLoader(patient_list_transformer_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(patient_list_transformer_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "if patient_list.nb_distinct_diseases_tot==None:\n",
    "    vocab_size = patient_list.get_nb_distinct_diseases_tot()\n",
    "if patient_list.nb_max_counts_same_disease==None:\n",
    "    max_count_same_disease = patient_list.get_max_count_same_disease()\n",
    "max_count_same_disease = patient_list.nb_max_counts_same_disease\n",
    "vocab_size = patient_list.get_nb_distinct_diseases_tot()\n",
    "\n",
    "print(f'\\n vocab_size : {vocab_size}, max_count : {max_count_same_disease}\\n', \n",
    "    f'length_patient = {patient_list.get_nb_max_distinct_diseases_patient()}\\n',\n",
    "    f'sparcity = {patient_list.sparsity}\\n',\n",
    "    f'nombres patients  = {len(patient_list)}')\n",
    "\n",
    "writer = SummaryWriter(log_tensorboard_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding  = EmbeddingPheno(method=embedding_method, \n",
    "                    counts_method=counts_method,\n",
    "                    vocab_size=vocab_size, \n",
    "                    max_count_same_disease=max_count_same_disease, \n",
    "                    Embedding_size=Embedding_size, \n",
    "                    rollup_depth=rollup_depth, \n",
    "                    freeze_embed=freeze_embedding,\n",
    "                    dicts=dataT.dicts)\n",
    "### creation of the model\n",
    "model = TransformerGeneModel_V2(pheno_method = pheno_method,\n",
    "                            Embedding = Embedding,\n",
    "                            Head_size=Head_size,\n",
    "                            binary_classes=binary_classes,\n",
    "                            n_head=n_head,\n",
    "                            n_layer=n_layer,\n",
    "                            mask_padding=masking_padding, \n",
    "                            padding_token=0, \n",
    "                            p_dropout=p_dropout, \n",
    "                            loss_version = loss_version, \n",
    "                            gamma = gamma,\n",
    "                            alpha = alpha,\n",
    "                            device = device,\n",
    "                            proj_embed=proj_embed,\n",
    "                            instance_size=instance_size)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate_max)\n",
    "lr_scheduler_warm_up = LinearLR(optimizer, start_factor=start_factor_lr , end_factor=1, total_iters=warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=total_epochs-warm_up_size, end_factor=end_factor_lr)\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[warm_up_size])\n",
    "\n",
    "\n",
    "output_file = log_slurm_outputs_path\n",
    "## Open tensor board writer\n",
    "dic_features_list = {\n",
    "'list_training_loss' : [],\n",
    "'list_validation_loss' : [],\n",
    "'list_proba_avg_zero' : [],\n",
    "'list_proba_avg_one' : [],\n",
    "'list_auc_validation' : [],\n",
    "'list_accuracy_validation' : [],\n",
    "'list_f1_validation' : [],\n",
    "'epochs' : [] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_diseases, batch_counts, batch_labels = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model.forward(batch_diseases, batch_counts, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.Embedding_gather.weight\n",
    "probas = torch.arange(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = (w.T * probas).T.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
