{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 15:54:46.245055: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-22 15:54:46.245120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-22 15:54:46.247114: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-22 15:54:57.680399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "##### Functionnal version with optionnal mask padding and dropouts, see Transformer_V1.ipynb for example\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "from torch.utils.data import DataLoader\n",
    "from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import LambdaLR, LinearLR, SequentialLR\n",
    "from codes.models.utils import clear_last_line, print_file, number_tests, Unbuffered, plot_infos, plot_ini_infos\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList, Patient\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functionnal version with optionnal mask padding and dropouts, see Transformer_V1.ipynb for example\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "\n",
    "### imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from codes.models.Transformers.Embedding import EmbeddingPheno\n",
    "from codes.models.metrics import calculate_roc_auc, calculate_classification_report, calculate_loss, get_proba\n",
    "\n",
    "### Transformer's instance\n",
    "# B, S, E, H, HN, MH = Batch_len, Sentence_len, Embedding_len, Head_size, Head number, MultiHead size.\n",
    "\n",
    "def custom_softmax(input_tensor, dim=-1):\n",
    "    # Calculer le softmax classique\n",
    "    softmax_output = F.softmax(input_tensor, dim)\n",
    "\n",
    "    # Trouver les colonnes avec que des -inf\n",
    "    nan_columns = torch.all(input_tensor == float('-inf'), dim=dim)\n",
    "\n",
    "    # Remplacer les valeurs de softmax par 0 pour les colonnes avec que des -inf\n",
    "    softmax_output[nan_columns] = 0\n",
    "\n",
    "    return softmax_output\n",
    "\n",
    "class TransformerGeneModel_V2(nn.Module):\n",
    "    def __init__(self, pheno_method, Embedding, Head_size, binary_classes, n_head, n_layer, mask_padding=False, padding_token=None, p_dropout=0, device='cpu', loss_version='cross_entropy', gamma=2, alpha=1, instance_size=None, proj_embed=True, L1=False):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.Embedding_size = Embedding.Embedding_size\n",
    "        self.binary_classes = binary_classes\n",
    "        self.instance_size=instance_size\n",
    "        self.proj_embed = proj_embed\n",
    "        if not self.proj_embed:\n",
    "            self.instance_size = self.Embedding_size\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size)\n",
    "        self.classes_nb = 2 if self.binary_classes else 3\n",
    "        self.blocks =PadMaskSequential(*[Block(self.instance_size, n_head=n_head, Head_size=Head_size, p_dropout=p_dropout) for _ in range(n_layer)]) #Block(self.instance_size, n_head=n_head, Head_size=Head_size) \n",
    "        self.ln_f = nn.LayerNorm(self.instance_size) # final layer norm\n",
    "        self.lm_head_logits = nn.Linear(self.instance_size, self.classes_nb) \n",
    "        self.lm_head_proba = nn.Linear(self.instance_size,1) # plus one for the probabilities\n",
    "        self.Embedding = Embedding\n",
    "        self.mask_padding = mask_padding\n",
    "        self.padding_token = padding_token\n",
    "        self.padding_mask = None\n",
    "        self.device = device\n",
    "        self.pheno_method = pheno_method\n",
    "        \n",
    "        \n",
    "        self.loss_version = loss_version\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.shap = False\n",
    "        self.L1 = L1\n",
    "       \n",
    "        self.diseases_embedding_table = Embedding.distinct_diseases_embeddings\n",
    "        if self.pheno_method == 'Paul':\n",
    "            self.counts_embedding_table = Embedding.counts_embeddings\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def create_padding_mask(self, diseases_sentence):\n",
    "        B, S = np.shape(diseases_sentence)[0], np.shape(diseases_sentence)[1]\n",
    "        mask = torch.where(diseases_sentence==self.padding_token)\n",
    "        padding_mask_mat = torch.ones((B, S, S), dtype=torch.int)\n",
    "        padding_mask_mat[mask] = 0\n",
    "        padding_mask_mat.transpose(-2,-1)[mask] = 0\n",
    "\n",
    "        \n",
    "        padding_mask_probas = torch.zeros((B, S))\n",
    "        padding_mask_probas[mask] = -torch.inf\n",
    "        padding_mask_probas = padding_mask_probas.view(B, S)\n",
    "        return padding_mask_mat.to(self.device), padding_mask_probas.to(self.device) # 1 if masked, 0 else\n",
    "\n",
    "    def set_padding_mask_transformer(self, padding_mask, padding_mask_probas):\n",
    "        self.padding_mask = padding_mask\n",
    "        self.padding_mask_probas = padding_mask_probas\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, diseases_sentence, counts_diseases, targets=None):\n",
    "\n",
    "        diseases_sentence = diseases_sentence.to(torch.long)\n",
    "        counts_diseases = counts_diseases.to(torch.long)\n",
    "\n",
    "        Batch_len, Sentence_len = diseases_sentence.shape\n",
    "\n",
    "        diseases_sentence = diseases_sentence.to(self.device)\n",
    "        counts_diseases = counts_diseases.to(self.device)\n",
    "        \n",
    "        if targets!=None:\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask, padding_mask_probas = self.create_padding_mask(diseases_sentence)\n",
    "            self.set_padding_mask_transformer(padding_mask, padding_mask_probas)\n",
    "            #self.blocks.set_padding_mask_sequential(self.padding_mask)\n",
    "\n",
    "        diseases_sentences_embedded = self.diseases_embedding_table(diseases_sentence) # shape B, S, E\n",
    "\n",
    "        x = diseases_sentences_embedded \n",
    "    \n",
    "        if self.pheno_method == 'Paul':\n",
    "            counts_diseases_embedded = self.counts_embedding_table(counts_diseases) # shape B, S, E\n",
    "            x = x + counts_diseases_embedded # shape B, S, E \n",
    "        \n",
    "        if self.proj_embed:\n",
    "            x = self.projection_embed(x)\n",
    "        x = self.blocks(x, self.padding_mask) # shape B, S, E\n",
    "        x = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        if self.mask_padding:\n",
    "            weights_logits = weights_logits + self.padding_mask_probas\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        #if self.mask_padding:\n",
    "           # probas = probas * self.padding_mask_probas\n",
    "        \n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, probas, targets, self.loss_version, self.gamma, self.alpha, L1=self.L1)\n",
    "\n",
    "        if self.shap:\n",
    "            return logits[:,0].view(Batch_len, 1)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def forward_decomposed(self, diseases_sentence, diseases_count, targets=None):\n",
    "        Batch_len, Sentence_len = diseases_sentence.shape\n",
    "        diseases_sentence = diseases_sentence.to(self.device)\n",
    "        counts_diseases = diseases_count.to(self.device)\n",
    "\n",
    "        if self.mask_padding:\n",
    "            padding_mask, padding_mask_probas = self.create_padding_mask(diseases_sentence)\n",
    "            self.set_padding_mask_transformer(padding_mask, padding_mask_probas)\n",
    "            #self.blocks.set_padding_mask_sequential(self.padding_mask)\n",
    "\n",
    "        diseases_sentences_embedded = self.diseases_embedding_table(diseases_sentence) # shape B, S, E\n",
    "\n",
    "        x = diseases_sentences_embedded \n",
    "        if self.pheno_method == 'Paul':\n",
    "            counts_diseases_embedded = self.counts_embedding_table(counts_diseases) # shape B, S, E\n",
    "            x = x + counts_diseases_embedded # shape B, S, E \n",
    "        if self.proj_embed:\n",
    "            x = self.projection_embed(x)\n",
    "        x= self.blocks.forward_decompose(x, self.padding_mask) # shape B, S, E\n",
    "        x_out = self.ln_f(x) # shape B, S, E\n",
    "        logits = self.lm_head_logits(x_out) #shape B, S, Classes_Numb, token logits\n",
    "        weights_logits = self.lm_head_proba(x).view(Batch_len, Sentence_len)\n",
    "        if self.mask_padding:\n",
    "            weights_logits = weights_logits + self.padding_mask_probas\n",
    "        probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n",
    "        #if self.mask_padding:\n",
    "           # probas = probas * self.padding_mask_probas\n",
    "        \n",
    "        logits = (logits.transpose(1, 2) @ probas.view(Batch_len, Sentence_len, 1)).view(Batch_len, self.classes_nb)# (B,Classes_Numb) Weighted Average logits\n",
    "        loss = calculate_loss(logits, probas, targets, self.loss_version, self.gamma, self.alpha, L1=self.L1)\n",
    "\n",
    "        return logits, probas, x_out, loss\n",
    "    \n",
    "\n",
    "    def predict(self, diseases_sentence, diseases_count):\n",
    "        logits, _ = self(diseases_sentence, diseases_count) # shape B, Classes_nb\n",
    "        return torch.argmax(logits, dim=1)  # (B,)\n",
    "        \n",
    "    def predict_proba(self, diseases_sentence, diseases_count):\n",
    "        logits, _ = self(diseases_sentence, diseases_count)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate(self, dataloader_test):\n",
    "        print('beginning inference evaluation')\n",
    "        start_time_inference = time.time()\n",
    "        predicted_labels_list = []\n",
    "        predicted_probas_list = []\n",
    "        true_labels_list = []\n",
    "\n",
    "        total_loss = 0.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_sentences, batch_counts, batch_labels in dataloader_test:\n",
    "\n",
    "\n",
    "                logits, loss = self(batch_sentences, batch_counts,  batch_labels)\n",
    "                total_loss += loss.item()\n",
    "                predicted_labels = self.predict(batch_sentences, batch_counts)\n",
    "                predicted_labels_list.extend(predicted_labels.cpu().numpy())\n",
    "                predicted_probas = self.predict_proba(batch_sentences, batch_counts)\n",
    "                predicted_probas_list.extend(predicted_probas.cpu().numpy())\n",
    "                true_labels_list.extend(batch_labels.cpu().numpy())\n",
    "        f1 = f1_score(true_labels_list, predicted_labels_list, average='macro')\n",
    "        accuracy = accuracy_score(true_labels_list, predicted_labels_list)\n",
    "        auc_score = calculate_roc_auc(true_labels_list, np.array(predicted_probas_list)[:, 1], return_nan=True)\n",
    "        proba_avg_zero, proba_avg_one = get_proba(true_labels_list, predicted_probas_list)\n",
    "    \n",
    "        self.train()\n",
    "        print(f'end inference evaluation in {time.time() - start_time_inference}s')\n",
    "        return f1, accuracy, auc_score, total_loss/len(dataloader_test), proba_avg_zero, proba_avg_one, predicted_probas_list, true_labels_list\n",
    "\n",
    "\n",
    "    def write_embedding(self, writer):\n",
    "        if self.proj_embed:\n",
    "            embedding_tensor = self.projection_embed(self.diseases_embedding_table.weight).detach().cpu().numpy()\n",
    "        else:\n",
    "            embedding_tensor = self.diseases_embedding_table.weight.detach().cpu().numpy()\n",
    "\n",
    "        writer.add_embedding(embedding_tensor, metadata=self.Embedding.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class PadMaskSequential(nn.Sequential):\n",
    "    def __init__(self, *args):\n",
    "        super(PadMaskSequential, self).__init__(*args)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sequential(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        for module in self:\n",
    "            #module.set_padding_mask_block(self.padding_mask)\n",
    "            x = module(x, padding_mask)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x, padding_mask):\n",
    "        for module in self:\n",
    "            #module.set_padding_mask_block(padding_mask)\n",
    "            x = module.forward_decompose(x, padding_mask)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, instance_size, n_head, Head_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadSelfAttention(n_head, Head_size, instance_size, p_dropout)\n",
    "        self.ffwd = FeedForward(instance_size, p_dropout)\n",
    "        self.ln1 = nn.LayerNorm(instance_size)\n",
    "        self.ln2 = nn.LayerNorm(instance_size)\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_block(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        #self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        #x = self.ln1(x)\n",
    "        x = x + self.sa(x, padding_mask=None)\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_decompose(self, x, padding_mask=None):\n",
    "        #self.sa.set_padding_mask_sa(self.padding_mask)\n",
    "        out_sa= self.sa.forward_decompose(x, padding_mask)\n",
    "        x = out_sa + x\n",
    "        x = x + self.ffwd(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, n_head, Head_size, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.qkv_network = nn.Linear(instance_size, Head_size * 3, bias = False) #group the computing of the nn.Linear for q, k and v, shape \n",
    "        self.proj = nn.Linear(Head_size, instance_size)\n",
    "        self.attention_dropout = nn.Dropout(p_dropout)\n",
    "        self.resid_dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "        self.multi_head_size = Head_size // n_head\n",
    "        self.flash = False\n",
    "        self.n_head = n_head\n",
    "        self.Head_size = Head_size\n",
    "        self.padding_mask = None\n",
    "\n",
    "    def set_padding_mask_sa(self, padding_mask):\n",
    "        self.padding_mask = padding_mask\n",
    "    \n",
    "\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "        q, k, v  = self.qkv_network(x).split(self.Head_size, dim=2) #q, k, v of shape (B, S, H)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            if padding_mask != None:\n",
    "                padding_mask_weights = -(1-padding_mask)*(10**10)\n",
    "                attention_weights = (attention_weights.transpose(0, 1)+padding_mask_weights).transpose(0, 1)\n",
    "            #print(f'wei0={attention_weights}')\n",
    "            attention_probas = F.softmax(attention_weights, dim=-1) # shape B, S, S\n",
    "            if padding_mask != None:\n",
    "                attention_probas = (attention_probas.transpose(0, 1)*padding_mask).transpose(0, 1)\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas = self.attention_dropout(attention_probas)\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        out = self.resid_dropout(out)\n",
    "        return out        \n",
    "    \n",
    "    def forward_decompose(self, x, padding_mask=None):\n",
    "        # x of size (B, S, E)\n",
    "        Batch_len, Sentence_len, _ = x.shape\n",
    "        q, k, v  = self.qkv_network(x).split(self.Head_size, dim=2) #q, k, v of shape (B, S, H)\n",
    "        # add a dimension to compute the different attention heads separately\n",
    "        q_multi_head = q.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2) #shape B, HN, S, MH\n",
    "        k_multi_head = k.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "        v_multi_head = v.view(Batch_len, Sentence_len, self.n_head, self.multi_head_size).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            out = torch.nn.functional.scaled_dot_product_attention(q_multi_head, k_multi_head, v_multi_head, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:    \n",
    "            attention_weights = (q_multi_head @ k_multi_head.transpose(-2, -1))/np.sqrt(self.multi_head_size) # shape B, S, S\n",
    "            ### padding mask #####\n",
    "            if padding_mask != None:\n",
    "                padding_mask_weights = -(1-padding_mask)*(10**10)\n",
    "                attention_weights = (attention_weights.transpose(0, 1)+padding_mask_weights).transpose(0, 1)\n",
    "\n",
    "\n",
    "            self.attention_probas = F.softmax(attention_weights, dim=-1)\n",
    "        \n",
    "            if padding_mask != None:\n",
    "                self.attention_probas = (self.attention_probas.transpose(0, 1)* padding_mask).transpose(0, 1)\n",
    "            self.attention_probas.retain_grad()\n",
    "\n",
    "           # attention_probas[attention_probas.isnan()]=0\n",
    "            attention_probas_dropout = self.attention_dropout(self.attention_probas)\n",
    "            \n",
    "\n",
    "\n",
    "            #print(f'wei1={attention_probas}')\n",
    "            #attention_probas = self.dropout(attention_probas)\n",
    "            ## weighted aggregation of the values\n",
    "            out = attention_probas_dropout @ v_multi_head # shape B, S, S @ B, S, MH = B, S, MH\n",
    "        out = out.transpose(1, 2).contiguous().view(Batch_len, Sentence_len, self.Head_size)\n",
    "        out = self.proj(out)\n",
    "        #out = self.resid_dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, instance_size, p_dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear( instance_size, 4 * instance_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * instance_size, instance_size),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def calculate_roc_auc(y_true, predicted_probabilities, return_nan=False, multi_class=None):\n",
    "    # Check the number of unique classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    print(f'num classes = {num_classes}')\n",
    "    # Check if there is more than one class\n",
    "    if num_classes > 1:\n",
    "        if multi_class == None:\n",
    "        # Compute ROC-AUC score\n",
    "            roc_auc = roc_auc_score(y_true, predicted_probabilities)\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_true, predicted_probabilities, multi_class=multi_class)\n",
    "\n",
    "        return roc_auc\n",
    "    else:\n",
    "        # Return NaN if there is only one class\n",
    "        if return_nan:\n",
    "            return np.nan\n",
    "        else:\n",
    "            print(\"Only one class present in y_true. ROC AUC score is not defined in that case.\")\n",
    "\n",
    "def calculate_classification_report(true_labels_list, predicted_labels_list, return_nan=True):\n",
    "    # Check the number of unique classes\n",
    "    num_classes = len(np.unique(true_labels_list))\n",
    "\n",
    "    # Check if there is more than one class\n",
    "    if num_classes > 1:\n",
    "        # Compute ROC-AUC score\n",
    "        report = classification_report(true_labels_list, predicted_labels_list)\n",
    "        return report\n",
    "    else:\n",
    "        # Return NaN if there is only one class\n",
    "        if return_nan:\n",
    "            return np.nan\n",
    "        else:\n",
    "            print(\"Only one class present in y_true. ROC AUC score is not defined in that case.\")\n",
    "\n",
    "def get_proba(true_labels_list, predicted_probas_list):\n",
    "    avg_proba_zero = np.mean(np.array(predicted_probas_list)[:,0][np.array(true_labels_list)==0])\n",
    "    avg_proba_one = np.mean(np.array(predicted_probas_list)[:,1][np.array(true_labels_list)==1])\n",
    "    return avg_proba_zero, avg_proba_one\n",
    "\n",
    "def calculate_loss(logits, logits_relevant, targets=None, loss_type='cross_entropy', gamma=None, alpha=None, L1=True):\n",
    "    if targets is None:\n",
    "            loss = None\n",
    "    else:\n",
    "        #target : shape B,\n",
    "        \n",
    "        if loss_type == 'cross_entropy':\n",
    "            cross_entropy = F.cross_entropy(logits, targets)\n",
    "            loss = cross_entropy\n",
    "        elif loss_type == 'focal_loss':\n",
    "            alphas = ((1 - targets) * (alpha-1)).to(torch.float) + 1\n",
    "            probas = F.softmax(logits)\n",
    "            certidude_factor =  (1-probas[[range(probas.shape[0]), targets]])**gamma * alphas\n",
    "            cross_entropy = F.cross_entropy(logits, targets, reduce=False)\n",
    "            loss = torch.dot(cross_entropy, certidude_factor)\n",
    "        elif loss_type == 'predictions':\n",
    "            probas = F.softmax(logits)\n",
    "            predictions = (probas[:,1] > 0.5).to(int)\n",
    "            loss = torch.sum((predictions-targets)**2)/len(predictions)\n",
    "        \n",
    "        if L1:\n",
    "            loss_l1 = torch.norm(logits_relevant, p=1)\n",
    "            return loss + loss_l1/10\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/gpfs/commons/groups/gursoy_lab/pmeddeb/phenotype_embedding')\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SineCosineEncoding(nn.Module):\n",
    "    def __init__(self, Embedding_size, max_len=1000):\n",
    "        super(SineCosineEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, Embedding_size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, Embedding_size, 2).float() * -(np.log(10000.0) / Embedding_size))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding.to(x.device)[x]\n",
    "\n",
    "class ZeroEmbedding(nn.Module):\n",
    "    def __init__(self, Embedding_size, max_len=1000):\n",
    "        super(ZeroEmbedding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, Embedding_size)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.encoding.to(x.device)[x]\n",
    "\n",
    "\n",
    "class EmbeddingPheno(nn.Module):\n",
    "    def __init__(self, method=None, counts_method=None, vocab_size=None, max_count_same_disease=None, Embedding_size=None, rollup_depth=4, freeze_embed=False, dicts=None):\n",
    "        super(EmbeddingPheno, self).__init__()\n",
    "\n",
    "        self.dicts = dicts\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.nb_distinct_diseases_patient = vocab_size\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.metadata = None\n",
    "        self.counts_method = counts_method\n",
    "\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        \n",
    "        if method == None:\n",
    "            self.distinct_diseases_embeddings = nn.Embedding(vocab_size, Embedding_size)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, Embedding_size)\n",
    "            torch.nn.init.normal_(self.distinct_diseases_embeddings.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.normal_(self.counts_embeddings.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        elif method == 'Abby':\n",
    "            embedding_file_diseases = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "\n",
    "\n",
    "\n",
    "        elif method=='Paul':\n",
    "            embedding_file_diseases = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "            pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "            self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "\n",
    "            self.distinct_diseases_embeddings = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed)\n",
    "            if self.counts_method == 'SineCosine':\n",
    "                self.counts_embeddings = SineCosineEncoding(self.Embedding_size, max_count_same_disease)\n",
    "            elif self.counts_method == 'no_counts':\n",
    "                self.counts_embeddings = ZeroEmbedding(self.Embedding_size, max_count_same_disease )\n",
    "            else:\n",
    "\n",
    "                self.counts_embeddings = nn.Embedding(max_count_same_disease, self.Embedding_size)\n",
    "    def write_embedding(self, writer):\n",
    "            embedding_tensor = self.distinct_diseases_embeddings.weight.data.detach().cpu().numpy()\n",
    "            writer.add_embedding(embedding_tensor, metadata=self.metadata, metadata_header=[\"Name\",\"Label\"])\n",
    "\n",
    "\n",
    "class EmbeddingPhenoCat(nn.Module):\n",
    "    def __init__(self, pheno_method=None,  method=None, proj_embed=None, counts_method=None, Embedding_size=10, instance_size=10, rollup_depth=4, freeze_embed=False, dic_embedding_cat_params={}, dicts=None, device='cpu'):\n",
    "        super(EmbeddingPhenoCat, self).__init__()\n",
    "\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.Embedding_size = Embedding_size\n",
    "        self.max_count_same_disease = None\n",
    "        self.dic_embedding_cat_params = dic_embedding_cat_params\n",
    "        dic_embedding_cat = {}\n",
    "        self.method = method\n",
    "        self.pheno_method = pheno_method\n",
    "        self.dicts = dicts\n",
    "        self.proj_embed = proj_embed\n",
    "        self.projection_embed = None\n",
    "        self.instance_size = instance_size\n",
    "        self.counts_method = counts_method\n",
    "\n",
    "        self.device = device\n",
    "        if self.dicts != None:\n",
    "            id_dict = self.dicts['id']\n",
    "            name_dict = self.dicts['name']\n",
    "            cat_dict = self.dicts['cat']\n",
    "            codes = list(id_dict.keys())\n",
    "            diseases_present = self.dicts['diseases_present']\n",
    "            self.metadata = [[name_dict[code], cat_dict[code]] for code in codes]\n",
    "\n",
    "        for cat, max_number  in self.dic_embedding_cat_params.items():\n",
    "        \n",
    "            if cat=='diseases':\n",
    "                if self.method == None:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, Embedding_size)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                elif self.method == 'Abby':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed).to(self.device)\n",
    "\n",
    "            \n",
    "\n",
    "                elif self.method=='Paul':\n",
    "                    embedding_file_diseases = f'/gpfs/commons/datasets/controlled/ukbb-gursoylab/mstoll/Data_Files/Embeddings/Paul_Glove/glove_UKBB_omop_rollup_closest_depth_{self.rollup_depth}_no_1_diseases.pth'\n",
    "                    pretrained_weights_diseases = torch.load(embedding_file_diseases)[diseases_present]\n",
    "                    self.Embedding_size = pretrained_weights_diseases.shape[1]\n",
    "                    dic_embedding_cat[cat] = nn.Embedding.from_pretrained(pretrained_weights_diseases, freeze=freeze_embed).to(self.device)\n",
    "                    \n",
    "            elif cat == 'counts':\n",
    "                if self.pheno_method == 'Paul':\n",
    "                    if self.counts_method[cat] == 'SineCosine':\n",
    "                        dic_embedding_cat[cat] = SineCosineEncoding(self.instance_size, max_number).to(self.device)\n",
    "                    elif self.counts_method[cat] == 'no_counts':\n",
    "                        dic_embedding_cat[cat] = ZeroEmbedding(self.instance_size, max_number).to(self.device)\n",
    "                    else:\n",
    "                        dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                        torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "            elif cat == 'age':\n",
    "                if self.counts_method[cat] == 'SineCosine':\n",
    "                    dic_embedding_cat[cat] = SineCosineEncoding(self.instance_size, max_number).to(self.device)\n",
    "                elif self.counts_method[cat] == 'no_counts':\n",
    "                    dic_embedding_cat[cat] = ZeroEmbedding(self.instance_size, max_number).to(self.device)\n",
    "                else:\n",
    "                    dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                    torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                dic_embedding_cat[cat] = nn.Embedding(max_number, self.instance_size).to(self.device)\n",
    "                torch.nn.init.normal_(dic_embedding_cat[cat].weight, mean=0.0, std=0.02)\n",
    "\n",
    "        if self.proj_embed:\n",
    "            self.projection_embed = nn.Linear(self.Embedding_size, self.instance_size).to(self.device)\n",
    "\n",
    "        self.dic_embedding_cat = dic_embedding_cat\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        list_env_embedded = []\n",
    "        for key, value in input_dict.items():\n",
    "            \n",
    "            batch_len = len(value)\n",
    "\n",
    "            if key=='diseases':\n",
    "                diseases_sentences_embedded = self.dic_embedding_cat[key](value)\n",
    "                if self.proj_embed:\n",
    "                    diseases_sentences_embedded = self.projection_embed(diseases_sentences_embedded)\n",
    "\n",
    "            elif key=='counts':\n",
    "                if self.pheno_method == 'Paul':\n",
    "                    counts_sentence_embedded = self.dic_embedding_cat[key](value)\n",
    "                    diseases_sentences_embedded = diseases_sentences_embedded + counts_sentence_embedded\n",
    "            \n",
    "\n",
    "            else:\n",
    "                list_env_embedded.append(self.dic_embedding_cat[key](value).view(batch_len, 1, self.instance_size))\n",
    "\n",
    "        env_embedded = torch.concat(list_env_embedded, dim=1)\n",
    "\n",
    "        return torch.concat([diseases_sentences_embedded, env_embedded], dim=1)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creation of the reference model\n",
    "#### framework constants:\n",
    "model_type = 'transformer'\n",
    "model_version = 'transformer_V2'\n",
    "test_name = 'baseline_model'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "tryout = True # True if we are doing a tryout, False otherwise \n",
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "rollup_depth = 4\n",
    "binary_classes = False #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "compute_features = False\n",
    "indices=None\n",
    "padding = True\n",
    "list_env_features = ['age', 'sex']\n",
    "### data format\n",
    "batch_size = 200\n",
    "data_share = 1#402555\n",
    "seuil_diseases = 600\n",
    "equalize_label = True\n",
    "decorelate = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 1000\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "##### model constants\n",
    "embedding_method = 'Abby' #None, Paul, Abby\n",
    "counts_method = 'normal'#{'counts': 'SineCos', 'age':'SineCos'}\n",
    "freeze_embedding = True\n",
    "Embedding_size = 10 # Size of embedding.\n",
    "proj_embed = True\n",
    "instance_size = 10\n",
    "n_head = 2# number of SA heads\n",
    "n_layer = 1# number of blocks in parallel\n",
    "Head_size = 8 # size of the \"single Attention head\", which is the sum of the size of all multi Attention heads\n",
    "eval_epochs_interval = 5 # number of epoch between each evaluation print of the model (no impact on results)\n",
    "eval_batch_interval = 40\n",
    "p_dropout = 0.3 # proba of dropouts in the model\n",
    "masking_padding = True # do we include padding masking or not\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_version = 'cross_entropy' #cross_entropy or focal_loss\n",
    "gamma = 2\n",
    "alpha = 1\n",
    "L1 = True\n",
    "##### training constants\n",
    "total_epochs = 50 # number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max\n",
    "\n",
    "warm_up_size = int(warm_up_frac*total_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT = DataTransfo_1SNP(SNP=SNP,\n",
    "                         CHR=CHR,\n",
    "                         method=pheno_method,\n",
    "                         padding=padding,  \n",
    "                         binary_classes=binary_classes,\n",
    "                         pad_token=padding_token, \n",
    "                         load_data=load_data, \n",
    "                         save_data=save_data, \n",
    "                         compute_features=compute_features,\n",
    "                         data_share=data_share,\n",
    "                         prop_train_test=prop_train_test,\n",
    "                         remove_none=remove_none,\n",
    "                         rollup_depth=rollup_depth,\n",
    "                         equalize_label=equalize_label,\n",
    "                         seuil_diseases=seuil_diseases,\n",
    "                         decorelate=decorelate,\n",
    "                         threshold_corr=threshold_corr,\n",
    "                         threshold_rare=threshold_rare,\n",
    "                         remove_rare=remove_rare,\n",
    "                         list_env_features=list_env_features, \n",
    "                         indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "data_loaded in 0.2398667335510254 s\n",
      "removing None values\n",
      "padding data\n"
     ]
    }
   ],
   "source": [
    "patient_list = dataT.get_patientlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not test_model:   \n",
    "    path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "    #check test name\n",
    "    model_dir = path + f'logs/runs/SNPS/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}'\n",
    "    model_plot_dir = path + f'logs/plots/tests/SNP/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}/'\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(model_plot_dir, exist_ok=True)\n",
    "    #check number tests\n",
    "    test_dir = f'{model_dir}/{test_name}/'\n",
    "    print(test_dir)\n",
    "    log_data_dir = f'{test_dir}/data/'\n",
    "    log_tensorboard_dir = f'{test_dir}/tensorboard/'\n",
    "    log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "    log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "    os.makedirs(log_data_dir, exist_ok=True)\n",
    "    os.makedirs(log_tensorboard_dir, exist_ok=True)\n",
    "    os.makedirs(log_slurm_outputs_dir, exist_ok=True)\n",
    "    os.makedirs(log_slurm_errors_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    log_data_path_pickle = f'{test_dir}/data/{test_name}.pkl'\n",
    "    log_tensorboard_path = f'{test_dir}/tensorboard/{test_name}'\n",
    "    log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{test_name}.txt'\n",
    "    log_slurm_error_path = f'{test_dir}/Slurm/Errors/{test_name}.txt'\n",
    "    model_plot_path = path + f'logs/plots/tests/SNP/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}/{test_name}.png'\n",
    "\n",
    "    sys.stdrerr = log_slurm_error_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " vocab_size : 1440, max_count : 2\n",
      " length_patient = 119\n",
      " sparcity = 0.9138499474185424\n",
      " nombres patients  = 12348\n"
     ]
    }
   ],
   "source": [
    "indices_train, indices_test = dataT.get_indices_train_test(nb_data=len(patient_list),prop_train_test=prop_train_test)\n",
    "patient_list_transformer_train, patient_list_transformer_test = patient_list.get_transformer_data(indices_train.astype(int), indices_test.astype(int))\n",
    "#creation of torch Datasets:\n",
    "dataloader_train = DataLoader(patient_list_transformer_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(patient_list_transformer_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "if patient_list.nb_distinct_diseases_tot==None:\n",
    "    vocab_size = patient_list.get_nb_distinct_diseases_tot()\n",
    "if patient_list.nb_max_counts_same_disease==None:\n",
    "    max_count_same_disease = patient_list.get_max_count_same_disease()\n",
    "max_count_same_disease = patient_list.nb_max_counts_same_disease\n",
    "vocab_size = patient_list.get_nb_distinct_diseases_tot()\n",
    "\n",
    "print(f'\\n vocab_size : {vocab_size}, max_count : {max_count_same_disease}\\n', \n",
    "    f'length_patient = {patient_list.get_nb_max_distinct_diseases_patient()}\\n',\n",
    "    f'sparcity = {patient_list.sparsity}\\n',\n",
    "    f'nombres patients  = {len(patient_list)}')\n",
    "\n",
    "if not test_model:\n",
    "    writer = SummaryWriter(log_tensorboard_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18715 M parameters\n"
     ]
    }
   ],
   "source": [
    "Embedding  = EmbeddingPheno(method=embedding_method, \n",
    "                    counts_method=counts_method,\n",
    "                    vocab_size=vocab_size, \n",
    "                    max_count_same_disease=max_count_same_disease, \n",
    "                    Embedding_size=Embedding_size, \n",
    "                    rollup_depth=rollup_depth, \n",
    "                    freeze_embed=freeze_embedding,\n",
    "                    dicts=dataT.dicts)\n",
    "### creation of the model\n",
    "model = TransformerGeneModel_V2(pheno_method = pheno_method,\n",
    "                            Embedding = Embedding,\n",
    "                            Head_size=Head_size,\n",
    "                            binary_classes=binary_classes,\n",
    "                            n_head=n_head,\n",
    "                            n_layer=n_layer,\n",
    "                            mask_padding=masking_padding, \n",
    "                            padding_token=0, \n",
    "                            p_dropout=p_dropout, \n",
    "                            loss_version = loss_version, \n",
    "                            gamma = gamma,\n",
    "                            alpha = alpha,\n",
    "                            device = device,\n",
    "                            proj_embed=proj_embed,\n",
    "                            instance_size=instance_size)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18715 M parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate_max)\n",
    "lr_scheduler_warm_up = LinearLR(optimizer, start_factor=start_factor_lr , end_factor=1, total_iters=warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=total_epochs-warm_up_size, end_factor=end_factor_lr)\n",
    "lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[warm_up_size])\n",
    "\n",
    "if not test_model:\n",
    "    output_file = log_slurm_outputs_path\n",
    "## Open tensor board writer\n",
    "dic_features_list = {\n",
    "'list_training_loss' : [],\n",
    "'list_validation_loss' : [],\n",
    "'list_proba_avg_zero' : [],\n",
    "'list_proba_avg_one' : [],\n",
    "'list_auc_validation' : [],\n",
    "'list_accuracy_validation' : [],\n",
    "'list_f1_validation' : [],\n",
    "'epochs' : [] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_diseases, batch_counts, batch_labels = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerGeneModel_V2(\n",
       "  (projection_embed): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (blocks): PadMaskSequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadSelfAttention(\n",
       "        (qkv_network): Linear(in_features=10, out_features=24, bias=False)\n",
       "        (proj): Linear(in_features=8, out_features=10, bias=True)\n",
       "        (attention_dropout): Dropout(p=0.3, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=10, out_features=40, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=40, out_features=10, bias=True)\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head_logits): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (lm_head_proba): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (Embedding): EmbeddingPheno(\n",
       "    (distinct_diseases_embeddings): Embedding(1440, 128)\n",
       "    (counts_embeddings): Embedding(2, 128)\n",
       "  )\n",
       "  (diseases_embedding_table): Embedding(1440, 128)\n",
       ")"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_21736/3967551285.py:132: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model.forward(batch_diseases, batch_counts, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_21736/3967551285.py:168: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n"
     ]
    }
   ],
   "source": [
    "logits, loss, x_out, loss = model.forward_decomposed(batch_diseases, batch_counts, batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiHeadSelfAttention' object has no attribute 'attention_probas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_probas\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiHeadSelfAttention' object has no attribute 'attention_probas'"
     ]
    }
   ],
   "source": [
    "model.blocks[0].sa.attention_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 2, 122, 122])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0].sa.attention_probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_21736/64605731.py:169: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(weights_logits) # shape B, S(represent the probas to be chosen)\n"
     ]
    }
   ],
   "source": [
    "logits, loss, x_out, loss = model.forward_decomposed(batch_diseases, batch_counts, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0707, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.1111, 0.1111, 0.1111,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1111, 0.1111, 0.1111,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1111, 0.1111, 0.1111,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.1111, 0.1111, 0.1111,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1111, 0.1111, 0.1111,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1111, 0.1111, 0.1111,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0909, 0.0909, 0.0909,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0833, 0.0833, 0.0833,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0833, 0.0833, 0.0833,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0833, 0.0833, 0.0833,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0833, 0.0833, 0.0833,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0833, 0.0833, 0.0833,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0833, 0.0833, 0.0833,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0].sa.attention_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_21736/3355736690.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  loss.grad\n"
     ]
    }
   ],
   "source": [
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_attributes(obj, prefix=''):\n",
    "    \"\"\"\n",
    "    Fonction récursive pour obtenir les noms des attributs stockés sur GPU.\n",
    "    \"\"\"\n",
    "    gpu_attributes = []\n",
    "    if isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
    "        gpu_attributes.append(prefix)\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        for i, item in enumerate(obj):\n",
    "            gpu_attributes.extend(get_gpu_attributes(item, prefix=f\"{prefix}[{i}]\"))\n",
    "    elif isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            gpu_attributes.extend(get_gpu_attributes(value, prefix=f\"{prefix}.{key}\"))\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        for key, value in obj.__dict__.items():\n",
    "            gpu_attributes.extend(get_gpu_attributes(value, prefix=f\"{prefix}.{key}\"))\n",
    "    return gpu_attributes\n",
    "\n",
    "# Exemple d'objet avec des attributs stockés sur CPU et GPU\n",
    "class ExempleObjet:\n",
    "    def __init__(self):\n",
    "        self.cpu_tensor = torch.randn(3, 3)\n",
    "        self.gpu_tensor = torch.randn(3, 3).cuda()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.padding_mask = model.padding_mask.to('cpu')\n",
    "model.padding_mask_probas = model.padding_mask.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gpu_attributes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.list_attention_layers[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, attr in model.__dict__.items():\n",
    "    if torch.is_tensor(attr) and attr.is_cuda:\n",
    "        print(f\"L'attribut {name} est sur le GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "start_time_training = time.time()\n",
    "print_file(output_file, f'Beginning of the program for {total_epochs} epochs', new_line=True)\n",
    "# Training Loop\n",
    "plot_ini_infos(model, output_file, dataloader_test, dataloader_train, writer, dic_features_list)\n",
    "for epoch in range(1, total_epochs+1):\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    total_loss = 0.0  \n",
    "    \n",
    "    #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "    for k, (batch_sentences, batch_counts, batch_labels) in enumerate(dataloader_train):\n",
    "        \n",
    "        batch_sentences = batch_sentences.to(device)\n",
    "        batch_counts = batch_counts.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(batch_sentences, batch_counts, batch_labels)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "    \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if k % eval_batch_interval == 0:\n",
    "            clear_last_line(output_file)\n",
    "            print_file(output_file, f'Progress in epoch {epoch}  = {round(k / len(dataloader_train)*100, 2)} %, time batch : {time.time() - start_time_epoch}', new_line=False)\n",
    "\n",
    "    if epoch % eval_epochs_interval == 0:\n",
    "        dic_features = plot_infos(model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, model_plot_path)\n",
    "\n",
    "    \n",
    "    \n",
    "    lr_scheduler.step()\n",
    "\n",
    "dic_features = dic_features\n",
    "model.to('cpu')\n",
    "Embedding.to('cpu')\n",
    "model.write_embedding(writer)\n",
    "# Print time\n",
    "print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "start_time = time.time()\n",
    "\n",
    "with open(log_data_path_pickle, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "\n",
    "  ## Add hyper parameters to tensorboard\n",
    "hyperparams = {\"CHR\" : CHR, \"SNP\" : SNP, \"ROLLUP LEVEL\" : rollup_depth,\n",
    "            'PHENO_METHOD': pheno_method, 'EMBEDDING_METHOD': embedding_method,\n",
    "            'EMBEDDING SIZE' : Embedding_size, 'ATTENTION HEADS' : n_head, 'BLOCKS' : n_layer,\n",
    "            'LR':1 , 'DROPOUT' : p_dropout, 'NUM_EPOCHS' : total_epochs, \n",
    "            'BATCH_SIZE' : batch_size, \n",
    "            'PADDING_MASKING': masking_padding,\n",
    "            'VERSION' : model_version,\n",
    "            'NB_Patients'  : len(patient_list),\n",
    "            'LOSS_VERSION'  : loss_version,\n",
    "            }\n",
    "writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "trained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nn.Embedding(10, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenos = torch.arange(10)\n",
    "weights = torch.arange(5)\n",
    "embeddings(phenos)*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
