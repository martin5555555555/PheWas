{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR, LinearLR, SequentialLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from codes.models.data_form.DataSets import TabDictDataset\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP\n",
    "from codes.models.Transformers.Embedding import EmbeddingPheno\n",
    "from codes.models.Transformers.dic_model_versions import DIC_MODEL_VERSIONS\n",
    "from codes.models.utils import clear_last_line, print_file, number_tests, Unbuffered, plot_infos, plot_ini_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR, LinearLR, SequentialLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList, Patient\n",
    "from codes.models.Transformers.Embedding import EmbeddingPheno, EmbeddingPhenoCat\n",
    "from codes.models.Transformers.dic_model_versions import DIC_MODEL_VERSIONS\n",
    "from codes.models.utils import clear_last_line, print_file, number_tests, Unbuffered, plot_infos, plot_ini_infos\n",
    "from codes.models.metrics import calculate_roc_auc\n",
    "from codes.models.Naive.Naive_model import NaiveModelWeights, CustomDatasetWithLabels\n",
    "from codes.models.Transformers.FT_Transformer import TabTransformerGeneModel_V2\n",
    "\n",
    "\n",
    "class TrainModel():\n",
    "    def __init__(self, model_type=None, model_version=None, test_name=None, pheno_method=None, tryout=None, binary_classes=None, counts_method=None, CHR=None, SNP=None, rollup_depth=None, padding_token=None, prop_train_test=None, load_data=None,\\\n",
    "                save_data=None, remove_none=None, compute_features=None, padding=None, batch_size=None, data_share=None, seuil_diseases=None, equalize_label=None, embedding_method=None, freeze_embedding=None, \\\n",
    "                Embedding_size=None, n_head=None, n_layer=None, Head_size=None, eval_epochs_interval=None, eval_batch_interval=None, p_dropout=None, masking_padding=None, \\\n",
    "                loss_version=None, gamma=None, alpha=None, total_epochs=None, learning_rate_max=None, \\\n",
    "                learning_rate_ini=None, learning_rate_final=None,\\\n",
    "                warm_up_frac=None, from_test=False, decorelate=False, threshold_corr=1, threshold_rare=0, remove_rare=None, list_env_features=[],\n",
    "                proj_embed=True, instance_size=None, indices=None, list_pheno_ids=None, L1=False): \n",
    "        self.model_type = model_type\n",
    "        self.model_version = model_version\n",
    "        self.test_name = test_name\n",
    "        self.pheno_method = pheno_method # Paul, Abby\n",
    "        self.tryout = tryout # True if we are doing a tryout, False otherwise \n",
    "        ### data constants:\n",
    "        self.CHR = CHR\n",
    "        self.SNP = SNP\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.binary_classes = binary_classes #nb of classes related to an SNP (here 0 or 1)\n",
    "       \n",
    "        self.padding_token = padding_token\n",
    "        self.prop_train_test = prop_train_test\n",
    "        self.load_data = load_data\n",
    "        self.save_data = save_data\n",
    "        self.remove_none = remove_none\n",
    "        self.compute_features = compute_features\n",
    "        self.padding = padding\n",
    "        ### data format\n",
    "        self.batch_size = batch_size\n",
    "        self.data_share = data_share #402555\n",
    "        self.seuil_diseases = seuil_diseases\n",
    "        self.equalize_label = equalize_label\n",
    "        self.decorelate = decorelate\n",
    "        self.threshold_corr = threshold_corr\n",
    "        self.threshold_rare = threshold_rare\n",
    "        self.remove_rare = remove_rare\n",
    "        ##### model constants\n",
    "        self.embedding_method = embedding_method #None, Paul, Abby\n",
    "        self.counts_method = counts_method\n",
    "        self.freeze_embedding = freeze_embedding\n",
    "        self.Embedding_size = Embedding_size # Size of embedding.\n",
    "        self.proj_embed = proj_embed\n",
    "        self.instance_size = instance_size\n",
    "        self.n_head = n_head # number of SA heads\n",
    "        self.n_layer = n_layer # number of blocks in parallel\n",
    "        self.Head_size = Head_size  # size of the \"single Attention head\", which is the sum of the size of all multi Attention heads\n",
    "        self.eval_epochs_interval = eval_epochs_interval # number of epoch between each evaluation print of the model (no impact on results)\n",
    "        self.eval_batch_interval = eval_batch_interval\n",
    "        self.p_dropout = p_dropout # proba of dropouts in the model\n",
    "        self.masking_padding = masking_padding # do we include padding masking or not\n",
    "        self.loss_version = loss_version #cross_entropy or focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.L1 = L1\n",
    "        ##### training constants\n",
    "        self.total_epochs = total_epochs # number of epochs\n",
    "        self.learning_rate_max = learning_rate_max # maximum learning rate (at the end of the warmup phase)\n",
    "        self.learning_rate_ini = learning_rate_ini # initial learning rate \n",
    "        self.learning_rate_final = learning_rate_final\n",
    "        self.warm_up_frac = warm_up_frac # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "        self.start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "        self.end_factor_lr = learning_rate_final / learning_rate_max\n",
    "        self.warm_up_size = int(total_epochs*warm_up_frac)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.number_test = None\n",
    "        self.number_test = self.get_number_test()\n",
    "        print(self.tryout)\n",
    "        self.trained = False\n",
    "        # to be defined with data\n",
    "        self.vocab_size = None \n",
    "        self.max_count_same_disease = None\n",
    "        #### essential components of a model\n",
    "        self.dataT = None\n",
    "        self.patient_list = None\n",
    "        self.model = None\n",
    "        self.number_test_trained = None\n",
    "        self.list_env_features = list_env_features\n",
    "        self.indices=indices\n",
    "        self.list_pheno_ids = list_pheno_ids\n",
    "\n",
    "       \n",
    "    @property\n",
    "    def get_test_name_with_infos(self):\n",
    "        return str(self.number_test) + '_' + self.test_name + 'tryout'*self.tryout\n",
    "    def get_number_test(self, update=False):\n",
    "        if self.number_test != None and update==False:\n",
    "            return self.number_test\n",
    "        else:\n",
    "            df_list_instance_test_saved_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "            df_list_instance_test_saved = pd.read_csv(df_list_instance_test_saved_path)\n",
    "            if len(df_list_instance_test_saved.index)==0:\n",
    "                    self.number_test = 1\n",
    "            else:\n",
    "                    number_test_ex =  np.max(df_list_instance_test_saved['number_test'])\n",
    "                    self.number_test = int(number_test_ex + 1)\n",
    "            \n",
    "            return self.number_test\n",
    "    \n",
    "    def get_dataT(self):\n",
    "        \n",
    "        if self.dataT != None:\n",
    "            pass        \n",
    "        else:\n",
    "            print('Bulding dataT')\n",
    "            dataT = DataTransfo_1SNP(SNP=self.SNP,\n",
    "                         CHR=self.CHR,\n",
    "                         method=self.pheno_method,\n",
    "                         padding=self.padding,  \n",
    "                         binary_classes=self.binary_classes,\n",
    "                         pad_token=self.padding_token, \n",
    "                         load_data=self.load_data, \n",
    "                         save_data=self.save_data, \n",
    "                         compute_features=self.compute_features,\n",
    "                         data_share=self.data_share,\n",
    "                         prop_train_test=self.prop_train_test,\n",
    "                         remove_none=self.remove_none,\n",
    "                         rollup_depth=self.rollup_depth,\n",
    "                         equalize_label=self.equalize_label,\n",
    "                         seuil_diseases=self.seuil_diseases,\n",
    "                         decorelate=self.decorelate,\n",
    "                         threshold_corr=self.threshold_corr,\n",
    "                         threshold_rare=self.threshold_rare,\n",
    "                         remove_rare=self.remove_rare,\n",
    "                         list_env_features=self.list_env_features, \n",
    "                         indices=self.indices,\n",
    "                         list_pheno_ids=self.list_pheno_ids)\n",
    "            self.dataT = dataT\n",
    "            print(f'got dataT')\n",
    "        return self.dataT\n",
    "        \n",
    "    def get_patient_list(self):\n",
    "        if self.patient_list == None:\n",
    "            self.patient_list = self.get_dataT().get_patientlist()\n",
    "        return self.patient_list\n",
    "    \n",
    "    def get_model(self):\n",
    "        Embedding  = EmbeddingPheno(method=self.embedding_method, \n",
    "                            counts_method=self.counts_method,\n",
    "                            vocab_size=self.vocab_size, \n",
    "                            max_count_same_disease=self.max_count_same_disease, \n",
    "                            Embedding_size=self.Embedding_size, \n",
    "                            rollup_depth=self.rollup_depth, \n",
    "                            freeze_embed=self.freeze_embedding,\n",
    "                            dicts=self.dataT.dicts)\n",
    "        self.Embedding = Embedding\n",
    "        ### creation of the model\n",
    "        ClassModel = DIC_MODEL_VERSIONS[self.model_version]\n",
    "        model = ClassModel(pheno_method = self.pheno_method,\n",
    "                                    Embedding = Embedding,\n",
    "                                    Head_size=self.Head_size,\n",
    "                                    binary_classes=self.binary_classes,\n",
    "                                    n_head=self.n_head,\n",
    "                                    n_layer=self.n_layer,\n",
    "                                    mask_padding=self.masking_padding, \n",
    "                                    padding_token=0, \n",
    "                                    p_dropout=self.p_dropout, \n",
    "                                    loss_version = self.loss_version, \n",
    "                                    gamma = self.gamma,\n",
    "                                    alpha = self.alpha,\n",
    "                                    device = self.device,\n",
    "                                    proj_embed=self.proj_embed,\n",
    "                                    instance_size=self.instance_size,\n",
    "                                    L1=self.L1)\n",
    "      \n",
    "        # print the number of parameters in the model\n",
    "        print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def generate_dirs(self, makedirs=True):\n",
    "\n",
    "        if self.model_type == 'transformer' or self.model_type == 'naive_model' or self.model_type == 'tab_transformer':\n",
    "            path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "\n",
    "            #check test name\n",
    "            model_dir = path + f'logs/runs/SNPS/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}'\n",
    "            model_plot_dir = path + f'logs/plots/tests/SNP/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}/'\n",
    "\n",
    "            if makedirs:\n",
    "                os.makedirs(model_dir, exist_ok=True)\n",
    "                os.makedirs(model_plot_dir, exist_ok=True)\n",
    "            #check number tests\n",
    "            test_dir = f'{model_dir}/{self.get_test_name_with_infos}/'\n",
    "            print(test_dir)\n",
    "            log_data_dir = f'{test_dir}/data/'\n",
    "            log_tensorboard_dir = f'{test_dir}/tensorboard/'\n",
    "            log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "            log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "            if makedirs:\n",
    "                os.makedirs(log_data_dir, exist_ok=True)\n",
    "                os.makedirs(log_tensorboard_dir, exist_ok=True)\n",
    "                os.makedirs(log_slurm_outputs_dir, exist_ok=True)\n",
    "                os.makedirs(log_slurm_errors_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "            log_data_path_pickle = f'{test_dir}/data/{self.test_name}.pkl'\n",
    "            log_tensorboard_path = f'{test_dir}/tensorboard/{self.test_name}'\n",
    "            log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{self.test_name}.txt'\n",
    "            log_slurm_error_path = f'{test_dir}/Slurm/Errors/{self.test_name}.txt'\n",
    "            model_plot_path = path + f'logs/plots/tests/SNP/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}/{self.get_test_name_with_infos}.png'\n",
    "\n",
    "            return log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, model_plot_path\n",
    "        elif self.model_type == 'logistic_regression':\n",
    "            path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "\n",
    "            #check test name\n",
    "            model_dir = path + f'logs/runs/SNPS/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}'\n",
    "            model_plot_dir = path + f'logs/plots/tests/SNP/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}/'\n",
    "\n",
    "            if makedirs:\n",
    "                os.makedirs(model_dir, exist_ok=True)\n",
    "                os.makedirs(model_plot_dir, exist_ok=True)\n",
    "            #check number tests\n",
    "            test_dir = f'{model_dir}/{self.get_test_name_with_infos}/'\n",
    "            print(test_dir)\n",
    "            log_data_dir = f'{test_dir}/data/'\n",
    "            log_res_dir = f'{test_dir}/res/'\n",
    "            log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "            log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "            if makedirs:\n",
    "                os.makedirs(log_data_dir, exist_ok=True)\n",
    "                os.makedirs(log_res_dir, exist_ok=True)\n",
    "                os.makedirs(log_slurm_outputs_dir, exist_ok=True)\n",
    "                os.makedirs(log_slurm_errors_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "            log_data_path_pickle = f'{test_dir}/data/{self.test_name}.pkl'\n",
    "            log_res_path_txt = f'{test_dir}/res/{self.test_name}.txt'\n",
    "            log_res_path_pkl = f'{test_dir}/res/{self.test_name}.pkl'\n",
    "\n",
    "            log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{self.test_name}.txt'\n",
    "            log_slurm_error_path = f'{test_dir}/Slurm/Errors/{self.test_name}.txt'\n",
    "\n",
    "            return log_data_path_pickle, log_res_path_txt, log_res_path_pkl, log_slurm_outputs_path, log_slurm_error_path\n",
    "\n",
    "    def to_csv(self):\n",
    "        dic_attrib = self.__dict__.copy()\n",
    "        keys_to_drop = ['dataT', 'patient_list', 'model', 'patient_list_transformer_train', 'patient_list_transformer_test', 'dic_data', 'dic_data_train', 'dic_data_test']\n",
    "        for key in keys_to_drop:\n",
    "            if key in list(dic_attrib.keys()):\n",
    "                dic_attrib.pop(key)\n",
    "        attributes = list(dic_attrib.keys())\n",
    "        values = list(dic_attrib.values())\n",
    "        df_row_test = pd.DataFrame([values],columns=attributes)\n",
    "        return df_row_test\n",
    "    \n",
    "    def add_to_trained(self):\n",
    "        df_row_test = self.to_csv()\n",
    "        df_list_instance_test_trained_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/instance_tests/list_instance_tests_train.csv'\n",
    "        df_list_instance_test_trained = pd.read_csv(df_list_instance_test_trained_path)\n",
    "        df_instance_test_row = self.to_csv()\n",
    "        df_list_instance_test_trained =  pd.concat([df_list_instance_test_trained, df_instance_test_row], ignore_index=True)\n",
    "        df_list_instance_test_trained.to_csv(df_list_instance_test_trained_path, index=False)\n",
    "    def add_to_saved(self):\n",
    "        df_row_test = self.to_csv()\n",
    "        df_list_instance_test_saved_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "        df_list_instance_test_saved = pd.read_csv(df_list_instance_test_saved_path)\n",
    "        df_instance_test_row = self.to_csv()\n",
    "        df_list_instance_test_saved =  pd.concat([df_list_instance_test_saved, df_instance_test_row], ignore_index=True)\n",
    "        df_list_instance_test_saved.to_csv(df_list_instance_test_saved_path, index=False)\n",
    "\n",
    "    def is_equal(self, test):\n",
    "        pass\n",
    "            \n",
    "    def save_model(self, update=False):\n",
    "        \n",
    "       \n",
    "        already_saved, number_test = self.test_already_saved()\n",
    "        if already_saved and not update:\n",
    "            self.number_test = number_test\n",
    "            print('test has already been saved')\n",
    "\n",
    "        else:\n",
    "            if not update:\n",
    "                self.get_number_test(update=True)\n",
    "            path_instance_test_save_dir= '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/' \n",
    "            path_instance_test_save = f'{path_instance_test_save_dir}{self.get_test_name_with_infos}.pkl'\n",
    "            with open(path_instance_test_save, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "            if update:\n",
    "                TrainModel.remove_instance_test_df(self.number_test)\n",
    "            self.add_to_saved()\n",
    "\n",
    "    def test_already_trained(self):\n",
    "        \n",
    "            df_instance_tests_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/instance_tests/list_instance_tests_train.csv'\n",
    "            df_instance_tests = pd.read_csv(df_instance_tests_path)\n",
    "            df_instance_tests_dropped = df_instance_tests.copy()\n",
    "            df_test = self.to_csv()\n",
    "\n",
    "            cols_drop_check_train = ['test_name', 'number_test', 'trained', 'vocab_size', 'max_count_same_disease', 'test_name_with_infos', 'device', 'number_test_trained']\n",
    "            for col in cols_drop_check_train:\n",
    "                if col in df_instance_tests_dropped.columns:\n",
    "                    df_instance_tests_dropped = df_instance_tests_dropped.drop(col, axis=1)\n",
    "                if col in df_test.columns:\n",
    "                    df_test = df_test.drop(col, axis=1)\n",
    "            already_trained = False\n",
    "            already_trained_index = list(df_instance_tests['number_test'][df_instance_tests_dropped.apply(lambda row: row.equals(df_test.iloc[0]), axis=1)])\n",
    "            if already_trained_index == []:\n",
    "                already_trained =False\n",
    "                number_test = 0\n",
    "            else:\n",
    "\n",
    "                already_trained = True\n",
    "                number_test = already_trained_index[0]\n",
    "                self.number_test_trained = number_test\n",
    "            self.save_model(update=True)\n",
    "            return already_trained, number_test  \n",
    "\n",
    "    def test_already_saved(self):\n",
    "        \n",
    "        df_instance_tests_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "        df_instance_tests = pd.read_csv(df_instance_tests_path)\n",
    "        df_instance_tests_dropped = df_instance_tests.copy()\n",
    "\n",
    "        df_test = self.to_csv()\n",
    "        cols_drop_check_train = ['number_test', 'trained', 'vocab_size', 'max_count_same_disease', 'test_name_with_infos', 'device', 'number_test_trained']\n",
    "        for col in cols_drop_check_train:\n",
    "            if col in df_instance_tests_dropped.columns:\n",
    "                df_instance_tests_dropped = df_instance_tests_dropped.drop(col, axis=1)\n",
    "            if col in df_test.columns:\n",
    "                df_test = df_test.drop(col, axis=1)\n",
    "        already_saved = False\n",
    "        already_saved_index = list(df_instance_tests['number_test'][df_instance_tests_dropped.apply(lambda row: row.equals(df_test.iloc[0]), axis=1)])\n",
    "        if already_saved_index == []:\n",
    "            already_saved =False\n",
    "            number_test = 0\n",
    "        else:\n",
    "            already_saved = True\n",
    "            number_test = already_saved_index[0]\n",
    "        return already_saved, number_test    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load_instance_test(instance_test_number):\n",
    "        instance_model_dir = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/'\n",
    "        for instance_test in os.listdir(instance_model_dir):\n",
    "            if instance_test == 'list_instance_tests_saved.csv':\n",
    "                pass\n",
    "            else:\n",
    "                number_instance_test_try = int(instance_test.split('_')[0])\n",
    "            \n",
    "                if number_instance_test_try == instance_test_number:\n",
    "                    with open(f'{instance_model_dir}{instance_test}', 'rb') as file:\n",
    "                        test_instance = pickle.load(file)\n",
    "                        return test_instance\n",
    "        print('test was not found')\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_instance_test(instance_test_number):\n",
    "        instance_model_dir = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/'\n",
    "        for instance_test in os.listdir(instance_model_dir):\n",
    "            if instance_test == 'list_instance_tests_saved.csv':\n",
    "                pass\n",
    "            else:\n",
    "                number_instance_test_try = int(instance_test.split('_')[0])\n",
    "            \n",
    "                if number_instance_test_try == instance_test_number:\n",
    "                    os.remove(f'{instance_model_dir}{instance_test}')\n",
    "        TrainModel.remove_instance_test_df(instance_test_number)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_instance_test_df(instance_test_number):\n",
    "        df_list_instance_test_saved_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "        df_list_instance_test_saved = pd.read_csv(df_list_instance_test_saved_path)\n",
    "        df_list_instance_test_saved = df_list_instance_test_saved[ df_list_instance_test_saved['number_test'] != instance_test_number]\n",
    "        df_list_instance_test_saved.to_csv(df_list_instance_test_saved_path, index=False)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_res_instance_test(instance_test_number=None, instance_test=None, actual_test=False):\n",
    "        if instance_test == None:\n",
    "            instance_test = TrainModel.load_instance_test(instance_test_number)\n",
    "        path_tensorboard_test_instance = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/tests/all_instances/SNPS/{str(instance_test.CHR)}/{instance_test.SNP}/{instance_test.model_type}/{instance_test.model_version}/{instance_test.pheno_method}/{instance_test.get_test_name_with_infos}'\n",
    "        if instance_test.number_test_trained != None:\n",
    "            instance_test_trained = TrainModel.load_instance_test(instance_test_number=instance_test.number_test_trained)\n",
    "            log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path  = instance_test_trained.generate_dirs()\n",
    "        else:\n",
    "            log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path  = instance_test.generate_dirs()\n",
    "\n",
    "        if os.path.exists(path_tensorboard_test_instance): \n",
    "            shutil.rmtree(path_tensorboard_test_instance)\n",
    "\n",
    "        shutil.copytree(log_tensorboard_path, path_tensorboard_test_instance )\n",
    "        if actual_test:\n",
    "            shutil.rmtree(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/')\n",
    "            os.makedirs(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/')\n",
    "            actual_test_dir = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/{instance_test.get_test_name_with_infos}'\n",
    "            shutil.copytree(path_tensorboard_test_instance, actual_test_dir)\n",
    "\n",
    "class TrainTransformerModel(TrainModel):\n",
    "    def __init__(self, model_version, test_name, pheno_method, tryout, CHR, SNP, rollup_depth, binary_classes,counts_method, padding_token, prop_train_test, load_data,\n",
    "                save_data, remove_none, compute_features, padding, batch_size, data_share, seuil_diseases, equalize_label, embedding_method, freeze_embedding, Embedding_size, n_head, \n",
    "                 n_layer, Head_size, eval_epochs_interval, eval_batch_interval, p_dropout, masking_padding, loss_version, gamma, alpha, total_epochs, learning_rate_max, learning_rate_ini, learning_rate_final,\n",
    "                    warm_up_frac, decorelate, threshold_corr, threshold_rare, remove_rare, list_env_features, proj_embed, instance_size, indices=None, list_pheno_ids=None, L1=False):\n",
    "        self.model_type = 'transformer'\n",
    "        super().__init__(model_type=self.model_type, model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                        CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, counts_method=counts_method, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                        load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                        data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                        freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                        eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                        loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                        learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare, remove_rare=remove_rare, list_env_features=list_env_features,\n",
    "                        proj_embed=proj_embed, instance_size=instance_size, indices=indices, list_pheno_ids=list_pheno_ids, L1=L1)\n",
    "        \n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        self.get_number_test() #update the test number\n",
    "        already_trained, number_test = self.test_already_trained()\n",
    "        \n",
    "        log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path = self.generate_dirs(makedirs = True)\n",
    "        # Redirect  output to a file\n",
    "        sys.stdout = open(log_slurm_outputs_path, 'w')\n",
    "        sys.stderr = open(log_slurm_error_path, 'w')\n",
    "        sys.stdout = Unbuffered(sys.stdout)\n",
    "        sys.stderr = Unbuffered(sys.stderr)\n",
    "        if already_trained:\n",
    "            print('model has already been trained')\n",
    "           \n",
    "        else: \n",
    "            print(f'Begining of training program, device={self.device}')\n",
    "            self.get_dataT()\n",
    "            self.get_patient_list()\n",
    "            \n",
    "\n",
    "\n",
    "            indices_train, indices_test = self.dataT.get_indices_train_test(nb_data=len(self.patient_list),prop_train_test=self.prop_train_test)\n",
    "            self.patient_list_transformer_train, self.patient_list_transformer_test = self.patient_list.get_transformer_data(indices_train.astype(int), indices_test.astype(int))\n",
    "            #creation of torch Datasets:\n",
    "            dataloader_train = DataLoader(self.patient_list_transformer_train, batch_size=self.batch_size, shuffle=True)\n",
    "            dataloader_test = DataLoader(self.patient_list_transformer_test, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "            if self.patient_list.nb_distinct_diseases_tot==None:\n",
    "                vocab_size = self.patient_list.get_nb_distinct_diseases_tot()\n",
    "            if self.patient_list.nb_max_counts_same_disease==None:\n",
    "                max_count_same_disease = self.patient_list.get_max_count_same_disease()\n",
    "            self.max_count_same_disease = self.patient_list.nb_max_counts_same_disease\n",
    "            self.vocab_size = self.patient_list.get_nb_distinct_diseases_tot()\n",
    "\n",
    "            print(f'\\n vocab_size : {self.vocab_size}, max_count : {self.max_count_same_disease}\\n', \n",
    "                f'length_patient = {self.patient_list.get_nb_max_distinct_diseases_patient()}\\n',\n",
    "                f'sparcity = {self.patient_list.sparsity}\\n',\n",
    "                f'nombres patients  = {len(self.patient_list)}')\n",
    "            \n",
    "            writer = SummaryWriter(log_tensorboard_path)\n",
    "\n",
    "            self.get_model()\n",
    "\n",
    "\n",
    "                                \n",
    "            self.model.to(self.device)\n",
    "            # print the number of parameters in the model\n",
    "            print(sum(p.numel() for p in self.model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate_max)\n",
    "            lr_scheduler_warm_up = LinearLR(optimizer, start_factor=self.start_factor_lr , end_factor=1, total_iters=self.warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "            lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=self.total_epochs-self.warm_up_size, end_factor=self.end_factor_lr)\n",
    "            lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[self.warm_up_size])\n",
    "\n",
    "\n",
    "            output_file = log_slurm_outputs_path\n",
    "            ## Open tensor board writer\n",
    "            dic_features_list = {\n",
    "            'list_training_loss' : [],\n",
    "            'list_validation_loss' : [],\n",
    "            'list_proba_avg_zero' : [],\n",
    "            'list_proba_avg_one' : [],\n",
    "            'list_auc_validation' : [],\n",
    "            'list_accuracy_validation' : [],\n",
    "            'list_f1_validation' : [],\n",
    "            'epochs' : [] }\n",
    "\n",
    "            # Training Loop\n",
    "            start_time_training = time.time()\n",
    "            print_file(output_file, f'Beginning of the program for {self.total_epochs} epochs', new_line=True)\n",
    "            # Training Loop\n",
    "            plot_ini_infos(self.model, output_file, dataloader_test, dataloader_train, writer, dic_features_list)\n",
    "            for epoch in range(1, self.total_epochs+1):\n",
    "\n",
    "                start_time_epoch = time.time()\n",
    "                total_loss = 0.0  \n",
    "                \n",
    "                #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "                for k, (batch_sentences, batch_counts, batch_labels) in enumerate(dataloader_train):\n",
    "                    \n",
    "                    batch_sentences = batch_sentences.to(self.device)\n",
    "                    batch_counts = batch_counts.to(self.device)\n",
    "                    batch_labels = batch_labels.to(self.device)\n",
    "\n",
    "                    # evaluate the loss\n",
    "                    logits, loss = self.model(batch_sentences, batch_counts, batch_labels)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                \n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if k % self.eval_batch_interval == 0:\n",
    "                        clear_last_line(output_file)\n",
    "                        print_file(output_file, f'Progress in epoch {epoch}  = {round(k / len(dataloader_train)*100, 2)} %, time batch : {time.time() - start_time_epoch}', new_line=False)\n",
    "\n",
    "                if epoch % self.eval_epochs_interval == 0:\n",
    "                    dic_features = plot_infos(self.model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, plots_path)\n",
    "\n",
    "                \n",
    "                \n",
    "                lr_scheduler.step()\n",
    "\n",
    "            self.dic_features = dic_features\n",
    "            if self.model.padding_mask != None:\n",
    "                self.model.padding_mask.to('cpu')\n",
    "                self.model.padding_mask_probas.to('cpu')\n",
    "\n",
    "            self.model.to('cpu')\n",
    "            self.Embedding.to('cpu')\n",
    "            self.model.write_embedding(writer)\n",
    "            # Print time\n",
    "            print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "            start_time = time.time()\n",
    "\n",
    "            with open(log_data_path_pickle, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "            print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            ## Add hyper parameters to tensorboard\n",
    "            hyperparams = {\"CHR\" : self.CHR, \"SNP\" : self.SNP, \"ROLLUP LEVEL\" : self.rollup_depth,\n",
    "                        'PHENO_METHOD': self.pheno_method, 'EMBEDDING_METHOD': self.embedding_method,\n",
    "                        'EMBEDDING SIZE' : self.Embedding_size, 'ATTENTION HEADS' : self.n_head, 'BLOCKS' : self.n_layer,\n",
    "                        'LR':1 , 'DROPOUT' : self.p_dropout, 'NUM_EPOCHS' : self.total_epochs, \n",
    "                        'BATCH_SIZE' : self.batch_size, \n",
    "                        'PADDING_MASKING': self.masking_padding,\n",
    "                        'VERSION' : self.model_version,\n",
    "                        'NB_Patients'  : len(self.patient_list),\n",
    "                        'LOSS_VERSION'  : self.loss_version,\n",
    "                        }\n",
    "            writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "            self.trained = True\n",
    "\n",
    "            self.save_model(update=True)\n",
    "            self.add_to_trained()\n",
    "\n",
    "class TrainTabTransformerModel(TrainModel):\n",
    "    def __init__(self, model_version, test_name, pheno_method, tryout, CHR, SNP, rollup_depth, binary_classes, counts_method,  padding_token, prop_train_test, load_data,\n",
    "                save_data, remove_none, compute_features, padding, batch_size, data_share, seuil_diseases, equalize_label, embedding_method, freeze_embedding, Embedding_size, n_head, \n",
    "                 n_layer, Head_size, eval_epochs_interval, eval_batch_interval, p_dropout, masking_padding, loss_version, gamma, alpha, total_epochs, learning_rate_max, learning_rate_ini, learning_rate_final,\n",
    "                    warm_up_frac, decorelate, threshold_corr, threshold_rare, remove_rare, list_env_features, proj_embed, instance_size, indices=None, list_pheno_ids=None):\n",
    "        self.model_type = 'tab_transformer'\n",
    "        super().__init__(model_type=self.model_type, model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                        CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, counts_method=counts_method, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                        load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                        data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                        freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                        eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                        loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                        learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare, remove_rare=remove_rare, list_env_features=list_env_features,\n",
    "                        proj_embed=proj_embed, instance_size=instance_size, indices=indices, list_pheno_ids=list_pheno_ids)\n",
    "        \n",
    "    \n",
    "    def get_model(self):\n",
    "        Embedding  = EmbeddingPhenoCat(method=self.embedding_method, \n",
    "                            max_count_same_disease=self.max_count_same_disease, \n",
    "                            Embedding_size=self.Embedding_size, \n",
    "                            rollup_depth=self.rollup_depth, \n",
    "                            freeze_embed=self.freeze_embedding,\n",
    "                            dic_embedding_cat_params=self.dic_embedding_cat_params,\n",
    "                            dicts=self.dataT.dicts,\n",
    "                            device=self.device)\n",
    "        self.Embedding = Embedding\n",
    "        ### creation of the model\n",
    "        model = TabTransformerGeneModel_V2(\n",
    "                                    pheno_method = self.pheno_method,\n",
    "                                    Embedding = Embedding,\n",
    "                                    Head_size=self.Head_size,\n",
    "                                    Classes_nb=self.Classes_nb,\n",
    "                                    n_head=self.n_head,\n",
    "                                    n_layer=self.n_layer,\n",
    "                                    mask_padding=self.masking_padding, \n",
    "                                    padding_token=0, \n",
    "                                    p_dropout=self.p_dropout, \n",
    "                                    loss_version = self.loss_version, \n",
    "                                    gamma = self.gamma,\n",
    "                                    alpha = self.alpha,\n",
    "                                    device = self.device,\n",
    "                                    proj_embed = self.proj_embed\n",
    "                                          )\n",
    "        # print the number of parameters in the model\n",
    "        print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train_model(self):\n",
    "        self.get_number_test() #update the test number\n",
    "        already_trained, number_test = self.test_already_trained()\n",
    "        \n",
    "        log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path = self.generate_dirs(makedirs = True)\n",
    "        # Redirect  output to a file\n",
    "        sys.stdout = open(log_slurm_outputs_path, 'w')\n",
    "        sys.stderr = open(log_slurm_error_path, 'w')\n",
    "        sys.stdout = Unbuffered(sys.stdout)\n",
    "        sys.stderr = Unbuffered(sys.stderr)\n",
    "        if already_trained:\n",
    "            print('model has already been trained')\n",
    "           \n",
    "        else: \n",
    "            print(f'Begining of training program, device={self.device}')\n",
    "            self.get_dataT()\n",
    "            self.dic_data= self.dataT.get_data_tabtransfo(actualise_phenos=True)\n",
    "            indices_train, indices_test = self.dataT.get_indices_train_test(nb_data=len(self.dic_data['diseases']))\n",
    "            dic_data_train = {key: np.array(self.dic_data[key])[indices_train] for key in self.dic_data.keys()}\n",
    "            dic_data_test = {key: np.array(self.dic_data[key])[indices_test] for key in self.dic_data.keys()}\n",
    "            max_number_diseases = self.patient_list.get_nb_distinct_diseases_tot()\n",
    "            max_number_counts = self.patient_list.get_max_count_same_disease()\n",
    "            max_number_age = np.max(np.array(self.dataT.dic_list_patients_test['age'])) + 1\n",
    "            max_number_sex = 2\n",
    "            self.dic_embedding_cat_params = {'diseases':max_number_diseases, 'counts':max_number_counts, 'age':max_number_age, 'sex':max_number_sex} \n",
    "\n",
    "            dataloader_train =  DataLoader(self.Dataset_tab_train, batch_size = self.batch_size, shuffle=True)\n",
    "            dataloader_test =  DataLoader(self.Dataset_tab_test, batch_size = self.batch_size, shuffle=True)\n",
    "            \n",
    "            if self.patient_list.nb_distinct_diseases_tot==None:\n",
    "                vocab_size = self.patient_list.get_nb_distinct_diseases_tot()\n",
    "            if self.patient_list.nb_max_counts_same_disease==None:\n",
    "                max_count_same_disease = self.patient_list.get_max_count_same_disease()\n",
    "            self.max_count_same_disease = self.patient_list.nb_max_counts_same_disease\n",
    "            self.vocab_size = self.patient_list.get_nb_distinct_diseases_tot()\n",
    "\n",
    "            print(f'\\n vocab_size : {self.vocab_size}, max_count : {self.max_count_same_disease}\\n', \n",
    "                f'length_patient = {self.patient_list.get_nb_max_distinct_diseases_patient()}\\n',\n",
    "                f'sparcity = {self.patient_list.sparsity}\\n',\n",
    "                f'nombres patients  = {len(self.patient_list)}')\n",
    "            \n",
    "            writer = SummaryWriter(log_tensorboard_path)\n",
    "\n",
    "            self.get_model()\n",
    "\n",
    "\n",
    "                                \n",
    "            self.model.to(self.device)\n",
    "            # print the number of parameters in the model\n",
    "            print(sum(p.numel() for p in self.model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate_max)\n",
    "            lr_scheduler_warm_up = LinearLR(optimizer, start_factor=self.start_factor_lr , end_factor=1, total_iters=self.warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "            lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=self.total_epochs-self.warm_up_size, end_factor=self.end_factor_lr)\n",
    "            lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[self.warm_up_size])\n",
    "\n",
    "\n",
    "            output_file = log_slurm_outputs_path\n",
    "            ## Open tensor board writer\n",
    "            dic_features_list = {\n",
    "            'list_training_loss' : [],\n",
    "            'list_validation_loss' : [],\n",
    "            'list_proba_avg_zero' : [],\n",
    "            'list_proba_avg_one' : [],\n",
    "            'list_auc_validation' : [],\n",
    "            'list_accuracy_validation' : [],\n",
    "            'list_f1_validation' : [],\n",
    "            'epochs' : [] }\n",
    "\n",
    "            # Training Loop\n",
    "            start_time_training = time.time()\n",
    "            print_file(output_file, f'Beginning of the program for {self.total_epochs} epochs', new_line=True)\n",
    "            # Training Loop\n",
    "            plot_ini_infos(self.model, output_file, self.dataloader_test, self.dataloader_train, writer, dic_features_list)\n",
    "            for epoch in range(1, self.total_epochs+1):\n",
    "\n",
    "                start_time_epoch = time.time()\n",
    "                total_loss = 0.0  \n",
    "                \n",
    "                #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "                for k, input_dict in enumerate(dataloader_train):\n",
    "                    \n",
    "    \n",
    "\n",
    "                    # evaluate the loss\n",
    "                    logits, loss = self.model(input_dict)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                \n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if k % self.eval_batch_interval == 0:\n",
    "                        clear_last_line(output_file)\n",
    "                        print_file(output_file, f'Progress in epoch {epoch}  = {round(k / len(dataloader_train)*100, 2)} %, time batch : {time.time() - start_time_epoch}', new_line=False)\n",
    "\n",
    "                if epoch % self.eval_epochs_interval == 0:\n",
    "                    dic_features = plot_infos(self.model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, plots_path)\n",
    "\n",
    "                \n",
    "                \n",
    "                lr_scheduler.step()\n",
    "\n",
    "            \n",
    "            self.dic_features = dic_features\n",
    "            self.model = self.model.to('cpu')\n",
    "            if self.model.padding_mask != None:\n",
    "                self.model.padding_mask.to('cpu')\n",
    "                self.model.padding_mask_probas.to('cpu')\n",
    "\n",
    "            self.model.write_embedding(writer)\n",
    "            # Print time\n",
    "            print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "            start_time = time.time()\n",
    "\n",
    "            with open(log_data_path_pickle, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "            print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            ## Add hyper parameters to tensorboard\n",
    "            hyperparams = {\"CHR\" : self.CHR, \"SNP\" : self.SNP, \"ROLLUP LEVEL\" : self.rollup_depth,\n",
    "                        'PHENO_METHOD': self.pheno_method, 'EMBEDDING_METHOD': self.embedding_method,\n",
    "                        'EMBEDDING SIZE' : self.Embedding_size, 'ATTENTION HEADS' : self.n_head, 'BLOCKS' : self.n_layer,\n",
    "                        'LR':1 , 'DROPOUT' : self.p_dropout, 'NUM_EPOCHS' : self.total_epochs, \n",
    "                        'BATCH_SIZE' : self.batch_size, \n",
    "                        'PADDING_MASKING': self.masking_padding,\n",
    "                        'VERSION' : self.model_version,\n",
    "                        'NB_Patients'  : len(self.patient_list),\n",
    "                        'LOSS_VERSION'  : self.loss_version,\n",
    "                        }\n",
    "            writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "            self.trained = True\n",
    "\n",
    "            self.save_model(update=True)\n",
    "            self.add_to_trained()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TrainLogRegModel(TrainModel):\n",
    "    def __init__(self, model_version, test_name, pheno_method, tryout, CHR, SNP, rollup_depth, binary_classes, counts_method, padding_token, prop_train_test, load_data,\n",
    "                save_data, remove_none, compute_features, padding, batch_size, data_share, seuil_diseases, equalize_label, embedding_method, freeze_embedding, Embedding_size, n_head, \n",
    "                 n_layer, Head_size, eval_epochs_interval, eval_batch_interval, p_dropout, masking_padding, loss_version, gamma, alpha, total_epochs, learning_rate_max, learning_rate_ini, learning_rate_final,\n",
    "                    warm_up_frac, decorelate, threshold_corr, threshold_rare, remove_rare, indices=None, list_pheno_ids=None):\n",
    "        self.model_type = 'logistic_regression'\n",
    "        super().__init__(model_type=self.model_type, model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                        CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, counts_method=counts_method, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                        load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                        data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                        freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                        eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                        loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                        learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare, remove_rare=remove_rare, indices=indices, list_pheno_ids=list_pheno_ids)\n",
    "        \n",
    "    def train_model(self):\n",
    "        self.get_number_test() #update the test number\n",
    "        already_trained, number_test = self.test_already_trained()\n",
    "        \n",
    "        log_data_path_pickle, log_res_path_txt, log_res_path_pickle, log_slurm_outputs_path, log_slurm_error_path = self.generate_dirs(makedirs = True)\n",
    "        # Redirect  output to a file\n",
    "        sys.stdout = open(log_slurm_outputs_path, 'w')\n",
    "        sys.stderr = open(log_slurm_error_path, 'w')\n",
    "        sys.stdout = Unbuffered(sys.stdout)\n",
    "        sys.stderr = Unbuffered(sys.stderr)\n",
    "        if already_trained:\n",
    "            print('model has already been trained')\n",
    "           \n",
    "        else: \n",
    "            print(f'Begining of training program, device={self.device}')\n",
    "            self.get_dataT()\n",
    "            self.get_patient_list()\n",
    "\n",
    "        if self.model_version == 'global':\n",
    "            indices = np.arange(len(self.patient_list))\n",
    "            np.random.shuffle(indices)\n",
    "            pheno_data, label_data = self.patient_list.get_tree_data()\n",
    "            pheno_data_train = np.array(pheno_data)[indices[:int(self.prop_train_test*len(self.patient_list))]]\n",
    "            label_data_train = np.array(label_data)[indices[:int(self.prop_train_test*len(self.patient_list))]]\n",
    "            label_data_test = np.array(label_data)[indices[int(self.prop_train_test*len(self.patient_list)):]]\n",
    "            pheno_data_test = np.array(pheno_data)[indices[int(self.prop_train_test*len(self.patient_list)):]]                        \n",
    "            class_weights = compute_sample_weight(class_weight='balanced', y=label_data_train)\n",
    "\n",
    "            # Adjust the input data with the square root of weights\n",
    "            sqrt_weights = np.sqrt(class_weights)\n",
    "            pheno_data_train_weighted = pheno_data_train * sqrt_weights[:, np.newaxis]\n",
    "            column_one_train = np.ones((pheno_data_train.shape[0],1 ))\n",
    "            column_one_test = np.ones((pheno_data_test.shape[0],1 ))\n",
    "\n",
    "            pheno_data_train_weighted_with_constant=  np.concatenate([column_one_train, pheno_data_train_weighted], axis = 1)\n",
    "            pheno_data_train_with_constant =  np.concatenate([column_one_train, pheno_data_train], axis = 1)\n",
    "            pheno_data_test_with_constant =  np.concatenate([column_one_test, pheno_data_test], axis = 1)\n",
    "\n",
    "            logit_model = sm.Logit(label_data_train, pheno_data_train_weighted_with_constant)\n",
    "            result = logit_model.fit(method='bfgs', disp=True)\n",
    "            ### visualisation des donnes avec df\n",
    "            proba_test = result.predict(pheno_data_test_with_constant)\n",
    "            proba_train = result.predict(pheno_data_train_weighted_with_constant)\n",
    "\n",
    "\n",
    "            labels_pred_test = (proba_test > 0.5).astype(int)\n",
    "            nb_positive_test = np.sum(labels_pred_test==0)\n",
    "            nb_negative_test = np.sum(labels_pred_test==1)\n",
    "            labels_pred_train = (proba_train > 0.5).astype(int)\n",
    "            nb_positive_train = np.sum(labels_pred_train==0)\n",
    "            nb_negative_train = np.sum(labels_pred_train==1)\n",
    "\n",
    "\n",
    "            TP_test = np.sum((label_data_test==0 )& (labels_pred_test == 0)) / nb_positive_test\n",
    "            FP_test = np.sum((label_data_test==1 )& (labels_pred_test == 0)) / nb_positive_test\n",
    "            TN_test = np.sum((label_data_test==1 )& (labels_pred_test == 1)) / nb_negative_test\n",
    "            FN_test = np.sum((label_data_test== 0)& (labels_pred_test == 1)) / nb_negative_test\n",
    "\n",
    "            TP_train = np.sum((label_data_train==0 )& (labels_pred_train == 0)) / nb_positive_train\n",
    "            FP_train = np.sum((label_data_train==1 )& (labels_pred_train == 0)) / nb_positive_train\n",
    "            TN_train = np.sum((label_data_train==1 )& (labels_pred_train == 1)) / nb_negative_train\n",
    "            FN_train = np.sum((label_data_train== 0)& (labels_pred_train == 1)) / nb_negative_train\n",
    "\n",
    "\n",
    "            auc_test = calculate_roc_auc(label_data_test, proba_test)\n",
    "            auc_train = calculate_roc_auc(label_data_train, proba_train)\n",
    "\n",
    "            with open(log_res_path_txt, 'w') as file:\n",
    "                file.write(f'TP_test={TP_test}\\n')\n",
    "                file.write(f'TN_test={TP_test}\\n')\n",
    "                file.write(f'TP_train={TP_train}\\n')\n",
    "                file.write(f'TN_train={TP_train}\\n')    \n",
    "                file.write(f'auc_test={auc_test}\\n')        \n",
    "                file.write(f'auc_train={auc_train}\\n')        \n",
    "\n",
    "            with open(log_data_path_pickle, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "                print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "            dic_res = {'label_test':label_data_test, 'label_train':label_data_train,\n",
    "                        'label_pred_test':labels_pred_test, 'label_pred_train':labels_pred_train, \n",
    "                        'proba_test':proba_test, 'proba_train':proba_train}\n",
    "            with open(log_res_path_pickle, 'wb') as file:\n",
    "                pickle.dump(dic_res, file)\n",
    "                print('Res saved to %s' % log_res_path_pickle)\n",
    "\n",
    "class TrainNaiveModel(TrainModel):\n",
    "    def __init__(self, model_version, test_name, pheno_method, tryout, CHR, SNP, rollup_depth, binary_classes, counts_method, padding_token, prop_train_test, load_data,\n",
    "                save_data, remove_none, compute_features, padding, batch_size, data_share, seuil_diseases, equalize_label, embedding_method, freeze_embedding, Embedding_size, n_head, \n",
    "                 n_layer, Head_size, eval_epochs_interval, eval_batch_interval, p_dropout, masking_padding, loss_version, gamma, alpha, total_epochs, learning_rate_max, learning_rate_ini, learning_rate_final,\n",
    "                    warm_up_frac, decorelate, threshold_corr, threshold_rare, remove_rare, list_env_features, indices=None, list_pheno_ids=None):\n",
    "        self.model_type = 'naive_model'\n",
    "        super().__init__(model_type=self.model_type, model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                        CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, counts_method=counts_method, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                        load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                        data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                        freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                        eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                        loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                        learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare, remove_rare=remove_rare, indices=indices, list_pheno_ids=list_pheno_ids)\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        self.get_number_test() #update the test number\n",
    "        already_trained, number_test = self.test_already_trained()\n",
    "\n",
    "        log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path = self.generate_dirs(makedirs = True)\n",
    "        # Redirect  output to a file\n",
    "        sys.stdout = open(log_slurm_outputs_path, 'w')\n",
    "        sys.stderr = open(log_slurm_error_path, 'w')\n",
    "        sys.stdout = Unbuffered(sys.stdout)\n",
    "        sys.stderr = Unbuffered(sys.stderr)\n",
    "        if already_trained:\n",
    "            print('model has already been trained')\n",
    "            self.save_model()\n",
    "        else: \n",
    "            print(f'Begining of training program, device={self.device}')\n",
    "            start_time_training = time.time()\n",
    "            self.get_dataT()\n",
    "            data, labels, indices_env, name_envs = self.dataT.get_tree_data(with_env=False)\n",
    "            if self.equalize_label:\n",
    "                data, labels = DataTransfo_1SNP.equalize_label(data, labels)  \n",
    "            self.data = data          \n",
    "            self.nb_phenos = data.shape[1]\n",
    "            data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size = 1-self.prop_train_test, random_state=42)\n",
    "            data_train = CustomDatasetWithLabels(data_train, labels)\n",
    "            dataloader_train = DataLoader(data_train, batch_size=self.batch_size, shuffle=True)\n",
    "            data_test = CustomDatasetWithLabels(data_test, labels)\n",
    "            dataloader_test = DataLoader(data_test, batch_size=self.batch_size, shuffle=True)\n",
    "            self.model = NaiveModelWeights(pheno_nb=self.nb_phenos)\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.0001)\n",
    "            output_file = log_slurm_outputs_path\n",
    "\n",
    "            writer = SummaryWriter(log_tensorboard_path)\n",
    "            dic_features_list = {\n",
    "                'list_training_loss' : [],\n",
    "                'list_validation_loss' : [],\n",
    "                'list_proba_avg_zero' : [],\n",
    "                'list_proba_avg_one' : [],\n",
    "                'list_auc_validation' : [],\n",
    "                'list_accuracy_validation' : [],\n",
    "                'list_f1_validation' : [],\n",
    "                'epochs' : [] }\n",
    "            for epoch in range(self.total_epochs):\n",
    "                start_time_epoch = time.time()\n",
    "                total_loss = 0\n",
    "                for k, batch in enumerate(dataloader_train):\n",
    "                    data_train= batch['data']\n",
    "                    labels_train = batch['label']\n",
    "                    # evaluate the loss\n",
    "                    pred_probas, loss = self.model(data_train, labels_train)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                if epoch % self.eval_epochs_interval==0:\n",
    "                    dic_features = plot_infos(self.model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, plots_path)\n",
    "\n",
    "\n",
    "                \n",
    "             # Print time\n",
    "            print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "            start_time = time.time()\n",
    "\n",
    "            with open(log_data_path_pickle, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "            print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ## Add hyper parameters to tensorboard\n",
    "            hyperparams = {\"CHR\" : self.CHR, \"SNP\" : self.SNP, \"ROLLUP LEVEL\" : self.rollup_depth,\n",
    "                        'PHENO_METHOD': self.pheno_method, 'EMBEDDING_METHOD': self.embedding_method,\n",
    "                        'EMBEDDING SIZE' : self.Embedding_size, 'ATTENTION HEADS' : self.n_head, 'BLOCKS' : self.n_layer,\n",
    "                        'LR':1 , 'DROPOUT' : self.p_dropout, 'NUM_EPOCHS' : self.total_epochs, \n",
    "                        'BATCH_SIZE' : self.batch_size, \n",
    "                        'PADDING_MASKING': self.masking_padding,\n",
    "                        'VERSION' : self.model_version,\n",
    "                        'NB_Patients'  : len(self.data),\n",
    "                        'LOSS_VERSION'  : self.loss_version,\n",
    "                        }\n",
    "            writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "            self.trained = True\n",
    "\n",
    "            self.save_model(update=True)\n",
    "            self.add_to_trained()\n",
    "class TestSet:\n",
    "    def __init__(self, test_cat, test_field, test_name, param_dic={}, test_ref=None):\n",
    "        self.test_cat = test_cat\n",
    "        self.test_field = test_field\n",
    "        self.test_name = test_name\n",
    "        self.param_dic = param_dic\n",
    "        self.test_ref = test_ref\n",
    "        self.list_number_test = []\n",
    "        self.number_test_set = None\n",
    "        self.number_test_set = self.get_number()\n",
    "        self.test_name_with_infos = f'{self.number_test_set}_{self.test_name}'\n",
    "\n",
    "        self.test_set_save_file = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/{self.test_cat}/{self.test_field}/{self.test_name_with_infos}'\n",
    "        self.test_set_tensorboard_dir = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tests/{self.test_cat}/{self.test_field}/{self.test_name_with_infos}'\n",
    "    @property\n",
    "    def get_nombre_model(self):\n",
    "        return len(list(self.param_dic.values())[0])\n",
    "    \n",
    "    def get_number(self):\n",
    "        if self.number_test_set == None:\n",
    "            file = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/list_tests.csv'\n",
    "            df_tests = pd.read_csv(file)\n",
    "            if len(df_tests.index)==0:\n",
    "                self.number_test_set = 0\n",
    "            else:\n",
    "                self.number_test_set = int(np.max(df_tests.number_test_set)+1)\n",
    "        return self.number_test_set\n",
    "    \n",
    "    def create_tests(self):\n",
    "        if self.test_ref != None:\n",
    "            list_number_test = []\n",
    "            list_number_test_to_train = []\n",
    "            list_tests = [ copy.deepcopy(self.test_ref) for _ in range(self.get_nombre_model)]\n",
    "            for k, test in enumerate(list_tests):\n",
    "                for param in self.param_dic.keys():\n",
    "                    test.test_name = f'{test.test_name}_{param}={str(self.param_dic[param][k])}'\n",
    "                    test.__setattr__(param, self.param_dic[param][k])\n",
    "                test.save_model()\n",
    "\n",
    "            self.list_test = list_tests\n",
    "            path_model_test = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/training/list_models_tests'\n",
    "            for test in list_tests:\n",
    "                test.get_number_test()\n",
    "                train, number = test.test_already_trained()\n",
    "                \n",
    "                if not train:\n",
    "                    list_number_test_to_train.append(test.number_test)\n",
    "                    list_number_test.append(test.number_test)\n",
    "                else:\n",
    "                    list_number_test.append(test.number_test)\n",
    "            self.list_number_test = list_number_test\n",
    "        else:\n",
    "            print('case without test_ref not made yet')\n",
    "        self.save_test_set()\n",
    "        return test, list_number_test, list_number_test_to_train\n",
    "\n",
    "    \n",
    "    def save_test_set(self):\n",
    "        save_dir =  f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/{self.test_cat}/{self.test_field}/'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = f'{save_dir}{self.test_name_with_infos}'\n",
    "        with open(save_path, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "        self.add_to_saved()\n",
    "            \n",
    "    def get_row(self):\n",
    "        values_test = [self.number_test_set, self.test_cat, self.test_field, self.test_name]\n",
    "        df_test_set = pd.DataFrame([values_test],columns = ['number_test_set', 'test_cat', 'test_field', 'test_name'])\n",
    "        for k, test_instance_number in enumerate(self.list_number_test):\n",
    "            df_test_set[f'nb_instance_test_{k}'] = test_instance_number\n",
    "        return df_test_set\n",
    "            \n",
    "        \n",
    "    def add_to_saved(self):\n",
    "        df_list_test_set_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/list_tests.csv'\n",
    "        df_list_test_set = pd.read_csv(df_list_test_set_path)\n",
    "        df_test_set = self.get_row()\n",
    "        df_list_test_set =  pd.concat([df_list_test_set, df_test_set], ignore_index=True)\n",
    "        df_list_test_set.to_csv(df_list_test_set_path, index=False)\n",
    "    @staticmethod\n",
    "    def load_test_set_from_number(test_set_number):\n",
    "        test_cat, test_field, test_name, list_instance_test_set = TestSet.load_from_df(test_set_number)\n",
    "        test_name_with_infos = f'{test_set_number}_{test_name}'\n",
    "        test_set_path = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/{test_cat}/{test_field}/{test_name_with_infos}'\n",
    "        with open(test_set_path, 'rb') as file:\n",
    "            test_set = pickle.load(file)\n",
    "        return test_set\n",
    "    @staticmethod\n",
    "    def load_from_df(test_set_number):\n",
    "        df_list_test_set_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/list_tests.csv'\n",
    "        df_list_test_set = pd.read_csv(df_list_test_set_path)\n",
    "        df_list_test_set.set_index('number_test_set', inplace=True)\n",
    "        infos_test_set = df_list_test_set.iloc[test_set_number]\n",
    "        test_cat = infos_test_set['test_cat']\n",
    "        test_field = infos_test_set['test_field']\n",
    "        test_name = infos_test_set['test_name']\n",
    "        list_instance_test_set = []        \n",
    "        for col in df_list_test_set.columns:\n",
    "            if 'nb' in col:\n",
    "                print(type(infos_test_set[col]))\n",
    "                if infos_test_set[col] > 0:\n",
    "                    list_instance_test_set.append(infos_test_set[col])\n",
    "        return test_cat, test_field, test_name, list_instance_test_set\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_analyse_res_test_set(test_set_number=None, test_set=None, actual_test=True):\n",
    "        if test_set == None:\n",
    "            test_set = TestSet.load_test_set_from_number(test_set_number)\n",
    "        path_tensorboard_test_set = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/tests/{test_set.test_cat}/{test_set.test_field}/{test_set.test_name_with_infos}'\n",
    "        tensorboard_dir = test_set.test_set_tensorboard_dir\n",
    "        for instance_test_nb in test_set.list_number_test:\n",
    "            instance_test = TrainModel.load_instance_test(instance_test_nb)\n",
    "            print(instance_test.number_test, instance_test.number_test_trained)\n",
    "            if instance_test.number_test_trained != None:\n",
    "                instance_test_trained = TrainModel.load_instance_test(instance_test_number=instance_test.number_test_trained)\n",
    "                log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path  = instance_test_trained.generate_dirs()\n",
    "            else:\n",
    "                log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path  = instance_test.generate_dirs()\n",
    "\n",
    "            path_tensorboard_test_instance = f'{path_tensorboard_test_set}/{instance_test.test_name}'\n",
    "            if not os.path.exists(path_tensorboard_test_instance):\n",
    "                shutil.copytree(log_tensorboard_path, path_tensorboard_test_instance )\n",
    "        if actual_test:\n",
    "            shutil.rmtree(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/')\n",
    "            os.makedirs(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/')\n",
    "            actual_test_dir = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/{test_set.test_cat}/{test_set.test_field}/{test_set.test_name_with_infos}'\n",
    "            \n",
    "            shutil.copytree(path_tensorboard_test_set, actual_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creation of the reference model\n",
    "#### framework constants:\n",
    "model_type = 'transformer'\n",
    "model_version = 'transformer_V2'\n",
    "test_name = 'baseline_model'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "tryout = True # True if we are doing a tryout, False otherwise \n",
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "rollup_depth = 4\n",
    "binary_classes = False #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = True\n",
    "save_data = False\n",
    "remove_none = True\n",
    "compute_features = False\n",
    "padding = True\n",
    "list_env_features = []#['age', 'sex']\n",
    "list_phenos_ids = None #list(np.load('/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/phewas/list_associations_snps/rs673604_paul.npy'))#None\n",
    "\n",
    "### data format\n",
    "batch_size = 200\n",
    "data_share = 1#402555\n",
    "seuil_diseases = 600\n",
    "equalize_label = True\n",
    "decorelate = False\n",
    "threshold_corr = 1\n",
    "threshold_rare = 1000\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "##### model constants\n",
    "embedding_method ='Abby' #None, Paul, Abby\n",
    "counts_method = 'normal'#{'counts': 'SineCos', 'age':'SineCos'}\n",
    "freeze_embedding = True\n",
    "Embedding_size = 10 # Size of embedding.\n",
    "proj_embed = False\n",
    "instance_size = 10\n",
    "n_head = 4# number of SA heads\n",
    "n_layer = 2# number of blocks in parallel\n",
    "Head_size = 8 # size of the \"single Attention head\", which is the sum of the size of all multi Attention heads\n",
    "eval_epochs_interval = 5 # number of epoch between each evaluation print of the model (no impact on results)\n",
    "eval_batch_interval = 40\n",
    "p_dropout = 0.3 # proba of dropouts in the model\n",
    "masking_padding = True # do we include padding masking or not\n",
    "loss_version = 'cross_entropy' #cross_entropy or focal_loss\n",
    "gamma = 2\n",
    "alpha = 1\n",
    "L1 = True\n",
    "##### training constants\n",
    "total_epochs = 50 # number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = TrainTransformerModel(model_version=model_version, test_name=test_name, counts_method=counts_method, pheno_method=pheno_method, tryout=tryout, \n",
    "                                    CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                                    load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                                    data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                                    freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                                    eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                                    loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                                    learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare,\n",
    "                                    remove_rare=remove_rare, list_env_features=list_env_features, proj_embed=proj_embed, instance_size=instance_size, list_pheno_ids=list_phenos_ids, L1=L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = 'comparaisons'\n",
    "test_field = 'L1'\n",
    "test_name = 'L1_Abby'\n",
    "param_dic = {\n",
    "    'L1':[True, False]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = TestSet(test_cat=test_cat, test_field=test_field, test_name=test_name, param_dic=param_dic, test_ref=train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, list_number_test, list_number_test_to_train = testset.create_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_number_test_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = TrainModel.load_instance_test(233)\n",
    "train_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script_train_model = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/launch_tests/launch_bash_script_train_model.sh'\n",
    "scripts = [ (bash_script_train_model, str(test_instance_number)) for test_instance_number in list_number_test_to_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bash_script, *number_instance_test in scripts:\n",
    "    commande = ['bash', bash_script] + number_instance_test\n",
    "    res = subprocess.run(commande, capture_output=True, text=True)\n",
    "\n",
    "    print(f\"Sortie standard de {bash_script} :\", res.stdout)\n",
    "\n",
    "    # Afficher la sortie d'erreur du script Bash s'il y en a\n",
    "    if res.stderr:\n",
    "        print(f\"Erreur de {bash_script} :\", res.stderr)\n",
    "\n",
    "    # Vérifier le code de retour du processus\n",
    "    if res.returncode == 0:\n",
    "        print(f\"{bash_script} s'est exécuté avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_test = TrainModel.load_instance_test(141)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "instance_test = instance_test.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_test.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_test.model.padding_mask_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = TrainTabTransformerModel(model_version=model_version, test_name=test_name, counts_method=counts_method, pheno_method=pheno_method, tryout=tryout, \n",
    "                                    CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                                    load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                                    data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                                    freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                                    eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                                    loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                                    learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare,\n",
    "                                    remove_rare=remove_rare, list_env_features=list_env_features, proj_embed=proj_embed, instance_size=instance_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('dic_data', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('dic_data_test', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('dic_data_train', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dic = {'decorelate':[True,False],\n",
    "             'threshold-corr':[,],\n",
    "             'threshold_rare':[,], \n",
    "             'remove_rare': ['all','all']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dic = {'CHR':[],\n",
    "             'SNP':[]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### search for the SNPS \n",
    "dir_chrs = f'{path}codes/Data_Files/Training/SNPS'\n",
    "phewas_cat_file = f'{path}codes/Data_Files/phewas/phewas-catalog.csv'\n",
    "\n",
    "\n",
    "list_files = os.listdir(dir_chrs)\n",
    "list_chrs = []\n",
    "for chr in list_files:\n",
    "    try:\n",
    "        list_chrs.append(int(chr))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for chr in list_chrs:\n",
    "    dir_SNPS = f'{dir_chrs}/{str(chr)}'\n",
    "    list_SNPS = os.listdir(dir_SNPS)\n",
    "    for SNP in list_SNPS:\n",
    "        param_dic['CHR'].append(chr)\n",
    "        param_dic['SNP'].append(SNP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_cat = 'comparaison' # comparaison, assert_model\n",
    "test_field = 'test_multiple_SNP'\n",
    "test_name = 'test_multiple_SNP_try1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TestSet(test_cat, test_field, test_name, param_dic, test_ref=train_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, list_number_test, list_number_test_to_train = test_set.create_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_number_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script_train_model = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/launch_tests/launch_bash_script_train_model.sh'\n",
    "scripts = [ (bash_script_train_model, str(test_instance_number)) for test_instance_number in list_number_test_to_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bash_script, *number_instance_test in scripts:\n",
    "    commande = ['bash', bash_script] + number_instance_test\n",
    "    res = subprocess.run(commande, capture_output=True, text=True)\n",
    "\n",
    "    print(f\"Sortie standard de {bash_script} :\", res.stdout)\n",
    "\n",
    "    # Afficher la sortie d'erreur du script Bash s'il y en a\n",
    "    if res.stderr:\n",
    "        print(f\"Erreur de {bash_script} :\", res.stderr)\n",
    "\n",
    "    # Vérifier le code de retour du processus\n",
    "    if res.returncode == 0:\n",
    "        print(f\"{bash_script} s'est exécuté avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = TrainModel.load_instance_test(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainModel.remove_instance_test(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet.get_analyse_res_test_set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instance_tests_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/instance_tests/list_instance_tests_train.csv'\n",
    "df_instance_tests = pd.read_csv(df_instance_tests_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instance_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('/gpfs/commons/groups/gursoy_lab/mstoll/codes/Data_Files/Embeddings/Abby/embedding_abby_no_1_diseases.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(4, 5, 6).to(bool)\n",
    "data = torch.rand(4, 2, 5, 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0][1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = data.transpose(1, -1).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[mask]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = u.transpose(1, 2).transpose(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Matrice d'exemple\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Condition : éléments supérieurs à 5\n",
    "condition = tensor > 5\n",
    "\n",
    "# Filtrer la matrice en fonction de la condition\n",
    "filtered_tensor = tensor[condition]\n",
    "\n",
    "print(\"Matrice originale :\\n\", tensor)\n",
    "print(\"\\nÉléments supérieurs à 5 après filtrage :\\n\", filtered_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
