{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lancement programme\n"
     ]
    }
   ],
   "source": [
    "print('lancement programme', flush=True)\n",
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "from codes.tests.TestsClass import TrainModel, TrainTransformerModel, TestSet, TrainNaiveModel\n",
    "from codes.models.utils import clear_last_line, print_file, number_tests, Unbuffered, plot_infos, plot_ini_infos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### creation of the reference model\n",
    "#### framework constants:\n",
    "model_type = 'naive_model'\n",
    "model_version = 'version_1_weight'\n",
    "test_name = 'ref_whole_abby_focal_loss_small_equalized'\n",
    "pheno_method = 'Abby' # Paul, Abby\n",
    "tryout = False # True if we are doing a tryout, False otherwise \n",
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'\n",
    "rollup_depth = 4\n",
    "Classes_nb = 2 #nb of classes related to an SNP (here 0 or 1)\n",
    "vocab_size = None # to be defined with data\n",
    "padding_token = 0\n",
    "prop_train_test = 0.8\n",
    "load_data = False\n",
    "save_data = True\n",
    "remove_none = True\n",
    "compute_features = False\n",
    "padding = True\n",
    "list_env_features = []\n",
    "### data format\n",
    "batch_size = 100\n",
    "data_share = 1#402555\n",
    "seuil_diseases = 600\n",
    "equalize_label = True\n",
    "decorelate = False\n",
    "threshold_corr = 0.9\n",
    "threshold_rare = 50\n",
    "remove_rare = 'all' # None, 'all', 'one_class'\n",
    "##### model constants\n",
    "embedding_method = None #None, Paul, Abby\n",
    "freeze_embedding = False\n",
    "Embedding_size = 5 # Size of embedding.\n",
    "n_head = 2 # number of SA heads\n",
    "n_layer = 1 # number of blocks in parallel\n",
    "Head_size = 4 # size of the \"single Attention head\", which is the sum of the size of all multi Attention heads\n",
    "eval_epochs_interval = 5 # number of epoch between each evaluation print of the model (no impact on results)\n",
    "eval_batch_interval = 40\n",
    "p_dropout = 0.4 # proba of dropouts in the model\n",
    "masking_padding = True # do we include padding masking or not\n",
    "loss_version = 'cross_entropy' #cross_entropy or focal_loss\n",
    "gamma = 2\n",
    "alpha = 60\n",
    "##### training constants\n",
    "total_epochs = 100 # number of epochs\n",
    "learning_rate_max = 0.001 # maximum learning rate (at the end of the warmup phase)\n",
    "learning_rate_ini = 0.00001 # initial learning rate \n",
    "learning_rate_final = 0.0001\n",
    "warm_up_frac = 0.5 # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "end_factor_lr = learning_rate_final / learning_rate_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/runs/SNPS/1/rs673604/naive_model/version_1_weight/Abby/115_ref_whole_abby_focal_loss_small_equalized/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/TestsClass.py:273: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_list_instance_test_saved =  pd.concat([df_list_instance_test_saved, df_instance_test_row], ignore_index=True)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'loss_tot' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m train_model \u001b[38;5;241m=\u001b[39m TrainNaiveModel(model_version\u001b[38;5;241m=\u001b[39mmodel_version, test_name\u001b[38;5;241m=\u001b[39mtest_name, pheno_method\u001b[38;5;241m=\u001b[39mpheno_method, tryout\u001b[38;5;241m=\u001b[39mtryout, \n\u001b[1;32m      2\u001b[0m                                     CHR\u001b[38;5;241m=\u001b[39mCHR, SNP\u001b[38;5;241m=\u001b[39mSNP, rollup_depth\u001b[38;5;241m=\u001b[39mrollup_depth, Classes_nb\u001b[38;5;241m=\u001b[39mClasses_nb, padding_token\u001b[38;5;241m=\u001b[39mpadding_token, prop_train_test\u001b[38;5;241m=\u001b[39mprop_train_test,\n\u001b[1;32m      3\u001b[0m                                     load_data\u001b[38;5;241m=\u001b[39mload_data,save_data\u001b[38;5;241m=\u001b[39msave_data, remove_none\u001b[38;5;241m=\u001b[39mremove_none, compute_features\u001b[38;5;241m=\u001b[39mcompute_features, padding\u001b[38;5;241m=\u001b[39mpadding, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                                     learning_rate_final\u001b[38;5;241m=\u001b[39mlearning_rate_final,warm_up_frac\u001b[38;5;241m=\u001b[39mwarm_up_frac, decorelate\u001b[38;5;241m=\u001b[39mdecorelate, threshold_corr\u001b[38;5;241m=\u001b[39mthreshold_corr, threshold_rare\u001b[38;5;241m=\u001b[39mthreshold_rare,\n\u001b[1;32m      9\u001b[0m                                     remove_rare\u001b[38;5;241m=\u001b[39mremove_rare, list_env_features\u001b[38;5;241m=\u001b[39mlist_env_features)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/TestsClass.py:717\u001b[0m, in \u001b[0;36mTrainNaiveModel.train_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# evaluate the loss\u001b[39;00m\n\u001b[1;32m    716\u001b[0m pred_probas, loss \u001b[38;5;241m=\u001b[39m naive_model(data_train, labels_train)\n\u001b[0;32m--> 717\u001b[0m \u001b[43mloss_tot\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    718\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    719\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'loss_tot' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model = TrainNaiveModel(model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                                    CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, Classes_nb=Classes_nb, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                                    load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                                    data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                                    freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                                    eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                                    loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                                    learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare,\n",
    "                                    remove_rare=remove_rare, list_env_features=list_env_features)\n",
    "\n",
    "\n",
    "train_model.train_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
