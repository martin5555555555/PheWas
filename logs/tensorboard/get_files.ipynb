{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 17:32:30.120578: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-07 17:32:30.120755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-07 17:32:30.124083: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-07 17:32:44.478317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/'\n",
    "sys.path.append(path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR, LinearLR, SequentialLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from codes.models.data_form.DataSets import TabDictDataset\n",
    "from codes.models.data_form.DataForm import DataTransfo_1SNP, PatientList, Patient\n",
    "from codes.models.Transformers.Embedding import EmbeddingPheno, EmbeddingPhenoCat\n",
    "from codes.models.Transformers.dic_model_versions import DIC_MODEL_VERSIONS\n",
    "from codes.models.utils import clear_last_line, print_file, number_tests, Unbuffered, plot_infos, plot_ini_infos\n",
    "from codes.models.metrics import calculate_roc_auc\n",
    "from codes.models.Naive.Naive_model import NaiveModelWeights, CustomDatasetWithLabels\n",
    "from codes.models.Transformers.FT_Transformer import TabTransformerGeneModel_V2\n",
    "\n",
    "\n",
    "class TrainModel():\n",
    "    def __init__(self, model_type=None, model_version=None, test_name=None, pheno_method=None, tryout=None, binary_classes=None, counts_method=None, CHR=None, SNP=None, rollup_depth=None, padding_token=None, prop_train_test=None, load_data=None,\\\n",
    "                save_data=None, remove_none=None, compute_features=None, padding=None, batch_size=None, data_share=None, seuil_diseases=None, equalize_label=None, embedding_method=None, freeze_embedding=None, \\\n",
    "                Embedding_size=None, n_head=None, n_layer=None, Head_size=None, eval_epochs_interval=None, eval_batch_interval=None, p_dropout=None, masking_padding=None, \\\n",
    "                loss_version=None, gamma=None, alpha=None, total_epochs=None, learning_rate_max=None, \\\n",
    "                learning_rate_ini=None, learning_rate_final=None,\\\n",
    "                warm_up_frac=None, from_test=False, decorelate=False, threshold_corr=1, threshold_rare=0, remove_rare=None, list_env_features=[],\n",
    "                proj_embed=True, instance_size=None, indices=None): \n",
    "        self.model_type = model_type\n",
    "        self.model_version = model_version\n",
    "        self.test_name = test_name\n",
    "        self.pheno_method = pheno_method # Paul, Abby\n",
    "        self.tryout = tryout # True if we are doing a tryout, False otherwise \n",
    "        ### data constants:\n",
    "        self.CHR = CHR\n",
    "        self.SNP = SNP\n",
    "        self.rollup_depth = rollup_depth\n",
    "        self.binary_classes = binary_classes #nb of classes related to an SNP (here 0 or 1)\n",
    "       \n",
    "        self.padding_token = padding_token\n",
    "        self.prop_train_test = prop_train_test\n",
    "        self.load_data = load_data\n",
    "        self.save_data = save_data\n",
    "        self.remove_none = remove_none\n",
    "        self.compute_features = compute_features\n",
    "        self.padding = padding\n",
    "        ### data format\n",
    "        self.batch_size = batch_size\n",
    "        self.data_share = data_share #402555\n",
    "        self.seuil_diseases = seuil_diseases\n",
    "        self.equalize_label = equalize_label\n",
    "        self.decorelate = decorelate\n",
    "        self.threshold_corr = threshold_corr\n",
    "        self.threshold_rare = threshold_rare\n",
    "        self.remove_rare = remove_rare\n",
    "        ##### model constants\n",
    "        self.embedding_method = embedding_method #None, Paul, Abby\n",
    "        self.counts_method = counts_method\n",
    "        self.freeze_embedding = freeze_embedding\n",
    "        self.Embedding_size = Embedding_size # Size of embedding.\n",
    "        self.proj_embed = proj_embed\n",
    "        self.instance_size = instance_size\n",
    "        self.n_head = n_head # number of SA heads\n",
    "        self.n_layer = n_layer # number of blocks in parallel\n",
    "        self.Head_size = Head_size  # size of the \"single Attention head\", which is the sum of the size of all multi Attention heads\n",
    "        self.eval_epochs_interval = eval_epochs_interval # number of epoch between each evaluation print of the model (no impact on results)\n",
    "        self.eval_batch_interval = eval_batch_interval\n",
    "        self.p_dropout = p_dropout # proba of dropouts in the model\n",
    "        self.masking_padding = masking_padding # do we include padding masking or not\n",
    "        self.loss_version = loss_version #cross_entropy or focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        ##### training constants\n",
    "        self.total_epochs = total_epochs # number of epochs\n",
    "        self.learning_rate_max = learning_rate_max # maximum learning rate (at the end of the warmup phase)\n",
    "        self.learning_rate_ini = learning_rate_ini # initial learning rate \n",
    "        self.learning_rate_final = learning_rate_final\n",
    "        self.warm_up_frac = warm_up_frac # fraction of the size of the warmup stage with regards to the total number of epochs.\n",
    "        self.start_factor_lr = learning_rate_ini / learning_rate_max\n",
    "        self.end_factor_lr = learning_rate_final / learning_rate_max\n",
    "        self.warm_up_size = int(total_epochs*warm_up_frac)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.number_test = None\n",
    "        self.number_test = self.get_number_test()\n",
    "        print(self.tryout)\n",
    "        self.trained = False\n",
    "        # to be defined with data\n",
    "        self.vocab_size = None \n",
    "        self.max_count_same_disease = None\n",
    "        #### essential components of a model\n",
    "        self.dataT = None\n",
    "        self.patient_list = None\n",
    "        self.model = None\n",
    "        self.number_test_trained = None\n",
    "        self.list_env_features = list_env_features\n",
    "        self.indices=indices\n",
    "\n",
    "       \n",
    "    @property\n",
    "    def get_test_name_with_infos(self):\n",
    "        return str(self.number_test) + '_' + self.test_name + 'tryout'*self.tryout\n",
    "    def get_number_test(self, update=False):\n",
    "        if self.number_test != None and update==False:\n",
    "            return self.number_test\n",
    "        else:\n",
    "            df_list_instance_test_saved_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "            df_list_instance_test_saved = pd.read_csv(df_list_instance_test_saved_path)\n",
    "            if len(df_list_instance_test_saved.index)==0:\n",
    "                    self.number_test = 1\n",
    "            else:\n",
    "                    number_test_ex =  np.max(df_list_instance_test_saved['number_test'])\n",
    "                    self.number_test = number_test_ex + 1\n",
    "            \n",
    "            return self.number_test\n",
    "    \n",
    "    def get_dataT(self):\n",
    "        \n",
    "        if self.dataT != None:\n",
    "            pass        \n",
    "        else:\n",
    "            print('Bulding dataT')\n",
    "            dataT = DataTransfo_1SNP(SNP=self.SNP,\n",
    "                         CHR=self.CHR,\n",
    "                         method=self.pheno_method,\n",
    "                         padding=self.padding,  \n",
    "                         binary_classes=self.binary_classes,\n",
    "                         pad_token=self.padding_token, \n",
    "                         load_data=self.load_data, \n",
    "                         save_data=self.save_data, \n",
    "                         compute_features=self.compute_features,\n",
    "                         data_share=self.data_share,\n",
    "                         prop_train_test=self.prop_train_test,\n",
    "                         remove_none=self.remove_none,\n",
    "                         rollup_depth=self.rollup_depth,\n",
    "                         equalize_label=self.equalize_label,\n",
    "                         seuil_diseases=self.seuil_diseases,\n",
    "                         decorelate=self.decorelate,\n",
    "                         threshold_corr=self.threshold_corr,\n",
    "                         threshold_rare=self.threshold_rare,\n",
    "                         remove_rare=self.remove_rare,\n",
    "                         list_env_features=self.list_env_features, \n",
    "                         indices=self.indices)\n",
    "            self.dataT = dataT\n",
    "            print(f'got dataT')\n",
    "        return self.dataT\n",
    "        \n",
    "    def get_patient_list(self):\n",
    "        if self.patient_list == None:\n",
    "            self.patient_list = self.get_dataT().get_patientlist()\n",
    "        return self.patient_list\n",
    "    \n",
    "    def get_model(self):\n",
    "        Embedding  = EmbeddingPheno(method=self.embedding_method, \n",
    "                            counts_method=self.counts_method,\n",
    "                            vocab_size=self.vocab_size, \n",
    "                            max_count_same_disease=self.max_count_same_disease, \n",
    "                            Embedding_size=self.Embedding_size, \n",
    "                            rollup_depth=self.rollup_depth, \n",
    "                            freeze_embed=self.freeze_embedding,\n",
    "                            dicts=self.dataT.dicts)\n",
    "        self.Embedding = Embedding\n",
    "        ### creation of the model\n",
    "        ClassModel = DIC_MODEL_VERSIONS[self.model_version]\n",
    "        model = ClassModel(pheno_method = self.pheno_method,\n",
    "                                    Embedding = Embedding,\n",
    "                                    Head_size=self.Head_size,\n",
    "                                    binary_classes=self.binary_classes,\n",
    "                                    n_head=self.n_head,\n",
    "                                    n_layer=self.n_layer,\n",
    "                                    mask_padding=self.masking_padding, \n",
    "                                    padding_token=0, \n",
    "                                    p_dropout=self.p_dropout, \n",
    "                                    loss_version = self.loss_version, \n",
    "                                    gamma = self.gamma,\n",
    "                                    alpha = self.alpha,\n",
    "                                    device = self.device,\n",
    "                                    proj_embed=self.proj_embed,\n",
    "                                    instance_size=self.instance_size)\n",
    "      \n",
    "        # print the number of parameters in the model\n",
    "        print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def generate_dirs(self, makedirs=True):\n",
    "\n",
    "        if self.model_type == 'transformer' or self.model_type == 'naive_model' or self.model_type == 'tab_transformer':\n",
    "            path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "\n",
    "            #check test name\n",
    "            model_dir = path + f'logs/runs/SNPS/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}'\n",
    "            model_plot_dir = path + f'logs/plots/tests/SNP/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}/'\n",
    "\n",
    "            if makedirs:\n",
    "                os.makedirs(model_dir, exist_ok=True)\n",
    "                os.makedirs(model_plot_dir, exist_ok=True)\n",
    "            #check number tests\n",
    "            test_dir = f'{model_dir}/{self.get_test_name_with_infos}/'\n",
    "            print(test_dir)\n",
    "            log_data_dir = f'{test_dir}/data/'\n",
    "            log_tensorboard_dir = f'{test_dir}/tensorboard/'\n",
    "            log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "            log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "            if makedirs:\n",
    "                os.makedirs(log_data_dir, exist_ok=True)\n",
    "                os.makedirs(log_tensorboard_dir, exist_ok=True)\n",
    "                os.makedirs(log_slurm_outputs_dir, exist_ok=True)\n",
    "                os.makedirs(log_slurm_errors_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "            log_data_path_pickle = f'{test_dir}/data/{self.test_name}.pkl'\n",
    "            log_tensorboard_path = f'{test_dir}/tensorboard/{self.test_name}'\n",
    "            log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{self.test_name}.txt'\n",
    "            log_slurm_error_path = f'{test_dir}/Slurm/Errors/{self.test_name}.txt'\n",
    "            model_plot_path = path + f'logs/plots/tests/SNP/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}/{self.get_test_name_with_infos}.png'\n",
    "\n",
    "            return log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, model_plot_path\n",
    "        elif self.model_type == 'logistic_regression':\n",
    "            path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "\n",
    "            #check test name\n",
    "            model_dir = path + f'logs/runs/SNPS/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}'\n",
    "            model_plot_dir = path + f'logs/plots/tests/SNP/{str(self.CHR)}/{self.SNP}/{self.model_type}/{self.model_version}/{self.pheno_method}/'\n",
    "\n",
    "            if makedirs:\n",
    "                os.makedirs(model_dir, exist_ok=True)\n",
    "                os.makedirs(model_plot_dir, exist_ok=True)\n",
    "            #check number tests\n",
    "            test_dir = f'{model_dir}/{self.get_test_name_with_infos}/'\n",
    "            print(test_dir)\n",
    "            log_data_dir = f'{test_dir}/data/'\n",
    "            log_res_dir = f'{test_dir}/res/'\n",
    "            log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "            log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "            if makedirs:\n",
    "                os.makedirs(log_data_dir, exist_ok=True)\n",
    "                os.makedirs(log_res_dir, exist_ok=True)\n",
    "                os.makedirs(log_slurm_outputs_dir, exist_ok=True)\n",
    "                os.makedirs(log_slurm_errors_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "            log_data_path_pickle = f'{test_dir}/data/{self.test_name}.pkl'\n",
    "            log_res_path_txt = f'{test_dir}/res/{self.test_name}.txt'\n",
    "            log_res_path_pkl = f'{test_dir}/res/{self.test_name}.pkl'\n",
    "\n",
    "            log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{self.test_name}.txt'\n",
    "            log_slurm_error_path = f'{test_dir}/Slurm/Errors/{self.test_name}.txt'\n",
    "\n",
    "            return log_data_path_pickle, log_res_path_txt, log_res_path_pkl, log_slurm_outputs_path, log_slurm_error_path\n",
    "\n",
    "    def to_csv(self):\n",
    "        dic_attrib = self.__dict__.copy()\n",
    "        keys_to_drop = ['dataT', 'patient_list', 'model', 'patient_list_transformer_train', 'patient_list_transformer_test']\n",
    "        for key in keys_to_drop:\n",
    "            if key in list(dic_attrib.keys()):\n",
    "                dic_attrib.pop(key)\n",
    "        attributes = list(dic_attrib.keys())\n",
    "        values = list(dic_attrib.values())\n",
    "        df_row_test = pd.DataFrame([values],columns=attributes)\n",
    "        return df_row_test\n",
    "    \n",
    "    def add_to_trained(self):\n",
    "        df_row_test = self.to_csv()\n",
    "        df_list_instance_test_trained_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/instance_tests/list_instance_tests_train.csv'\n",
    "        df_list_instance_test_trained = pd.read_csv(df_list_instance_test_trained_path)\n",
    "        df_instance_test_row = self.to_csv()\n",
    "        df_list_instance_test_trained =  pd.concat([df_list_instance_test_trained, df_instance_test_row], ignore_index=True)\n",
    "        df_list_instance_test_trained.to_csv(df_list_instance_test_trained_path, index=False)\n",
    "    def add_to_saved(self):\n",
    "        df_row_test = self.to_csv()\n",
    "        df_list_instance_test_saved_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "        df_list_instance_test_saved = pd.read_csv(df_list_instance_test_saved_path)\n",
    "        df_instance_test_row = self.to_csv()\n",
    "        df_list_instance_test_saved =  pd.concat([df_list_instance_test_saved, df_instance_test_row], ignore_index=True)\n",
    "        df_list_instance_test_saved.to_csv(df_list_instance_test_saved_path, index=False)\n",
    "\n",
    "    def is_equal(self, test):\n",
    "        pass\n",
    "            \n",
    "    def save_model(self, update=False):\n",
    "        \n",
    "       \n",
    "        already_saved, number_test = self.test_already_saved()\n",
    "        if already_saved and not update:\n",
    "            self.number_test = number_test\n",
    "            print('test has already been saved')\n",
    "\n",
    "        else:\n",
    "            if not update:\n",
    "                self.get_number_test(update=True)\n",
    "            path_instance_test_save_dir= '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/' \n",
    "            path_instance_test_save = f'{path_instance_test_save_dir}{self.get_test_name_with_infos}.pkl'\n",
    "            with open(path_instance_test_save, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "            if update:\n",
    "                TrainModel.remove_instance_test_df(self.number_test)\n",
    "            self.add_to_saved()\n",
    "\n",
    "    def test_already_trained(self):\n",
    "        \n",
    "            df_instance_tests_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/instance_tests/list_instance_tests_train.csv'\n",
    "            df_instance_tests = pd.read_csv(df_instance_tests_path)\n",
    "            df_instance_tests_dropped = df_instance_tests.copy()\n",
    "            df_test = self.to_csv()\n",
    "\n",
    "            cols_drop_check_train = ['test_name', 'number_test', 'trained', 'vocab_size', 'max_count_same_disease', 'test_name_with_infos', 'device', 'number_test_trained']\n",
    "            for col in cols_drop_check_train:\n",
    "                if col in df_instance_tests_dropped.columns:\n",
    "                    df_instance_tests_dropped = df_instance_tests_dropped.drop(col, axis=1)\n",
    "                if col in df_test.columns:\n",
    "                    df_test = df_test.drop(col, axis=1)\n",
    "            already_trained = False\n",
    "            already_trained_index = list(df_instance_tests['number_test'][df_instance_tests_dropped.apply(lambda row: row.equals(df_test.iloc[0]), axis=1)])\n",
    "            if already_trained_index == []:\n",
    "                already_trained =False\n",
    "                number_test = 0\n",
    "            else:\n",
    "\n",
    "                already_trained = True\n",
    "                number_test = already_trained_index[0]\n",
    "                self.number_test_trained = number_test\n",
    "            self.save_model(update=True)\n",
    "            return already_trained, number_test  \n",
    "\n",
    "    def test_already_saved(self):\n",
    "        \n",
    "        df_instance_tests_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "        df_instance_tests = pd.read_csv(df_instance_tests_path)\n",
    "        df_instance_tests_dropped = df_instance_tests.copy()\n",
    "\n",
    "        df_test = self.to_csv()\n",
    "        cols_drop_check_train = ['number_test', 'trained', 'vocab_size', 'max_count_same_disease', 'test_name_with_infos', 'device', 'number_test_trained']\n",
    "        for col in cols_drop_check_train:\n",
    "            if col in df_instance_tests_dropped.columns:\n",
    "                df_instance_tests_dropped = df_instance_tests_dropped.drop(col, axis=1)\n",
    "            if col in df_test.columns:\n",
    "                df_test = df_test.drop(col, axis=1)\n",
    "        already_saved = False\n",
    "        already_saved_index = list(df_instance_tests['number_test'][df_instance_tests_dropped.apply(lambda row: row.equals(df_test.iloc[0]), axis=1)])\n",
    "        if already_saved_index == []:\n",
    "            already_saved =False\n",
    "            number_test = 0\n",
    "        else:\n",
    "            already_saved = True\n",
    "            number_test = already_saved_index[0]\n",
    "        return already_saved, number_test    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load_instance_test(instance_test_number):\n",
    "        instance_model_dir = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/'\n",
    "        for instance_test in os.listdir(instance_model_dir):\n",
    "            if instance_test == 'list_instance_tests_saved.csv':\n",
    "                pass\n",
    "            else:\n",
    "                number_instance_test_try = int(instance_test.split('_')[0])\n",
    "            \n",
    "                if number_instance_test_try == instance_test_number:\n",
    "                    with open(f'{instance_model_dir}{instance_test}', 'rb') as file:\n",
    "                        test_instance = pickle.load(file)\n",
    "                        return test_instance\n",
    "        print('test was not found')\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_instance_test(instance_test_number):\n",
    "        instance_model_dir = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/'\n",
    "        for instance_test in os.listdir(instance_model_dir):\n",
    "            if instance_test == 'list_instance_tests_saved.csv':\n",
    "                pass\n",
    "            else:\n",
    "                number_instance_test_try = int(instance_test.split('_')[0])\n",
    "            \n",
    "                if number_instance_test_try == instance_test_number:\n",
    "                    os.remove(f'{instance_model_dir}{instance_test}')\n",
    "        TrainModel.remove_instance_test_df(instance_test_number)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_instance_test_df(instance_test_number):\n",
    "        df_list_instance_test_saved_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/list_instance_tests_saved.csv'\n",
    "        df_list_instance_test_saved = pd.read_csv(df_list_instance_test_saved_path)\n",
    "        df_list_instance_test_saved = df_list_instance_test_saved[ df_list_instance_test_saved['number_test'] != instance_test_number]\n",
    "        df_list_instance_test_saved.to_csv(df_list_instance_test_saved_path, index=False)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_res_instance_test(instance_test_number=None, instance_test=None, actual_test=False,add=True):\n",
    "        if instance_test == None:\n",
    "            instance_test = TrainModel.load_instance_test(instance_test_number)\n",
    "        path_tensorboard_test_instance = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/tests/all_instances/SNPS/{str(instance_test.CHR)}/{instance_test.SNP}/{instance_test.model_type}/{instance_test.model_version}/{instance_test.pheno_method}/{instance_test.get_test_name_with_infos}'\n",
    "        if instance_test.number_test_trained != None:\n",
    "            instance_test_trained = TrainModel.load_instance_test(instance_test_number=instance_test.number_test_trained)\n",
    "            log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path  = instance_test_trained.generate_dirs()\n",
    "        else:\n",
    "            log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path  = instance_test.generate_dirs()\n",
    "\n",
    "        if os.path.exists(path_tensorboard_test_instance): \n",
    "            shutil.rmtree(path_tensorboard_test_instance)\n",
    "\n",
    "        shutil.copytree(log_tensorboard_path, path_tensorboard_test_instance )\n",
    "        if actual_test:\n",
    "            if add==False:\n",
    "                shutil.rmtree(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/')\n",
    "            os.makedirs(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/', exist_ok=True)\n",
    "            actual_test_dir = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/{instance_test.get_test_name_with_infos}'\n",
    "            if os.path.exists(actual_test_dir): \n",
    "                shutil.rmtree(actual_test_dir)\n",
    "\n",
    "            shutil.copytree(path_tensorboard_test_instance, actual_test_dir)\n",
    "\n",
    "class TrainTransformerModel(TrainModel):\n",
    "    def __init__(self, model_version, test_name, pheno_method, tryout, CHR, SNP, rollup_depth, binary_classes,counts_method, padding_token, prop_train_test, load_data,\n",
    "                save_data, remove_none, compute_features, padding, batch_size, data_share, seuil_diseases, equalize_label, embedding_method, freeze_embedding, Embedding_size, n_head, \n",
    "                 n_layer, Head_size, eval_epochs_interval, eval_batch_interval, p_dropout, masking_padding, loss_version, gamma, alpha, total_epochs, learning_rate_max, learning_rate_ini, learning_rate_final,\n",
    "                    warm_up_frac, decorelate, threshold_corr, threshold_rare, remove_rare, list_env_features, proj_embed, instance_size, indices=None):\n",
    "        self.model_type = 'transformer'\n",
    "        super().__init__(model_type=self.model_type, model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                        CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, counts_method=counts_method, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                        load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                        data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                        freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                        eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                        loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                        learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare, remove_rare=remove_rare, list_env_features=list_env_features,\n",
    "                        proj_embed=proj_embed, instance_size=instance_size, indices=indices)\n",
    "        \n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        self.get_number_test() #update the test number\n",
    "        already_trained, number_test = self.test_already_trained()\n",
    "        \n",
    "        log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path = self.generate_dirs(makedirs = True)\n",
    "        # Redirect  output to a file\n",
    "        sys.stdout = open(log_slurm_outputs_path, 'w')\n",
    "        sys.stderr = open(log_slurm_error_path, 'w')\n",
    "        sys.stdout = Unbuffered(sys.stdout)\n",
    "        sys.stderr = Unbuffered(sys.stderr)\n",
    "        if already_trained:\n",
    "            print('model has already been trained')\n",
    "           \n",
    "        else: \n",
    "            print(f'Begining of training program, device={self.device}')\n",
    "            self.get_dataT()\n",
    "            self.get_patient_list()\n",
    "            \n",
    "\n",
    "\n",
    "            indices_train, indices_test = self.dataT.get_indices_train_test(nb_data=len(self.patient_list),prop_train_test=self.prop_train_test)\n",
    "            self.patient_list_transformer_train, self.patient_list_transformer_test = self.patient_list.get_transformer_data(indices_train.astype(int), indices_test.astype(int))\n",
    "            #creation of torch Datasets:\n",
    "            dataloader_train = DataLoader(self.patient_list_transformer_train, batch_size=self.batch_size, shuffle=True)\n",
    "            dataloader_test = DataLoader(self.patient_list_transformer_test, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "            if self.patient_list.nb_distinct_diseases_tot==None:\n",
    "                vocab_size = self.patient_list.get_nb_distinct_diseases_tot()\n",
    "            if self.patient_list.nb_max_counts_same_disease==None:\n",
    "                max_count_same_disease = self.patient_list.get_max_count_same_disease()\n",
    "            self.max_count_same_disease = self.patient_list.nb_max_counts_same_disease\n",
    "            self.vocab_size = self.patient_list.get_nb_distinct_diseases_tot()\n",
    "\n",
    "            print(f'\\n vocab_size : {self.vocab_size}, max_count : {self.max_count_same_disease}\\n', \n",
    "                f'length_patient = {self.patient_list.get_nb_max_distinct_diseases_patient()}\\n',\n",
    "                f'sparcity = {self.patient_list.sparsity}\\n',\n",
    "                f'nombres patients  = {len(self.patient_list)}')\n",
    "            \n",
    "            writer = SummaryWriter(log_tensorboard_path)\n",
    "\n",
    "            self.get_model()\n",
    "\n",
    "\n",
    "                                \n",
    "            self.model.to(self.device)\n",
    "            # print the number of parameters in the model\n",
    "            print(sum(p.numel() for p in self.model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate_max)\n",
    "            lr_scheduler_warm_up = LinearLR(optimizer, start_factor=self.start_factor_lr , end_factor=1, total_iters=self.warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "            lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=self.total_epochs-self.warm_up_size, end_factor=self.end_factor_lr)\n",
    "            lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[self.warm_up_size])\n",
    "\n",
    "\n",
    "            output_file = log_slurm_outputs_path\n",
    "            ## Open tensor board writer\n",
    "            dic_features_list = {\n",
    "            'list_training_loss' : [],\n",
    "            'list_validation_loss' : [],\n",
    "            'list_proba_avg_zero' : [],\n",
    "            'list_proba_avg_one' : [],\n",
    "            'list_auc_validation' : [],\n",
    "            'list_accuracy_validation' : [],\n",
    "            'list_f1_validation' : [],\n",
    "            'epochs' : [] }\n",
    "\n",
    "            # Training Loop\n",
    "            start_time_training = time.time()\n",
    "            print_file(output_file, f'Beginning of the program for {self.total_epochs} epochs', new_line=True)\n",
    "            # Training Loop\n",
    "            plot_ini_infos(self.model, output_file, dataloader_test, dataloader_train, writer, dic_features_list)\n",
    "            for epoch in range(1, self.total_epochs+1):\n",
    "\n",
    "                start_time_epoch = time.time()\n",
    "                total_loss = 0.0  \n",
    "                \n",
    "                #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "                for k, (batch_sentences, batch_counts, batch_labels) in enumerate(dataloader_train):\n",
    "                    \n",
    "                    batch_sentences = batch_sentences.to(self.device)\n",
    "                    batch_counts = batch_counts.to(self.device)\n",
    "                    batch_labels = batch_labels.to(self.device)\n",
    "\n",
    "                    # evaluate the loss\n",
    "                    logits, loss = self.model(batch_sentences, batch_counts, batch_labels)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                \n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if k % self.eval_batch_interval == 0:\n",
    "                        clear_last_line(output_file)\n",
    "                        print_file(output_file, f'Progress in epoch {epoch}  = {round(k / len(dataloader_train)*100, 2)} %, time batch : {time.time() - start_time_epoch}', new_line=False)\n",
    "\n",
    "                if epoch % self.eval_epochs_interval == 0:\n",
    "                    dic_features = plot_infos(self.model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, plots_path)\n",
    "\n",
    "                \n",
    "                \n",
    "                lr_scheduler.step()\n",
    "\n",
    "            self.dic_features = dic_features\n",
    "            self.model.to('cpu')\n",
    "            self.Embedding.to('cpu')\n",
    "            self.model.write_embedding(writer)\n",
    "            # Print time\n",
    "            print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "            start_time = time.time()\n",
    "\n",
    "            with open(log_data_path_pickle, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "            print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            ## Add hyper parameters to tensorboard\n",
    "            hyperparams = {\"CHR\" : self.CHR, \"SNP\" : self.SNP, \"ROLLUP LEVEL\" : self.rollup_depth,\n",
    "                        'PHENO_METHOD': self.pheno_method, 'EMBEDDING_METHOD': self.embedding_method,\n",
    "                        'EMBEDDING SIZE' : self.Embedding_size, 'ATTENTION HEADS' : self.n_head, 'BLOCKS' : self.n_layer,\n",
    "                        'LR':1 , 'DROPOUT' : self.p_dropout, 'NUM_EPOCHS' : self.total_epochs, \n",
    "                        'BATCH_SIZE' : self.batch_size, \n",
    "                        'PADDING_MASKING': self.masking_padding,\n",
    "                        'VERSION' : self.model_version,\n",
    "                        'NB_Patients'  : len(self.patient_list),\n",
    "                        'LOSS_VERSION'  : self.loss_version,\n",
    "                        }\n",
    "            writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "            self.trained = True\n",
    "\n",
    "            self.save_model(update=True)\n",
    "            self.add_to_trained()\n",
    "\n",
    "class TrainTabTransformerModel(TrainModel):\n",
    "    def __init__(self, model_version, test_name, pheno_method, tryout, CHR, SNP, rollup_depth, binary_classes, counts_method,  padding_token, prop_train_test, load_data,\n",
    "                save_data, remove_none, compute_features, padding, batch_size, data_share, seuil_diseases, equalize_label, embedding_method, freeze_embedding, Embedding_size, n_head, \n",
    "                 n_layer, Head_size, eval_epochs_interval, eval_batch_interval, p_dropout, masking_padding, loss_version, gamma, alpha, total_epochs, learning_rate_max, learning_rate_ini, learning_rate_final,\n",
    "                    warm_up_frac, decorelate, threshold_corr, threshold_rare, remove_rare, list_env_features, proj_embed, instance_size, indices=None):\n",
    "        self.model_type = 'tab_transformer'\n",
    "        super().__init__(model_type=self.model_type, model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                        CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, counts_method=counts_method, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                        load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                        data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                        freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                        eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                        loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                        learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare, remove_rare=remove_rare, list_env_features=list_env_features,\n",
    "                        proj_embed=proj_embed, instance_size=instance_size, indices=indices)\n",
    "        \n",
    "    \n",
    "    def get_model(self):\n",
    "        Embedding  = EmbeddingPhenoCat(method=self.embedding_method, \n",
    "                            pheno_method=self.pheno_method,\n",
    "                            Embedding_size=self.Embedding_size, \n",
    "                            rollup_depth=self.rollup_depth, \n",
    "                            freeze_embed=self.freeze_embedding,\n",
    "                            dic_embedding_cat_params=self.dic_embedding_cat_params,\n",
    "                            dicts=self.dataT.dicts)\n",
    "        self.Embedding = Embedding\n",
    "        ### creation of the model\n",
    "        model = TabTransformerGeneModel_V2(\n",
    "                                    pheno_method = self.pheno_method,\n",
    "                                    Embedding = Embedding,\n",
    "                                    Head_size=self.Head_size,\n",
    "                                    binary_classes=self.binary_classes,\n",
    "                                    n_head=self.n_head,\n",
    "                                    n_layer=self.n_layer,\n",
    "                                    mask_padding=self.masking_padding, \n",
    "                                    padding_token=0, \n",
    "                                    p_dropout=self.p_dropout, \n",
    "                                    loss_version = self.loss_version, \n",
    "                                    gamma = self.gamma,\n",
    "                                    alpha = self.alpha,\n",
    "                                    device = self.device,\n",
    "                                    list_env_features = self.list_env_features\n",
    "                                          )\n",
    "        # print the number of parameters in the model\n",
    "        print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train_model(self):\n",
    "        self.get_number_test() #update the test number\n",
    "        already_trained, number_test = self.test_already_trained()\n",
    "        \n",
    "        log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path = self.generate_dirs(makedirs = True)\n",
    "        # Redirect  output to a file\n",
    "        sys.stdout = open(log_slurm_outputs_path, 'w')\n",
    "        sys.stderr = open(log_slurm_error_path, 'w')\n",
    "        sys.stdout = Unbuffered(sys.stdout)\n",
    "        sys.stderr = Unbuffered(sys.stderr)\n",
    "        if already_trained:\n",
    "            print('model has already been trained')\n",
    "           \n",
    "        else: \n",
    "            print(f'Begining of training program, device={self.device}')\n",
    "            self.get_dataT()\n",
    "            self.dic_data= self.dataT.get_data_tabtransfo(actualise_phenos=True)\n",
    "            indices_train, indices_test = self.dataT.get_indices_train_test(nb_data=len(self.dic_data['diseases']))\n",
    "\n",
    "            self.dic_data_train = {key: np.array(self.dic_data[key])[indices_train] for key in self.dic_data.keys()}\n",
    "            self.dic_data_test = {key: np.array(self.dic_data[key])[indices_test] for key in self.dic_data.keys()}\n",
    "\n",
    "            max_number_diseases = len(self.dataT.dicts['id'] ) \n",
    "            max_number_counts = np.max([np.max(self.dic_data['counts'][k]) for k in range(len(self.dic_data['counts']))]) + 1\n",
    "            max_number_age = np.max(np.array(self.dic_data['age'])) + 1\n",
    "            max_number_sex = 2\n",
    "            self.dic_embedding_cat_params = {'diseases':max_number_diseases, 'counts':max_number_counts, 'age':max_number_age, 'sex':max_number_sex} \n",
    "\n",
    "            dataset_train = TabDictDataset(self.dic_data_train)\n",
    "            dataset_test = TabDictDataset(self.dic_data_test)\n",
    "\n",
    "            dataloader_train =  DataLoader(dataset_train, batch_size = self.batch_size, shuffle=True)\n",
    "            dataloader_test =  DataLoader(dataset_test, batch_size = self.batch_size, shuffle=True)\n",
    "            \n",
    "            self.dataloader_train = dataloader_train\n",
    "            self.dataloader_test = dataloader_test\n",
    "            self.vocab_size = max_number_diseases\n",
    "            self.max_count_same_disease =max_number_diseases\n",
    "\n",
    "            print(f'\\n vocab_size : {self.vocab_size}, max_count : {self.max_count_same_disease}\\n', \n",
    "                f'length_patient = {len(self.dic_data[\"diseases\"][0])}',\n",
    "                f'nombres patients  = {len(self.dic_data[\"diseases\"])}')\n",
    "            \n",
    "            writer = SummaryWriter(log_tensorboard_path)\n",
    "\n",
    "            self.get_model()\n",
    "\n",
    "\n",
    "                                \n",
    "            self.model.to(self.device)\n",
    "            # print the number of parameters in the model\n",
    "            print(sum(p.numel() for p in self.model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate_max)\n",
    "            lr_scheduler_warm_up = LinearLR(optimizer, start_factor=self.start_factor_lr , end_factor=1, total_iters=self.warm_up_size, verbose=False) # to schedule a modification in the learning rate\n",
    "            lr_scheduler_final = LinearLR(optimizer, start_factor=1, total_iters=self.total_epochs-self.warm_up_size, end_factor=self.end_factor_lr)\n",
    "            lr_scheduler = SequentialLR(optimizer, schedulers=[lr_scheduler_warm_up, lr_scheduler_final], milestones=[self.warm_up_size])\n",
    "\n",
    "\n",
    "            output_file = log_slurm_outputs_path\n",
    "            ## Open tensor board writer\n",
    "            dic_features_list = {\n",
    "            'list_training_loss' : [],\n",
    "            'list_validation_loss' : [],\n",
    "            'list_proba_avg_zero' : [],\n",
    "            'list_proba_avg_one' : [],\n",
    "            'list_auc_validation' : [],\n",
    "            'list_accuracy_validation' : [],\n",
    "            'list_f1_validation' : [],\n",
    "            'epochs' : [] }\n",
    "\n",
    "            # Training Loop\n",
    "            start_time_training = time.time()\n",
    "            print_file(output_file, f'Beginning of the program for {self.total_epochs} epochs', new_line=True)\n",
    "            # Training Loop\n",
    "            plot_ini_infos(self.model, output_file, dataloader_test, dataloader_train, writer, dic_features_list)\n",
    "            for epoch in range(1, self.total_epochs+1):\n",
    "\n",
    "                start_time_epoch = time.time()\n",
    "                total_loss = 0.0  \n",
    "                \n",
    "                #with tqdm(total=len(dataloader_train), position=0, leave=True) as pbar:\n",
    "                for k, input_dict in enumerate(dataloader_train):\n",
    "                    \n",
    "    \n",
    "\n",
    "                    # evaluate the loss\n",
    "                    logits, loss = self.model(input_dict)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                \n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if k % self.eval_batch_interval == 0:\n",
    "                        clear_last_line(output_file)\n",
    "                        print_file(output_file, f'Progress in epoch {epoch}  = {round(k / len(dataloader_train)*100, 2)} %, time batch : {time.time() - start_time_epoch}', new_line=False)\n",
    "\n",
    "                if epoch % self.eval_epochs_interval == 0:\n",
    "                    dic_features = plot_infos(self.model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, plots_path)\n",
    "\n",
    "                \n",
    "                \n",
    "                lr_scheduler.step()\n",
    "\n",
    "            \n",
    "            self.dic_features = dic_features\n",
    "            self.model = self.model.to('cpu')\n",
    "            self.model.write_embedding(writer)\n",
    "            # Print time\n",
    "            print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "            start_time = time.time()\n",
    "\n",
    "            with open(log_data_path_pickle, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "            print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            ## Add hyper parameters to tensorboard\n",
    "            hyperparams = {\"CHR\" : self.CHR, \"SNP\" : self.SNP, \"ROLLUP LEVEL\" : self.rollup_depth,\n",
    "                        'PHENO_METHOD': self.pheno_method, 'EMBEDDING_METHOD': self.embedding_method,\n",
    "                        'EMBEDDING SIZE' : self.Embedding_size, 'ATTENTION HEADS' : self.n_head, 'BLOCKS' : self.n_layer,\n",
    "                        'LR':1 , 'DROPOUT' : self.p_dropout, 'NUM_EPOCHS' : self.total_epochs, \n",
    "                        'BATCH_SIZE' : self.batch_size, \n",
    "                        'PADDING_MASKING': self.masking_padding,\n",
    "                        'VERSION' : self.model_version,\n",
    "                        'NB_Patients'  : len(self.patient_list),\n",
    "                        'LOSS_VERSION'  : self.loss_version,\n",
    "                        }\n",
    "            writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "            self.trained = True\n",
    "\n",
    "            self.save_model(update=True)\n",
    "            self.add_to_trained()\n",
    "\n",
    "class TrainLogRegModel(TrainModel):\n",
    "    def __init__(self, model_version, test_name, pheno_method, tryout, CHR, SNP, rollup_depth, binary_classes, counts_method, padding_token, prop_train_test, load_data,\n",
    "                save_data, remove_none, compute_features, padding, batch_size, data_share, seuil_diseases, equalize_label, embedding_method, freeze_embedding, Embedding_size, n_head, \n",
    "                 n_layer, Head_size, eval_epochs_interval, eval_batch_interval, p_dropout, masking_padding, loss_version, gamma, alpha, total_epochs, learning_rate_max, learning_rate_ini, learning_rate_final,\n",
    "                    warm_up_frac, decorelate, threshold_corr, threshold_rare, remove_rare, indices=None):\n",
    "        self.model_type = 'logistic_regression'\n",
    "        super().__init__(model_type=self.model_type, model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                        CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, counts_method=counts_method, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                        load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                        data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                        freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                        eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                        loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                        learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare, remove_rare=remove_rare, indices=indices)\n",
    "        \n",
    "    def train_model(self):\n",
    "        self.get_number_test() #update the test number\n",
    "        already_trained, number_test = self.test_already_trained()\n",
    "        \n",
    "        log_data_path_pickle, log_res_path_txt, log_res_path_pickle, log_slurm_outputs_path, log_slurm_error_path = self.generate_dirs(makedirs = True)\n",
    "        # Redirect  output to a file\n",
    "        sys.stdout = open(log_slurm_outputs_path, 'w')\n",
    "        sys.stderr = open(log_slurm_error_path, 'w')\n",
    "        sys.stdout = Unbuffered(sys.stdout)\n",
    "        sys.stderr = Unbuffered(sys.stderr)\n",
    "        if already_trained:\n",
    "            print('model has already been trained')\n",
    "           \n",
    "        else: \n",
    "            print(f'Begining of training program, device={self.device}')\n",
    "            self.get_dataT()\n",
    "            self.get_patient_list()\n",
    "\n",
    "        if self.model_version == 'global':\n",
    "            indices = np.arange(len(self.patient_list))\n",
    "            np.random.shuffle(indices)\n",
    "            pheno_data, label_data = self.patient_list.get_tree_data()\n",
    "            pheno_data_train = np.array(pheno_data)[indices[:int(self.prop_train_test*len(self.patient_list))]]\n",
    "            label_data_train = np.array(label_data)[indices[:int(self.prop_train_test*len(self.patient_list))]]\n",
    "            label_data_test = np.array(label_data)[indices[int(self.prop_train_test*len(self.patient_list)):]]\n",
    "            pheno_data_test = np.array(pheno_data)[indices[int(self.prop_train_test*len(self.patient_list)):]]                        \n",
    "            class_weights = compute_sample_weight(class_weight='balanced', y=label_data_train)\n",
    "\n",
    "            # Adjust the input data with the square root of weights\n",
    "            sqrt_weights = np.sqrt(class_weights)\n",
    "            pheno_data_train_weighted = pheno_data_train * sqrt_weights[:, np.newaxis]\n",
    "            column_one_train = np.ones((pheno_data_train.shape[0],1 ))\n",
    "            column_one_test = np.ones((pheno_data_test.shape[0],1 ))\n",
    "\n",
    "            pheno_data_train_weighted_with_constant=  np.concatenate([column_one_train, pheno_data_train_weighted], axis = 1)\n",
    "            pheno_data_train_with_constant =  np.concatenate([column_one_train, pheno_data_train], axis = 1)\n",
    "            pheno_data_test_with_constant =  np.concatenate([column_one_test, pheno_data_test], axis = 1)\n",
    "\n",
    "            logit_model = sm.Logit(label_data_train, pheno_data_train_weighted_with_constant)\n",
    "            result = logit_model.fit(method='bfgs', disp=True)\n",
    "            ### visualisation des donnes avec df\n",
    "            proba_test = result.predict(pheno_data_test_with_constant)\n",
    "            proba_train = result.predict(pheno_data_train_weighted_with_constant)\n",
    "\n",
    "\n",
    "            labels_pred_test = (proba_test > 0.5).astype(int)\n",
    "            nb_positive_test = np.sum(labels_pred_test==0)\n",
    "            nb_negative_test = np.sum(labels_pred_test==1)\n",
    "            labels_pred_train = (proba_train > 0.5).astype(int)\n",
    "            nb_positive_train = np.sum(labels_pred_train==0)\n",
    "            nb_negative_train = np.sum(labels_pred_train==1)\n",
    "\n",
    "\n",
    "            TP_test = np.sum((label_data_test==0 )& (labels_pred_test == 0)) / nb_positive_test\n",
    "            FP_test = np.sum((label_data_test==1 )& (labels_pred_test == 0)) / nb_positive_test\n",
    "            TN_test = np.sum((label_data_test==1 )& (labels_pred_test == 1)) / nb_negative_test\n",
    "            FN_test = np.sum((label_data_test== 0)& (labels_pred_test == 1)) / nb_negative_test\n",
    "\n",
    "            TP_train = np.sum((label_data_train==0 )& (labels_pred_train == 0)) / nb_positive_train\n",
    "            FP_train = np.sum((label_data_train==1 )& (labels_pred_train == 0)) / nb_positive_train\n",
    "            TN_train = np.sum((label_data_train==1 )& (labels_pred_train == 1)) / nb_negative_train\n",
    "            FN_train = np.sum((label_data_train== 0)& (labels_pred_train == 1)) / nb_negative_train\n",
    "\n",
    "\n",
    "            auc_test = calculate_roc_auc(label_data_test, proba_test)\n",
    "            auc_train = calculate_roc_auc(label_data_train, proba_train)\n",
    "\n",
    "            with open(log_res_path_txt, 'w') as file:\n",
    "                file.write(f'TP_test={TP_test}\\n')\n",
    "                file.write(f'TN_test={TP_test}\\n')\n",
    "                file.write(f'TP_train={TP_train}\\n')\n",
    "                file.write(f'TN_train={TP_train}\\n')    \n",
    "                file.write(f'auc_test={auc_test}\\n')        \n",
    "                file.write(f'auc_train={auc_train}\\n')        \n",
    "\n",
    "            with open(log_data_path_pickle, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "                print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "            dic_res = {'label_test':label_data_test, 'label_train':label_data_train,\n",
    "                        'label_pred_test':labels_pred_test, 'label_pred_train':labels_pred_train, \n",
    "                        'proba_test':proba_test, 'proba_train':proba_train}\n",
    "            with open(log_res_path_pickle, 'wb') as file:\n",
    "                pickle.dump(dic_res, file)\n",
    "                print('Res saved to %s' % log_res_path_pickle)\n",
    "\n",
    "class TrainNaiveModel(TrainModel):\n",
    "    def __init__(self, model_version, test_name, pheno_method, tryout, CHR, SNP, rollup_depth, binary_classes, counts_method, padding_token, prop_train_test, load_data,\n",
    "                save_data, remove_none, compute_features, padding, batch_size, data_share, seuil_diseases, equalize_label, embedding_method, freeze_embedding, Embedding_size, n_head, \n",
    "                 n_layer, Head_size, eval_epochs_interval, eval_batch_interval, p_dropout, masking_padding, loss_version, gamma, alpha, total_epochs, learning_rate_max, learning_rate_ini, learning_rate_final,\n",
    "                    warm_up_frac, decorelate, threshold_corr, threshold_rare, remove_rare, list_env_features, indices=None):\n",
    "        self.model_type = 'naive_model'\n",
    "        super().__init__(model_type=self.model_type, model_version=model_version, test_name=test_name, pheno_method=pheno_method, tryout=tryout, \n",
    "                        CHR=CHR, SNP=SNP, rollup_depth=rollup_depth, binary_classes=binary_classes, counts_method=counts_method, padding_token=padding_token, prop_train_test=prop_train_test,\n",
    "                        load_data=load_data,save_data=save_data, remove_none=remove_none, compute_features=compute_features, padding=padding, batch_size=batch_size,\n",
    "                        data_share=data_share, seuil_diseases=seuil_diseases, equalize_label=equalize_label, embedding_method=embedding_method, \n",
    "                        freeze_embedding=freeze_embedding, Embedding_size=Embedding_size, n_head=n_head, n_layer=n_layer, Head_size=Head_size,\n",
    "                        eval_epochs_interval=eval_epochs_interval, eval_batch_interval=eval_batch_interval, p_dropout=p_dropout, masking_padding=masking_padding,\n",
    "                        loss_version=loss_version, gamma=gamma, alpha=alpha, total_epochs=total_epochs, learning_rate_max=learning_rate_max, learning_rate_ini=learning_rate_ini,\n",
    "                        learning_rate_final=learning_rate_final,warm_up_frac=warm_up_frac, decorelate=decorelate, threshold_corr=threshold_corr, threshold_rare=threshold_rare, remove_rare=remove_rare, indices=indices)\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        self.get_number_test() #update the test number\n",
    "        already_trained, number_test = self.test_already_trained()\n",
    "\n",
    "        log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path = self.generate_dirs(makedirs = True)\n",
    "        # Redirect  output to a file\n",
    "        sys.stdout = open(log_slurm_outputs_path, 'w')\n",
    "        sys.stderr = open(log_slurm_error_path, 'w')\n",
    "        sys.stdout = Unbuffered(sys.stdout)\n",
    "        sys.stderr = Unbuffered(sys.stderr)\n",
    "        if already_trained:\n",
    "            print('model has already been trained')\n",
    "            self.save_model()\n",
    "        else: \n",
    "            print(f'Begining of training program, device={self.device}')\n",
    "            start_time_training = time.time()\n",
    "            self.get_dataT()\n",
    "            data, labels, indices_env, name_envs = self.dataT.get_tree_data(with_env=False)\n",
    "            if self.equalize_label:\n",
    "                data, labels = DataTransfo_1SNP.equalize_label(data, labels)  \n",
    "            self.data = data          \n",
    "            self.nb_phenos = data.shape[1]\n",
    "            data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size = 1-self.prop_train_test, random_state=42)\n",
    "            data_train = CustomDatasetWithLabels(data_train, labels)\n",
    "            dataloader_train = DataLoader(data_train, batch_size=self.batch_size, shuffle=True)\n",
    "            data_test = CustomDatasetWithLabels(data_test, labels)\n",
    "            dataloader_test = DataLoader(data_test, batch_size=self.batch_size, shuffle=True)\n",
    "            self.model = NaiveModelWeights(pheno_nb=self.nb_phenos)\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.0001)\n",
    "            output_file = log_slurm_outputs_path\n",
    "\n",
    "            writer = SummaryWriter(log_tensorboard_path)\n",
    "            dic_features_list = {\n",
    "                'list_training_loss' : [],\n",
    "                'list_validation_loss' : [],\n",
    "                'list_proba_avg_zero' : [],\n",
    "                'list_proba_avg_one' : [],\n",
    "                'list_auc_validation' : [],\n",
    "                'list_accuracy_validation' : [],\n",
    "                'list_f1_validation' : [],\n",
    "                'epochs' : [] }\n",
    "            for epoch in range(self.total_epochs):\n",
    "                start_time_epoch = time.time()\n",
    "                total_loss = 0\n",
    "                for k, batch in enumerate(dataloader_train):\n",
    "                    data_train= batch['data']\n",
    "                    labels_train = batch['label']\n",
    "                    # evaluate the loss\n",
    "                    pred_probas, loss = self.model(data_train, labels_train)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                if epoch % self.eval_epochs_interval==0:\n",
    "                    dic_features = plot_infos(self.model, output_file, epoch, total_loss, start_time_epoch, dataloader_train, dataloader_test, optimizer, writer, dic_features_list, plots_path)\n",
    "\n",
    "\n",
    "                \n",
    "             # Print time\n",
    "            print_file(output_file, f\"Training finished: {int(time.time() - start_time_training)} seconds\", new_line=True)\n",
    "            start_time = time.time()\n",
    "\n",
    "            with open(log_data_path_pickle, 'wb') as file:\n",
    "                pickle.dump(self, file)\n",
    "            print('Model saved to %s' % log_data_path_pickle)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ## Add hyper parameters to tensorboard\n",
    "            hyperparams = {\"CHR\" : self.CHR, \"SNP\" : self.SNP, \"ROLLUP LEVEL\" : self.rollup_depth,\n",
    "                        'PHENO_METHOD': self.pheno_method, 'EMBEDDING_METHOD': self.embedding_method,\n",
    "                        'EMBEDDING SIZE' : self.Embedding_size, 'ATTENTION HEADS' : self.n_head, 'BLOCKS' : self.n_layer,\n",
    "                        'LR':1 , 'DROPOUT' : self.p_dropout, 'NUM_EPOCHS' : self.total_epochs, \n",
    "                        'BATCH_SIZE' : self.batch_size, \n",
    "                        'PADDING_MASKING': self.masking_padding,\n",
    "                        'VERSION' : self.model_version,\n",
    "                        'NB_Patients'  : len(self.data),\n",
    "                        'LOSS_VERSION'  : self.loss_version,\n",
    "                        }\n",
    "            writer.add_hparams(hyperparams, dic_features)\n",
    "\n",
    "            self.trained = True\n",
    "\n",
    "            self.save_model(update=True)\n",
    "            self.add_to_trained()\n",
    "class TestSet:\n",
    "    def __init__(self, test_cat, test_field, test_name, param_dic={}, test_ref=None):\n",
    "        self.test_cat = test_cat\n",
    "        self.test_field = test_field\n",
    "        self.test_name = test_name\n",
    "        self.param_dic = param_dic\n",
    "        self.test_ref = test_ref\n",
    "        self.list_number_test = []\n",
    "        self.number_test_set = None\n",
    "        self.number_test_set = self.get_number()\n",
    "        self.test_name_with_infos = f'{self.number_test_set}_{self.test_name}'\n",
    "\n",
    "        self.test_set_save_file = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/{self.test_cat}/{self.test_field}/{self.test_name_with_infos}'\n",
    "        self.test_set_tensorboard_dir = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tests/{self.test_cat}/{self.test_field}/{self.test_name_with_infos}'\n",
    "    @property\n",
    "    def get_nombre_model(self):\n",
    "        return len(list(self.param_dic.values())[0])\n",
    "    \n",
    "    def get_number(self):\n",
    "        if self.number_test_set == None:\n",
    "            file = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/list_tests.csv'\n",
    "            df_tests = pd.read_csv(file)\n",
    "            if len(df_tests.index)==0:\n",
    "                self.number_test_set = 0\n",
    "            else:\n",
    "                self.number_test_set = np.max(df_tests.number_test_set)+1\n",
    "        return self.number_test_set\n",
    "    \n",
    "    def create_tests(self):\n",
    "        if self.test_ref != None:\n",
    "            list_number_test = []\n",
    "            list_number_test_to_train = []\n",
    "            list_tests = [ copy.deepcopy(self.test_ref) for _ in range(self.get_nombre_model)]\n",
    "            for k, test in enumerate(list_tests):\n",
    "                for param in self.param_dic.keys():\n",
    "                    test.test_name = f'{test.test_name}_{param}={str(self.param_dic[param][k])}'\n",
    "                    test.__setattr__(param, self.param_dic[param][k])\n",
    "                    test.save_model()\n",
    "\n",
    "            self.list_test = list_tests\n",
    "            path_model_test = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/training/list_models_tests'\n",
    "            for test in list_tests:\n",
    "                test.get_number_test()\n",
    "                train, number = test.test_already_trained()\n",
    "                \n",
    "                if not train:\n",
    "                    list_number_test_to_train.append(test.number_test)\n",
    "                    list_number_test.append(test.number_test)\n",
    "                else:\n",
    "                    list_number_test.append(test.number_test)\n",
    "            self.list_number_test = list_number_test\n",
    "        else:\n",
    "            print('case without test_ref not made yet')\n",
    "        self.save_test_set()\n",
    "        return test, list_number_test, list_number_test_to_train\n",
    "\n",
    "    \n",
    "    def save_test_set(self):\n",
    "        save_dir =  f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/{self.test_cat}/{self.test_field}/'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = f'{save_dir}{self.test_name_with_infos}'\n",
    "        with open(save_path, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "        self.add_to_saved()\n",
    "            \n",
    "    def get_row(self):\n",
    "        values_test = [self.number_test_set, self.test_cat, self.test_field, self.test_name]\n",
    "        df_test_set = pd.DataFrame([values_test],columns = ['number_test_set', 'test_cat', 'test_field', 'test_name'])\n",
    "        for k, test_instance_number in enumerate(self.list_number_test):\n",
    "            df_test_set[f'nb_instance_test_{k}'] = test_instance_number\n",
    "        return df_test_set\n",
    "            \n",
    "        \n",
    "    def add_to_saved(self):\n",
    "        df_list_test_set_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/list_tests.csv'\n",
    "        df_list_test_set = pd.read_csv(df_list_test_set_path)\n",
    "        df_test_set = self.get_row()\n",
    "        df_list_test_set =  pd.concat([df_list_test_set, df_test_set], ignore_index=True)\n",
    "        df_list_test_set.to_csv(df_list_test_set_path, index=False)\n",
    "    @staticmethod\n",
    "    def load_test_set_from_number(test_set_number):\n",
    "        test_cat, test_field, test_name, list_instance_test_set = TestSet.load_from_df(test_set_number)\n",
    "        test_name_with_infos = f'{test_set_number}_{test_name}'\n",
    "        test_set_path = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/{test_cat}/{test_field}/{test_name_with_infos}'\n",
    "        with open(test_set_path, 'rb') as file:\n",
    "            test_set = pickle.load(file)\n",
    "        return test_set\n",
    "    @staticmethod\n",
    "    def load_from_df(test_set_number):\n",
    "        df_list_test_set_path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/list_tests/list_tests.csv'\n",
    "        df_list_test_set = pd.read_csv(df_list_test_set_path)\n",
    "        df_list_test_set.set_index('number_test_set', inplace=True)\n",
    "        infos_test_set = df_list_test_set.iloc[test_set_number]\n",
    "        test_cat = infos_test_set['test_cat']\n",
    "        test_field = infos_test_set['test_field']\n",
    "        test_name = infos_test_set['test_name']\n",
    "        list_instance_test_set = []        \n",
    "        for col in df_list_test_set.columns:\n",
    "            if 'nb' in col:\n",
    "                print(type(infos_test_set[col]))\n",
    "                if infos_test_set[col] > 0:\n",
    "                    list_instance_test_set.append(infos_test_set[col])\n",
    "        return test_cat, test_field, test_name, list_instance_test_set\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_analyse_res_test_set(test_set_number=None, test_set=None, actual_test=True):\n",
    "        if test_set == None:\n",
    "            test_set = TestSet.load_test_set_from_number(test_set_number)\n",
    "        path_tensorboard_test_set = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/tests/{test_set.test_cat}/{test_set.test_field}/{test_set.test_name_with_infos}'\n",
    "        tensorboard_dir = test_set.test_set_tensorboard_dir\n",
    "        for instance_test_nb in test_set.list_number_test:\n",
    "            instance_test = TrainModel.load_instance_test(instance_test_nb)\n",
    "            print(instance_test.number_test, instance_test.number_test_trained)\n",
    "            if instance_test.number_test_trained != None:\n",
    "                instance_test_trained = TrainModel.load_instance_test(instance_test_number=instance_test.number_test_trained)\n",
    "                log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path  = instance_test_trained.generate_dirs()\n",
    "            else:\n",
    "                log_data_path_pickle, log_tensorboard_path, log_slurm_outputs_path, log_slurm_error_path, plots_path  = instance_test.generate_dirs()\n",
    "\n",
    "            path_tensorboard_test_instance = f'{path_tensorboard_test_set}/{instance_test.test_name}'\n",
    "            if not os.path.exists(path_tensorboard_test_instance):\n",
    "                shutil.copytree(log_tensorboard_path, path_tensorboard_test_instance )\n",
    "        if actual_test:\n",
    "            shutil.rmtree(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/')\n",
    "            os.makedirs(f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/')\n",
    "            actual_test_dir = f'/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/actual_test/{test_set.test_cat}/{test_set.test_field}/{test_set.test_name_with_infos}'\n",
    "            \n",
    "            shutil.copytree(path_tensorboard_test_set, actual_test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mTrainModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_res_instance_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m241\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactual_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 400\u001b[0m, in \u001b[0;36mTrainModel.get_res_instance_test\u001b[0;34m(instance_test_number, instance_test, actual_test, add)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_res_instance_test\u001b[39m(instance_test_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, instance_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, actual_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,add\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m         instance_test \u001b[38;5;241m=\u001b[39m \u001b[43mTrainModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_instance_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_test_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     path_tensorboard_test_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/tensorboard/tests/all_instances/SNPS/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(instance_test\u001b[38;5;241m.\u001b[39mCHR)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_test\u001b[38;5;241m.\u001b[39mSNP\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_test\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_test\u001b[38;5;241m.\u001b[39mmodel_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_test\u001b[38;5;241m.\u001b[39mpheno_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_test\u001b[38;5;241m.\u001b[39mget_test_name_with_infos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance_test\u001b[38;5;241m.\u001b[39mnumber_test_trained \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 372\u001b[0m, in \u001b[0;36mTrainModel.load_instance_test\u001b[0;34m(instance_test_number)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m number_instance_test_try \u001b[38;5;241m==\u001b[39m instance_test_number:\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_model_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minstance_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 372\u001b[0m                 test_instance \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m    373\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m test_instance\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest was not found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/storage.py:337\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/serialization.py:1256\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1254\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1255\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1256\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1260\u001b[0m offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;28;01mif\u001b[39;00m f_should_read_directly \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/serialization.py:1193\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1193\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1194\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1195\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1196\u001b[0m     deserialized_objects[root_key] \u001b[38;5;241m=\u001b[39m typed_storage\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/site-packages/torch/serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    255\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    260\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    261\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    262\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "TrainModel.get_res_instance_test(241, actual_test=True, add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "237 None\n",
      "/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/runs/SNPS/1/rs673604/transformer/transformer_V2/Abby/237_baseline_model_L1=Truetryout/\n",
      "238 None\n",
      "/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/runs/SNPS/1/rs673604/transformer/transformer_V2/Abby/238_baseline_model_L1=Falsetryout/\n"
     ]
    }
   ],
   "source": [
    "TestSet.get_analyse_res_test_set(33, actual_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ref_whole_abby_focal_loss_small_decorelate'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/commons/groups/gursoy_lab/mstoll/codes/tests/TestsClass.py:232: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_list_instance_test_saved =  pd.concat([df_list_instance_test_saved, df_instance_test_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "train_model.test_name = 'ref_10%_abby_small_model_freeze_embedding=False'\n",
    "train_model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.float64'>\n",
      "71 None\n",
      "/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/runs/SNPS/1/rs673604/transformer/transformer_V2/Abby/71_ref_10%_abby_small_model_decorelate=True/\n",
      "72 None\n",
      "/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/runs/SNPS/1/rs673604/transformer/transformer_V2/Abby/72_ref_10%_abby_small_model_decorelate=False/\n"
     ]
    }
   ],
   "source": [
    "TestSet.get_analyse_res_test_set(11, actual_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_model_dir = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/models/list_models_class/'\n",
    "for instance_test in os.listdir(instance_model_dir):\n",
    "    if instance_test == 'list_instance_tests_saved.csv':\n",
    "        pass\n",
    "    else:\n",
    "        number_instance_test_try = int(instance_test.split('_')[0])\n",
    "    \n",
    "        if number_instance_test_try == instance_test_number:\n",
    "            with open(f'{instance_model_dir}{instance_test}', 'rb') as file:\n",
    "                test_instance = pickle.load(file)\n",
    "                return test_instance\n",
    "print('test was not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/runs/SNPS/1/rs673604/transformer/transformer_V2/Paul/15_baseline_model_focal/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/runs/SNPS/1/rs673604/transformer/transformer_V2/Paul/15_baseline_model_focal//tensorboard/baseline_model_focal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mTrainModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_res_instance_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactual_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 411\u001b[0m, in \u001b[0;36mTrainModel.get_res_instance_test\u001b[0;34m(instance_test_number, instance_test, actual_test, add)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path_tensorboard_test_instance): \n\u001b[1;32m    409\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(path_tensorboard_test_instance)\n\u001b[0;32m--> 411\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_tensorboard_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_tensorboard_test_instance\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actual_test:\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m add\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/phewas/lib/python3.11/shutil.py:559\u001b[0m, in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Recursively copy a directory tree and return the destination directory.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03mIf exception(s) occur, an Error is raised with a list of reasons.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m`src` tree.\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutil.copytree\u001b[39m\u001b[38;5;124m\"\u001b[39m, src, dst)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m itr:\n\u001b[1;32m    560\u001b[0m     entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itr)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _copytree(entries\u001b[38;5;241m=\u001b[39mentries, src\u001b[38;5;241m=\u001b[39msrc, dst\u001b[38;5;241m=\u001b[39mdst, symlinks\u001b[38;5;241m=\u001b[39msymlinks,\n\u001b[1;32m    562\u001b[0m                  ignore\u001b[38;5;241m=\u001b[39mignore, copy_function\u001b[38;5;241m=\u001b[39mcopy_function,\n\u001b[1;32m    563\u001b[0m                  ignore_dangling_symlinks\u001b[38;5;241m=\u001b[39mignore_dangling_symlinks,\n\u001b[1;32m    564\u001b[0m                  dirs_exist_ok\u001b[38;5;241m=\u001b[39mdirs_exist_ok)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gpfs/commons/groups/gursoy_lab/mstoll/codes/logs/runs/SNPS/1/rs673604/transformer/transformer_V2/Paul/15_baseline_model_focal//tensorboard/baseline_model_focal'"
     ]
    }
   ],
   "source": [
    "TrainModel.get_res_instance_test(15, actual_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all_dir(model_type, model_version, CHR, SNP, pheno_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### framework constants:\n",
    "\n",
    "test_name = 'abby_without_masking_without_embedding'\n",
    "numer_test =1\n",
    "tryout = False # True if we are ding a tryout, False otherwise \n",
    "### data constants:\n",
    "CHR = 1\n",
    "SNP = 'rs673604'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check test name\n",
    "#check number tests\n",
    "test_name_with_infos = str(number_test) + '_' + test_name + 'tryout'*tryout\n",
    "test_dir = f'{model_dir}/{test_name_with_infos}/'\n",
    "log_model_dir = f'{test_dir}/model/'\n",
    "log_data_dir = f'{test_dir}/data/'\n",
    "log_info_dir = f'{test_dir}/infos/tensorboard/'\n",
    "log_slurm_outputs_dir = f'{test_dir}/Slurm/Outputs/'\n",
    "log_slurm_errors_dir = f'{test_dir}/Slurm/Errors/'\n",
    "\n",
    "\n",
    "log_model_path_torch = f'{test_dir}/model/{test_name}.pth'\n",
    "log_model_path_pickle = f'{test_dir}/model/{test_name}.pkl'\n",
    "log_data_path_pickle = f'{test_dir}/data/{test_name}.pkl'\n",
    "\n",
    "\n",
    "log_info_path = f'{test_dir}/infos/tensorboard/{test_name}.pth'\n",
    "log_slurm_outputs_path = f'{test_dir}/Slurm/Outputs/{test_name}.pth'\n",
    "log_slurm_error_path = f'{test_dir}/Slurm/Errors/{test_name}.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### compare list\n",
    "category_test = 'balance_impact'\n",
    "comparaison_name = 'low_lr_balance_test'\n",
    "dic_test_1 = {'CHR':1,\n",
    "              'SNP':'rs673604',\n",
    "              'model':'transformer',\n",
    "              'version':'transformer_V2',\n",
    "              'pheno_method':'Abby',\n",
    "              'test_name': 'test_balance_small_data_abby_balanced',\n",
    "              'number_test':21,\n",
    "              'tryout':False\n",
    "             }\n",
    "dic_test_1 = {'CHR':1,\n",
    "              'SNP':'rs673604',\n",
    "              'model':'transformer',\n",
    "              'version':'transformer_V2',\n",
    "              'pheno_method':'Abby',\n",
    "              'test_name': 'test_balance_small_data_abby_unbalanced',\n",
    "              'number_test':22,\n",
    "              'tryout':False\n",
    "             }\n",
    "list_dic_tests = [dic_test_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load actual test dir\n",
    "actual_test_dir = f'actual_test/{category_test}/{comparaison_name}/'\n",
    "path = '/gpfs/commons/groups/gursoy_lab/mstoll/codes/'\n",
    "os.makedirs(actual_test_dir, exist_ok=True)\n",
    "shutil.rmtree(actual_test_dir)\n",
    "os.makedirs(actual_test_dir)\n",
    "for dic_test in list_dic_tests:\n",
    "    CHR = dic_test['CHR']\n",
    "    SNP = dic_test['SNP']\n",
    "    model_type = dic_test['model']\n",
    "    model_version = dic_test['version']\n",
    "    pheno_method = dic_test['pheno_method']\n",
    "    test_name = dic_test['test_name']\n",
    "    number_test = dic_test['number_test']\n",
    "    tryout = dic_test['tryout']\n",
    "    \n",
    "    model_dir = path + f'logs/SNPS/{str(CHR)}/{SNP}/{model_type}/{model_version}/{pheno_method}'\n",
    "    test_name_with_infos = str(number_test) + '_' + test_name + 'tryout'*tryout\n",
    "    test_dir = f'{model_dir}/{test_name_with_infos}/'\n",
    "    log_info_dir = f'{test_dir}infos/tensorboard/{test_name}'\n",
    "\n",
    "    shutil.copytree(log_info_dir, f'{actual_test_dir}{test_name_with_infos}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phewas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
